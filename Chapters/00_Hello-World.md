
\tableofcontents

# Hello, World!

The common first letters a programmer types into its digital computing machine is some form of ‘hello world’ along with braces, punctuation, and words the machine should interpret. In my favorite language, JavaScript, it can look as simple as this `console.log(“Hello, World!”)`. After executing this short string of text in the browser, the computer will perform a series of events and likewise respond with ‘Hello, World!’ in the developer console.

This ritual of greeting each other (and the world) goes back to Brian W. Kernighan, who wrote the popular guide *The C Programming Language*[^1] in 1978, where he proclaims that the only way to learn a programming language is by writing programs in it.

I start this introduction with the same greeting, and as I press these letters on my keyboard, they simultaneously appear on the screen in front of me. This translation from the tactical key press to the pixels on the screen becomes possible through a chain of electrical signals that reach my retina in a matter of milliseconds and close the loop. Thousands of people have created these intricate systems which interoperate and depend on each other, each new layer abstracting underlying operations further. Only the text editor I am using to write these lines already has hundreds of contributors writing thousands of lines of code, which in turn rely on the functions of the operating system created by kernel developers.  

This chain of creativity represents the core of open source software development based on collaboration and sharing knowledge. Every contribution, no matter how small or insignificant it may seem, is valuable in creating something more significant than what was there before. Whether it is a bug fix, a feature request, or a new idea - every contribution makes a difference. 

In this thesis, I focus on the known and unknown human connections we make with and through computers, specifically in machine learning. My hypothesis is that the current trend of framing ‘Artificial Intelligence’ as autonomous computational beings would be better described as ‘Co-Intelligence’, where programming such systems is a collaborative effort with many hidden actors. While I address some technical details of current machine learning systems, I will put them into a social context because software inherently is a social tool. To describe the social inheritance, I use my own and other artists’ artworks to gain a broader aesthetic perspective, which can go beyond logic and language. 

Even though I focus on the social nature of computing and am influenced by many of my peers, I am describing these topics from my subjective perspective. I don’t claim any objective truths on the descriptions of my collaborative works and am naturally biased toward my contribution to them. As I write this thesis in the context of an art program, I deviate from rigorous scientific methods and describe my observations more freely. Instead of trying to answer the question of the nature of AI as subjective entities, I focus on the practices we developed through the engagement with that question.


## Dreaming

![“PuppySlug” - posted by a deleted user on /r/creepy with the title “This image was generated by a computer on its own (from a friend working on AI)” on January 10th, 2015. Seven days before the Google blog post](6ocuQsZ.jpeg)

My journey into the hype machine of ‘Artificial Intelligence’ started around 2015, when Google released DeepDream[^2] on its research blog. The developers Alexander Mordvintsev, Michael Tyka, and Christopher Olah were initially trying to peek inside an ‘Artificial Neural Network’ that was ‘trained’ for image classification tasks. They reversed the task of the classifier, and instead of identifying what a set of pixels look like, the pixel values are changed to look more similar to what the network has ‘learned’. This created images that amplified the textures embedded in their networks, transforming the input image into swirls of PuppySlugs.[^3] This imagery that reminded people of bad dreams and hallucinogenic trips was widely shared on the internet, which skyrocketed the awareness that ‘Deep Learning’ and biologically inspired computing have returned from their hibernation during the AI winter.

In 2012 ‘Artificial Neural Networks’ made an incredible comeback into the field of computer vision. A team around Fei-Fei Li created ImageNet,[^5] a dataset of images organized into categories by anonymous workers from the internet. The ImageNet project has held a yearly competition since 2010. For the first two years, the best algorithm recognized these images with a 74% accuracy until an ‘Artificial Neural Network’ topped the score with 85% accuracy and woke the computer vision community to their potential powers. 

With DeepDream, this technique became tangible for the general public as it created something to look at, something for humans to empathize with the outputs of an abstract mathematical model. This tendency to anthropomorphize this type of software becomes apparent in how we use language to describe it. We talk of ‘Neural Networks’ as biological entities that can be ‘taught’ and which ‘learn’, ‘experience’, and ‘read’ data from the world. This turning point got me and many other artists interested in ‘collaborating’ with seemingly autonomous machines. Machines that we do not have to understand through mathematics but through interpretation and experimentation of ‘their dreams’. Yet, the outputs of DeepDream quickly became kitsch. Google shared the code with an open source license, and many people created apps and APIs to generate PuppySlug textures on top of their own images.[^7] As with most other memes on the internet, the hype around DeepDream died quickly, and by the end of 2015, the world was already saturated with computer-generated hallucinogenic images. Other researchers have since picked up[^8] on visualizing nodes in ‘Artificial Neural Networks’, and Alex Mordvintsev—together with his wife—has since become an artist, exhibiting in art fairs.[^9] What stayed was the notion that ‘Artificial Intelligence’ became some form of computational being different from the humans who wrote the code, labeled, and sorted the data.


## Terms

I have deliberately used quotation marks around some of the previous words, as I am critical of the language used when discussing AI systems. In his book *The Artist in the Machine* Arthur I. Miller concludes that machines could be seen as creative and will be considered “artists, writers, and musicians in their own right.” [^10] He is using a typical techno-utopian argument that the technology is not quite there yet to *really* be creative but that it will change in the foreseeable future. At the same time, Miller explains in great detail the actual creative work of Mordvintsev and his human experience of insomnia to come up[^11] with the generative system for DeepDream and how he shared it with his peers at Google. 

Alex Mordvintsev was not collaborating with his computer; he created an emergent algorithmic system with it. The software does not hang pictures on exhibition walls, talk to gallerists, and has no agency in the process if it were to generate images or not. Peter Weibel has formulated it bluntly: Artificial intelligence does not exist. But an ensemble of machines, media, programs, algorithms, hardware, and software has resulted in an extraordinarily large, diverse, and productive field of research called AI.[^12] 

Where Arthur Miller is feeding the narrative of humanoid robots with glowing blue brains,[^13] most computer scientists in the field of artificial intelligence today are working in the subdiscipline of ‘Machine Learning’. The terms have converged in the media landscape in the last ten years. Still, where intelligence is rejected as a suitcase word with a multiplicity of meanings, ‘Machine Learning’ tries to define the task more narrowly to some form of pattern recognition and extrapolation of existing data. They use a variety of computational and statistical techniques to form abstract models for domain-specific problems. In these fields, ’Artificial Intelligence’ is a buzzword to convince governments and venture capitalists to fund projects. The artist and researcher Francis Hunger recently shared a list of alternative terms in an attempt to dehumanize our language for AI systems:

1. ‘Artificial Intelligence’ => ‘Automated Pattern Recognition’
2. ‘Machine Learning’ => ‘Machine Conditioning’ OR ‘Automated Classification’
3. ‘Neural Network’ => ‘Weighted Network’
4. ‘Deep Learning’ => ‘Deep Conditioning’
5. ‘Neuron’ => ‘Weight’ or ‘Node’ \
 [^14] 

In an accompanying talk[^15], he explains the aim of those terms is to invoke passivity, that we deal with machines and humans set those machines in motion, even when, in the end, we form ‘human-machine assemblages’. I like many of the proposed terms, even though they are still closely associated with biological operations. The conditioning of machines, for example, reminds me of Pavlov’s dog experiments or B.F. Skinner’s box to modify pigeon behavior by reinforcement or punishment.[^16] This analogy serves well to create an image of the machine as a system that can be controlled by changing the parameters of its virtual environment. Therefore I will use ‘conditioning’ and ‘weighted networks’ where applicable. 

Using ‘Automated Pattern Recognition’ instead of ‘Artificial Intelligence’, however, becomes too narrow of a definition and is counterintuitive to me, as AI serves precisely the function of being ill-defined. Pei Wang has made a great effort to define the different strands of AI research and their working definitions.[^17] He clusters them into Structure-AI (recreating the human brain), Behavior-AI (recreating human actions), Capability-AI (domain problem solving), Function-AI (developing cognitive functions), and Principle-AI (finding underlying principles) and comes up with a working definition to where AI research should be headed and how it can be unified. My initial goal for this thesis was to avoid the term’ Artificial Intelligence’ altogether, but as I have already failed in that task and coming up with a less anthropomorphic term does not seem feasible to convey the research in the field and its media reception. Therefore I will keep using the abbreviation AI.

Contrary to articles and sci-fi novels, I will not use the personification of ‘an AI’ but instead talk of AI systems, meaning complex emergent programs. Many of the systems today use statistical modeling. Yet, nobody would ask if statistics can be creative, so I decided not to engage with philosophical questions of consciousness and creativity. Instead, I want to explore how AI systems can include the human knowledge and work that goes into building them, challenging the AI ideology of machine autonomy and proposing human-centric goals rather than creating a machine as a goal in itself.


## Getting Started

I structured this thesis around four chapters, combining historical, computational, and collective knowledge. Throughout each chapter, I include examples of artworks by me or others that either relate to the topic or support the argument.

First, I am revisiting collective experiences I have organized with other artists, like the _Silicon Friend Camp_ in the Austrian mountains. This week-long retreat brought together 17 artists and researchers to focus on human-computer conversations and evolved into an exhibition and a symposium.

From there, I delve into the history of talking machines, from Wolfgang von Kempelen's speech automatons to today's digital assistants and the social changes that come with them. Talking to computers sets the foundation for my definition that these interactions are not with, but through technology, with other humans.

The next chapter explores the building blocks for complex statistical modeling in AI systems. Researchers require large datasets, often gathered through ethically questionable means by aggregating data from internet users. A closer look is taken into the StyleGAN dataset as the model that emerged from it was a turning point in synthetic image creation.

Similarly, GPT-2 generates seemingly coherent text based on scraped websites and books. So the last chapter focuses on the underpinning architecture of the model and its ability to exploit the human drive to find meaning in symbols. I conclude with The Chiromancer, an artwork that explores how we place trust, hopes, and wishes into our cold silicon companions.

---

[^1]: The C Programming Language Book defined many standards of programming languages today and how technical descriptions are written. While it focuses on C and the UNIX system, I find this advice from *Chapter 1.1 Getting Started* particularly interesting “On other systems, the rules will be different; check with a local expert.” as it describes the social necessity of learning computers specifically.
[^2]: See @InceptionismGoingDeeper2015.
[^3]: The initial classifier was conditioned on ImageNet, which contains many images of dog breeds. Therefore DeepDream is biased to generate textures of dog faces.
[^5]: See @ImageNet.
[^7]: See @tykaDeepdreamInceptionismRecap for a list of projects released within a month after publishing the source code of DeepDream. 
[^8]: See @molnar10LearnedFeatures for more examples and explanations in the interpretable machine learning book. He notes that “Feature visualizations give unique insight into the working of neural networks”, but the “visualizations can convey the illusion that we understand what the neural network is doing”. 
[^9]: See @baileyDeepDreamCreatorUnveils2018.
[^10]: @millerArtistMachineWorld2019, p.122.
[^11]: Mordvintsev was inspired by a previous paper exploring the generative potential of CNNs: *Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps* by Simonyan et al. (2014).
[^12]: See @weibelAAA.
[^13]: See @BetterImagesAI for alternative imagery about AI. 
[^14]: See @francishungerArtificialIntelligenceAutomated2021
[^15]: See @hungerTalkUnhypeAI2021
[^16]: In behaviorist psychology, Ivan Pavlov’s experiments with dogs are known as ‘classical conditioning’. Using lever machines, B.F. Skinners experiments on rats and pigeons are called ‘operant conditioning’. 
[^17]: @wangDefiningArtificialIntelligence2019
