\tableofcontents

# Hello, World!

The common first letters a programmer types into its digital computing machine is some form of ‘hello world’ along with braces, punctuation and words that the machine should interpret. In my favorite language JavaScript it can look as simple as this `console.log("Hello, World!")`.  After executing this short string of text in the browser, the computer will set of a series of events and likewise respond with ‘Hello, World!’ in the developer console.

This ritual of greeting each other (and the world) is accredited to Brian W. Kernighan, who wrote the popular guide *The C Programming Language*[^1] in 1978, where he proclaims that the only way to learn a programming language is by writing programs in it.

I start this introduction with the same greeting and as I am pressing these letters on my keyboard they simultaneously appear on the screen in front of me. This translation from the tactical key press to the pixels on the screen becomes possible through a chain of electrical signals, that in a matter of milliseconds reach my retina and close the loop. Thousands of people have created these intricate systems which interoperate and depend on each other, each new layer abstracting underlying operations further. Just the text editor I am using to write these lines has hundreds of contributors writing thousands of lines of code, which in turn rely on the functions of the operating system created by kernel developers.  

This chain of creativity represents the core of open source software development, which is based on collaboration and sharing knowledge. Every contribution, no matter how small or insignificant it may seem, is valuable in creating something greater than what was there before. Whether it is a bug fix, a feature request or a new idea - every contribution makes a difference. 

In this thesis I focus on the known and unknown human connections we make with and through computers, specifically in the realm of machine learning. My hypothesis is that the current trend of framing ‘Artificial Intelligence’ as autonomous computational beings, would be better described as ‘Co-Intelligence’, where the programming of such systems is a collaborative effort with many hidden actors. While I address some technical details of current machine learning systems, I will put them into social context, because software inherently is a social tool. To describe the social inheritance I use my own and other artists artworks to gain a broader aesthetic perspective, which can go beyond logic and language. 

Even though I focus on the social nature of computing and I am influenced by many of my peers, I am describing these topics from my subjective perspective. I don't claim any objective truths on the descriptions of the collaborative works I have done and am naturally biased towards my own contribution to it. As this thesis is written in the context of an art program, I tend to deviate from rigorous scientific methods and describe my observations more freely. Instead of trying to answer the question of the nature of AI as subjective entities, I focus on the practices we developed through the engagement with that question.


## Dreaming

![“PuppySlug” - posted by deleted user on /r/creepy with the title “This image was generated by a computer on its own (from a friend working on AI)” on January 10th 2015. 7 days before the Google blog post](6ocuQsZ.jpeg)

My journey into the hype machine of ‘Artificial Intelligence’ started around 2015, when Google released DeepDream[^2] on it's research blog. The developers Alexander Mordvintsev, Michael Tyka and Christopher Olah were initially trying to peek inside of an ‘Artificial Neural Network’ that was ‘trained’ for image classification tasks. They reversed the task of the classifier and instead of identifying what a set of pixels look like the pixel values were changed to look more similar to what the network has ‘learned’. This created images that amplified the textures embedded in their networks transforming the input image into swirls of PuppySlugs.[^3] This imagery that reminded people of bad dreams and hallucinogenic trips was widely shared on the internet which skyrocketed the awareness that ‘Deep Learning’ and biologically inspired computing has come back from it's hibernation during the AI winter.
In 2012 ‘Artificial Neural Networks’ have made an incredible comeback into the field of computer vision. A team around Fei-Fei Li created ImageNet,[^5] a dataset of images organized into categories by anonymous workers from the internet. The ImageNet project held a yearly competition since 2010 and for the first two years the best algorithm was recognizing these images with a 74% accuracy until a ‘Artificial Neural Network’ topped the score with 85% accuracy and woke up the computer vision community to their potential powers. 
With DeepDream this technique became tangible for the general public as it created something to look at, something for humans to empathize with the outputs of an abstract mathematical model. This tendency to anthropomorphize this type of software becomes already clear in the way we use language to describe it. We talk of ‘Neural Networks’ as if they are biological entities, that can be ‘taught’ and which ‘learn’, ‘experience’ and ‘read’ data from the world. This was a turning point that got me and many other artists interested in ‘collaborating’ with seemingly autonomous machines. Machines that we do not have to understand through mathematics but through interpretation and experimentation of ‘their dreams’. Yet, the outputs of DeepDream quickly became kitsch. Google shared the code with an open source license and many people created apps and APIs to generate PuppySlug textures on top of their own images.[^7] As with most other memes on the internet the hype around DeepDream died quickly and by the end of 2015 the world was already saturated with computer generated hallucinogenic images. Other researchers have since picked up[^8] on visualizing nodes in ‘Artificial Neural Networks’ and Alex Mordvintsev—together with his wife—has since become an artist, exhibiting in art fairs.[^9] What stayed was the notion that ‘Artificial Intelligence’ became some form of computational being different from the humans who wrote the code, labeled and sorted the data.


## Terms

I have been deliberately using quotation marks around some of the previous words, as I am critical of the language used when talking of AI systems. In his book *The Artist in the Machine* Arthur I. Miller comes to the conclusion that machines could in fact be seen as creative and will be considered “artists, writers, and musicians in their own right.”[^10] He is using a typical techno-utopian argument that the technology is not quite there yet to *really* be creative, but that it will change in the foreseeable future. At the same time Miller is explaining in great detail the actual creative work of Mordvintsev and his human experience of insomnia to come up[^11] with the generative system for DeepDream and how he shared it with his peers at Google. Alex Mordvintsev was not collaborating with his computer, he created an emergent algorithmic system with it. The software does not hang pictures on exhibition walls, talk to gallerists and has no agency in the process if it were to generate pictures or not. Peter Weibel has formulated it bluntly: Artificial intelligence does not exist. But an ensemble of machines, media, programs, algorithms, hardware, and software has resulted in an extraordinarily large, diverse, and productive field of research that is called AI.[^12] Where Arthur Miller is feeding the narrative of humanoid robots with glowing blue brains[^13], most computer scientists in the field of artificial intelligence today are working in the subdiscipline of ‘Machine Learning’. In the last 10 years the terms have converged in the media landscape, but where intelligence is rejected as a suitcase word with a multiplicity of meanings, ‘Machine Learning’ tries to define the task more narrowly to some form of pattern recognition and extrapolation of existing data. They are using a variety of computational and statistical techniques to form abstract models for domain specific problems. In these fields ‘Artificial Intelligence’ is a buzzword to convince governments and venture capitalists to fund projects. The artist and researcher Francis Hunger has recently shared a list of alternative terms in an attempt to dehumanize our language for AI systems:

1. 'Artificial Intelligence' => ' Automated Pattern Recognition'
2. 'Machine Learning' => 'Machine Conditioning' OR ‘Automated Classification’
3. 'Neural Network' => 'Weighted Network'
4. ‘Deep Learning’ => ‘Deep Conditioning’
5. ‘Neuron’ => ‘Weight’ or ‘Node’ \
 [^14] 

In an accompanying talk[^15] he explains the aim of those terms is to invoke passivity, that we deal with machines and humans set those machines in motion, even when in the end we form ‘human-machine assemblages’. I like many of the proposed terms, even though they are still closely associated with biological operations. The conditioning of machines, for example, reminds me of Pavlov's dog experiments or B.F. Skinner's box to modify pigeon behavior by reinforcement or punishment.[^16] This analogy serves the purpose well of creating an image of the machine as a system that can be controlled by changing the parameters of it's virtual environment. Therefor I will use ‘conditioning’ and ‘weighted networks’, where it is applicable. Using ‘Automated Pattern Recognition’ instead of ‘Artificial Intelligence’, however, becomes too narrow of a definition and is counter intuitive to me, as AI serves exactly the function of being ill-defined. Pei Wang has made a great effort in defining the different strands of AI research and their working definitions.[^17] He clusters them into Structure-AI (recreating the human brain), Behavior-AI (recreating human actions), Capability-AI (domain problem solving), Function-AI (developing cognitive functions) and Principle-AI (finding underlying principles) and comes up with his own working definition to where AI research should be headed and how it can be unified. My initial goal for this thesis was to completely avoid the term ‘Artificial Intelligence’, but as I have already failed in that task and coming up with a less anthropomorphic term does not seem feasible to convey the research in the field and it's media reception, therefore I will keep using the abbreviation AI. Contrary to articles and sci-fi novels I will not use the personification of ‘an AI’, but will rather talk of AI systems, meaning emergent complex programs. Many of the systems today use statistical modeling, yet nobody would ask the question if statistics can be creative, which is why I decided not to engage with philosophical questions of consciousness and creativity. Instead I want to explore how AI systems can include the human knowledge and work that goes into building them, which is challenging the AI ideology of machine autonomy and proposes human-centric goals, rather than building a machine as a goal in itself.


## Getting Started

[[Revision Needed]]

I structured this thesis around 4 chapters, which combine historical, computational and collective knowledge. Starting with a history of talking machines from Wolfgang von Kempelen's speech automatons to digital assistants today, not forgetting that the first computers were women doing calculation. The literal act of speaking to computers sets the base of defining that we are actually talking through computers with other humans. The second chapter deals with the building blocks for complex statistical modeling in AI systems. To make such systems possible, researchers need to create large datasets, often using ethically questionable techniques aggregating data from internet users. I will look closely into the StyleGAN dataset, as it serves a dual purpose, because the model defined another turning point for artists to generate synthetic media. The same is true for GPT-2, a model that can generate coherent looking text based on large amounts of scraped websites and books. In the third chapter I will explore the transformer architecture and how artists are using it to make (non-)sense automatic writing. Lastly I am revisiting collective experiences that I have been organizing with other artists. I had the great pleasure to work with the net culture initiative servus.at to organize the *Silicon Friend Camp* in the Austrian mountains, where we invited 17 artists and researchers for a week long retreat, focusing on human computer conversations. The camp resulted in the exhibition *Camping with Computers* and an online symposium on *Conversations with Computers*.

**Bitte hier erwähnen, dass Arbeiten von anderen Kunstschaffenden in die verschiedenen Kapitel theoretisch eingewebt wurden, da sie dort beispielhaft deine thesen untermauern…**

**Außerdem bitte hier abschließend erwähnen, dass in dieser theoretischen Masterarbeit nur jene Arbeiten von dir präsentiert werden, die direkt mit diesem Inhalt in Verbindung stehen. Alle weiteren Arbeiten, die während deiner Zeit an der Kunstuni entstanden, bitte kurz beschrieben in den Anhang stellen… Danke!**

---

[^1]: The C Programming Language Book defined many standards of programming languages today and how technical descriptions are written. While it focuses on C and the UNIX system, I find this advice from *Chapter 1.1 Getting Started* particularly interesting "On other systems, the rules will be different; check with a local expert." as it describes the social necessity of learning computers specifically.

[^2]: Original Google Blog post. See: [@InceptionismGoingDeeper2015]

[^3]: The initial classifier was conditioned on ImageNet, which contains many images of dog breeds. Therefore DeepDream is biased to generate textures of dog faces.

[^5]: About ImageNet see the website: [@ImageNet]

[^7]: Mike Tyka has compiled a list of projects, released within a month of publishing the source code. See: [@tykaDeepdreamInceptionismRecap]

[^8]: A collection of tools and papers for feature visualization is collected at the github tensorflow/lucid repository. Note that Chris Olah and Alex Mordvintsev are contributors. Even more examples and explanations are collected in the interpretable machine learning book by the statistician Christoph Molnar, who also notes that "Feature visualizations give unique insight into the working of neural networks", but the "visualizations can convey the illusion that we understand what the neural network is doing". [@molnar10LearnedFeaturesa]

[^9]: Interview with Alexander Mordvintsev by artnome. See: [@baileyDeepDreamCreatorUnveils]

[^10]: [@millerArtistMachineWorld2019], p.122

[^11]: Mordvintsev himself was inspired by a previous paper exploring the generative potential of CNNs from Karen Simonyan: Simonyan, Vedaldi, and Zisserman 2014.

[^12]: Translated from the article 'AAA - Art, Algorithmen, Artificial Intelligence' at Kunstforum Bd. 278. See: [@weibelAAA]

[^13]: Referring to typical AI stock images. Alternative imagery is currently built by betterimagesofai.org. See: [@BetterImagesAI]

[^14]: Twitter post by Francis Hunger. See: [@francishungerArtificialIntelligenceAutomated2021]

[^15]: Unhype AI. See: [@hungerTalkUnhypeAI2021]

[^16]: In behaviorist psychology Ivan Pavlov's experiments with dogs is known as ‘classical conditioning’ and B.F. Skinner who experimented on rats and pigeons using lever machines is called ‘operant conditioning’. 

[^17]: ^ [@wangDefiningArtificialIntelligence2019]

