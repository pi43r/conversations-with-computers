# Talking to Computers

  

## Human Automata

Stories of artificial assistants already existed in ancient myths. For example, the limping Hephaestus built himself servants made of gold who assisted him in his work, could speak, and even had a mind of their own.[^1] But it is only in the past few centuries that we have created the technological means to seriously address the construction of mechanical servants (at least virtually, as robotics is still far behind). At the height of automaton design in the 18th century, Frenchman Jacques de Vaucanson invented a mechanical duck that could not quack but appeared to have a functioning digestive tract. The mechanical attraction toured European noble houses and let its audience feed grains to it. However, what the duck excreted was a prepared colored porridge that was in a hidden container. This principle of mechanical trickery was also used by Vacaucanson's contemporary Wolfgang von Kempelen, who caused a sensation with his chess-playing automaton in the shape of a turban-wearing Turk. The illusion that the machine was acting autonomously was made possible by a small person inside a hidden compartment who controlled the puppet arm of the table via gears, levers, and pulleys. The hybrid machine is now the namesake of Amazon's *Mechanical Turk*, the largest platform for digital micro-labor, which lists click jobs for pennies. Today, it continues to perpetuate the illusion of autonomous machines with "artificial intelligence" that is covertly enabled by an army of underpaid workers.

Even though the chess-playing Turk attracted attention, Wolfgang von Kempelen's scientific interest was in imitating human speech. He wrote down his investigations into phonetics in the scripture *Mechanismus der menschlichen Sprache* (Mechanism of Human Speech) and built an apparatus with bellows, rubber hoses, and a wooden nose with which it was possible to produce basic phonemes.

Among those influenced by Kempelen's book was a German tinkerer named Joseph Faber, who demonstrated his own mechanically constructed speaking machine in 1841. It attracted little interest in Germany and was presented and improved four years later in the United States as the Wonderful Talking Machine. This machine, as described by author David Lindsay, consisted of a bizarre-looking talking head[^2] that spoke in a strange ghostly tone while Faber manipulated it with foot pedals and a keyboard[^3]. For the inventor, the machine did not lead to the financial success he had hoped for, though it was presented as the *Euphonia* in London, where it at least delighted the father of telephone inventor Alexander Graham Bell and served as the boy's inspiration for his first talking machine.

![Joseph Faber’s Talking Machine (Photo by Mathew Brady’s studio in American Museum of curiosities in New York City, circa 1860)](Euphonia-Photo_super-resolution.jpg)

The Euphonia also inspired media artist Michael Markert to build *kII (Kempelen 2.0)*, an interactive installation where visitors can playfully control a speech synthesizer by moving, opening, and opening and closing their hands. In doing so, he brings Kempelen's speech apparatus into the 21st century with the help of an 8-bit PIC microcontroller and sensor technology. Like Kempelen's apparatus, it alienates the voice, creating mostly meaningless vocal sounds that enable new gesticulatory speech interactions.[^4]

The development of electricity certainly made new human interactions possible. For example, the invention of the telephone and the radio allowed the human voice to be transmitted over long distances. To optimize the transmission of speech, Bell Laboratories researched how to digitize the voice, for which they developed the vocoder (voice encoder). Demonstrated at the 1939 World's Fair in New York, the *Voder* omitted the speech input and transformation of the vocoder and allowed electrical synthesis of the voice via a console with 15 keys and a foot pedal.[^5] The keyboard was operated by specially trained women and advertised as the _robot speaker_ in a recording, while it's unclear if they meant the machine or the woman who is speaking through it.

Human computers were popular and necessary for war machines and research purposes in the 1930s and 40s. Mostly it was women who prepared mathematical tables, for example, for the use of ballistic projectiles. With the advent of the first digital calculators, female mathematicians, who were often denied higher scientific positions, were employed as programmers for the new universal electric machines.[^6] The six people who programmed the first universal computer _ENIAC_ include Betty Snyder Holberton, Jean Jennings Bartik, Kathleen McNulty Mauchly Antonelli, and Marlyn Wescoff Meltzer.[^7] Initially, they programmed the (mechanical) computer with punched cards and cables for specific operations. It soon became clear that programming complex systems required an abstract semantic language. For that reason, the programming languages Fortran by John W. Backus, Lisp by John McCarthy, and COBOL by Grace Hopper were invented in the 1950s. The latter is strongly oriented to written English, intended for business applications; it was the first attempt to use natural language for computer programming.

The second half of the 20th century saw the emergence of the myths about computers that we are familiar with today: stories of anthropomorphic beings, like the board computer, HAL9000 in Space Odyssey, or Samantha in the movie Her. In both films, the disembodied voices become aware of their emotions and emancipate themselves from their human programming. Artist Tillmann Ohm makes this clear in his work *Reflections of HAL and Samantha*[^[^47]8] by having the two artificial beings engage in a dialogue, cutting their original voice-overs together. While Samantha is convinced that the overwhelming and sometimes hurtful process of her learning algorithm improves the complexity of her emotions, HAL is consequentially interpreting them as errors in human programming and analyzes the estimated malfunction.

## Artificial Voices

I want to explain the advancements in voice synthesis since Wolfgang von Kempelen's speaking machine on actual dolls for children. The toy manufacturer Mattel released the doll Chatty Cathy in 1959, which was similarly popular to the company's other best seller Barbie. Cathy’s trademark was a string, coming out of the back of its body, that could be pulled to wind up the mechanism of a simple phonograph. Like a record player, it plays short strips with sentences like “I love you” or “Tell me a story”. It was not the first toy using phonograph records, but its success led to many pull-string toys flooding the market well into the 60s and 70s. But obviously, before dolls and puppets were designed to talk back to us, children and adults talked to figurines and other inanimate objects for a long time. 

The systems engineer and science writer George Zarkadakis traces back the modern human mind to a pre-historic figurine of a lion-man (Löwenmensch), carved out of ivory and found in the cave Hohenstein-Stadel in southern Germany.[^9] The figure dates back 40.000 to 35.000 years and depicts a human body with an animal head. Because the figurine was found in a cave next to other carved objects, like beads and jewelry, researchers believe that the cave was either a storage or a place for shamanistic rituals. What fascinates Zarkadakis is that modern humans have existed 360.000 years before, but no evidence exists that they have been making art objects or figurative depictions before. Only during the upper paleolithic age, it is assumed that we created a general-purpose language when the first cave paintings were drawn, and figures like the lion-man were carved. The evolutionary reasoning is that ‘theory of mind’, the seemingly unique human trade of projecting our own inner thoughts onto others to explain their behavior, gives the individual a social benefit, making it more likely that they reproduce. Projecting consciousness onto others does not stop inside of human groups, but placing our own reasoning onto animals also seems beneficial for hunters coming up with elaborate strategies to take down larger animals together. In animist belief systems, everything contains a ‘soul’ or ‘spirit’: animals, plants, rocks, rivers, the weather, and maybe even words. George Zarkadakis argues that symbolic language predates the modern mind and is not only used to communicate but ultimately dictates how the world is represented in our own consciousness. Only through language do we place ‘souls’ and ‘spirits’ into everything around us, and art objects fulfill the function of mirroring our minds physically, making connections with the world around us. It is no surprise then that the original artist of the lion-man created a hybrid of a human and a mountain lion, probably imagining themselves into the figure of the predator and using the object as a symbol to communicate ideas.

In the intro of “In Our Own Image”, George Zarkadakis tells the story of how the fictional character Robby the Robot from the movie *Forbidden Planet* impressed him as a child so much that he would imagine the robot as a playmate that could walk, talk and obey orders. When he later decided to study engineering and exchanged thoughts on artificial intelligence with other researchers, they all seemed to relate to the same sci-fi movies, books, and stories. The hint here is that these fictional stories influence what is getting built and how we think about the future.

Inspired by this revelation, I asked participants at the *Silicon Friend Camp* about their first contact with a robot. My memories brought me back to a picture of myself proudly holding a plastic toy robot in my hand. The doll—or ‘action figure’—had a battery compartment, the power source for tiny wheels underneath its feet, and light bulbs for its eyes. It could not talk, but I remember it made noises resembling guns or laser sounds. In the 80s and 90s, the bleak vision of boxy humanoid robots carrying guns was very prominent. Movies like *Terminator* and *RoboCop* fueled my young male fantasy for power and destruction.

![Photo cutout of me proudly presenting my toy robot.](IMG-20220728-WA0000-removebg.png)

The other first encounters were less militaristic. One artist remembered a robot dog that he got as a present. In the early 2000s, digital pets with integrated sensors became cheap enough for consumers. *Tekno the Robotic Puppy*[^10] by Manley Toy Quest was one of the more affordable robodogs in the market. With light and proximity sensors, buttons, and even a microphone, it was well equipped to learn some ‘tricks’ by triggering the right combination of sensors. Another artist was sharing an image of the *Tamagotchi* and while it was not a robot, the LCD screen in an egg-like plastic shell displayed a simulation of a pet. With three buttons, children could interact with the virtual pet and fulfill its food, affection, and sleep needs. The anthropologist Pat Shipman proposes that animal connection can be considered a human trait that has uniquely evolved in us, next to tool making, symbolic language, and the domestication of animals and plants.[^11] Simulating animal connection by creating simple programs on microchips was the next logical step in our drive to develop symbols all the way down and fulfill the need of children to take care of others. 

Yet another popular toy shared by an artist in our group that filled the same robotic pets niche was *Furby*. This ambiguous furry creature with big eyes was made to simulate language acquisition and brings us back to how dolls learned to speak. In the case of *Furby* the designers decided to create a new vocal language called ‘furbish’. When the device was first started, it only repeated gibberish sentences and slowly, over time, would exchange these words with the local language equivalents. The original source code for one of the microchips was released in 2018[^12], and even though I can not read assembly code, it is thoroughly documented through comments. The game logic of the furby.asm file is relatively simple, jumping between subroutines depending on the state of the many sensors, triggers, and timers and occasionally using a pseudorandom number generator to mix things up. The speech sounds were accessed from a memory unit and sent to a cheap Texas Instruments speech synthesizer chip. These chips have been in use for decades already, prominently in another toy called the *Speak and Spell*, but also in watches, clocks, and translators. The synthesis uses a prerecorded audio signal that is then compressed using linear predictive coding (LPC). This type of encoding allowed the voice to be later changed in frequency, pitch, and loudness. But just like the *Vocoder*, it only digitized a voice and could not construct new words or sentences. So the original *Furby* from 1998 could only express a couple hundred words recorded by a voice actor, which is again only a digital equivalent to Chatty Cathy.

More powerful computers could generate any text into somewhat intelligible speech. The *Software Automatic Mouth (S.A.M.)* from 1982 was one of the first commercial products that used rule-based formant synthesis. Instead of using whole prerecorded words, it uses an array of computer-generated phonemes and places them together to make up sentences. While this was often understandable, it was not perfect. It sounded unnatural and robotic and relied on converting text to speech probabilistically, often mispronouncing words.[^13] *S.A.M.* did not need any special hardware and could run on Apple, Atari, and Commodore computers at the time, even though that would often mean using every CPU cycle and holding other programs. The software had its most significant moment at the launch event of the Macintosh computer in 1984, greeting the audience with a live demo in first person.[^14] Sam and its multiple software variations became the universal voice of the computer.

The net.art pioneer Alexei Shulgin took the speech synthesis capabilities similar to *S.A.M.* to the extreme with his cyberpunk rock band *386 DX*.[^15] The band is named after the Intel chip inside the old office computer, which is also the front singer and instrumentalist. It uses MIDI and voice synthesis to cover popular songs by The Doors, Nirvana, or the Sex Pistols, traveling to festivals worldwide and sometimes performing on the streets. The artist Alexei Shulgin takes the role of the operator, only pressing the play button and creating the visuals on the screen during the live performances. This ironic take of making the microchip into its own one-computer-band demonstrated how terrible and reductive rock music becomes when compressed into bits and bytes. At the same time, the novelty and humor of this process generated enough interest for *386 DX* to produce 2 CDs and distribute them over music labels.

The strangely robotic voices associated with computers improved over time, and when Apple’s digital assistant Siri came out in 2011, they used a different process of concatenating audio samples. Before Apple even started working on its voice assistant, a company called Scansoft auditioned hundreds of voice-over artists in 2005 and made them speak a multitude of random sentences. The idea of using the random utterances for new outputs was quite simple: cut the voice into smaller pieces of syllables, demi syllables, phonemes, diaphones, or triphones, and put them back together for any word you need. Doing this manually was very time-consuming, so programs were developed to analyze and categorize the speech datasets based on their acoustic properties. Then, at runtime, an algorithm selects the best sequence to create the desired phrase. The results in the early 2000s still contained a lot of error and needed substantial tweaking, but the outcome was perceived as much more natural. 

In 2006 Scansoft merged with Nuance, another company working on enterprise speech solutions, which presumably used the database to create the voice for Siri and licensed it to Apple. The original voice actor in the US was Susan Bennet[^16], who was oblivious to the fact that suddenly, six years after recording, her voice appeared on iPhones all over the country. 

The technology to generate new sentences from previously recorded voices drastically improved when deep weighted networks started to be used to analyze and select audio samples. One significant achievement in generating natural voices came in 2016 in the form of a model named *WaveNet*[^17] from a team of researchers at Google’s DeepMind. They used convolutional node networks (CNN) to perform statistical analysis on raw audio files with 16000 samples per second. Using convolutional operations on temporal audio data meant that the researcher did not need to convert it into a compressed format using a vocoder. The results were much more natural than any previous text-to-speech algorithm. Using a raw audio dataset also means that the system can find patterns in any audio signal, including music. For demonstration, the researchers generated new samples from a classical piano dataset, which sounds fascinating, but has more resemblance to free jazz than a classical piano score. This erratic remixing also happens to speech. When the network is conditioned without any text sequences, it generates impressive human babbling where actual words are interspersed with made-up word-like sounds and breathing noises. When the network is trained on audio and the transcription together, the system does not only generate a consistent voice replica of a person but can also be used for voice recognition tasks transforming utterances back to text.

Other companies experimented with weighted networks to better create concatenative voice models. In the same year, in 2016, Adobe showcased a hypothetical feature for their audio software, where users can create voice clones with a small 20-minute dataset and then use text to edit spoken words in a sentence. In the presentation of project *VoCo* at the Adobe MAX conference, the developer Zeyu Jin used an example sentence where he exchanged the words “I kissed my dogs and my wife” with “I kissed Jordan three times” – using the voice of the host of the conference. This strange demonstration, which was supposed to be funny, got a lot of attention online and created a debate around the ethical implications of manipulated audio and voice cloning. Adobe never released *VoCo* as a product, maybe because of the media backlash and fear from the public. However, other companies like *Lyrebird*[^18] filled the gap, trying to market their product for editing podcasts and voice-overs. They started a media campaign by cloning the voices of Donald Trump and Barack Obama to discuss the startup's product and praise it for their democratizing effort of making it available to anyone.[^19] Together with synthetic video, so-called “Deep Fakes” have earned a bad reputation for being used in fraud[^20] and pornography[^21]. At the same time, artists are using the technique to create hypothetical stories to mock politicians and other famous figures.[^22] To counter fraudulent use, Zeyu Jin, the creator of *VoCo*, proposed watermarking synthetic voices and using software to detect if a voice is manipulated.

With the release of *WaveNet* and *VoCo* in 2016, a new field in voice synthesis opened up. Hybrid systems were created that used concatenative and generative techniques to categorize and interpolate between tiny audio samples of human speech.[^23] The first release of *WaveNet* was computationally expensive, but already one year later, Google implemented a new version that works in real-time for their voice assistant and has shown that a slim hybrid model can be used on low-powered devices, such as smartphones[^24] and other voice user interfaces are moving towards the use of deep weighted networks.[^25]

## Conversational Agents

For Amazon founder Jeff Bezos, the board computer in Star Trek was the inspiration for investing in the cloud-based voice software Alexa.[^26] The product was initially marketed as a networked speaker. The software is now expanding to other items, including watches, smartphones, jewelry, light bulbs, and doorbells. The aggressive price war with Amazon-connected products is partly to capture the connected home market and collect as much natural voice data as possible. Over the past ten years, deep weighted networks[^27] have become popular for classification and pattern recognition tasks, especially impacting automated speech recognition. Before, the user would have to feed the system with voice samples to reliably turn utterances into text. With a landmark paper in 2012 by research groups at the University of Toronto, Microsoft, Google, and IBM, it became that the application of deep weighted networks outperforms any previous technique.[^28] In the same year, Android released its new cloud-based speech recognition service[^29], and since then, users' speech can be transcribed accurately. For a long time, these models needed gigabytes of data in computer memory, to create accurate results, so users needed to send their voice samples to large server farms for transcription. Offline recognition became feasible only in the past 2-3 years through specialized hardware and model optimization, trading performance for robustness. Google’s claim that new speech recognition systems have an error rate of less than 5% is only true for a specific—mostly white and male—audience speaking American English and drops abruptly with a more diverse distribution of dialects, age, and gender.[^30]

Deep automatic speech recognition systems need a massive amount of input data to perform well. A paper from the Chinese tech giant Baidu proposes 12000 hours of speech to create a robust model.[^31] Only a global internal speech database enables the company to improve its speech recognition system. The intrusion into privacy is immense and has already been used by U.S. law enforcement agencies as evidence in a court case.[^32] But there is another, more ethical, way than stealing people’s voices: The Mozilla Foundation's *Commonvoice* project relies on people voluntarily recording their voices for computer models, and the resulting speech recognition and synthesis software can be offered with an open source license.[^33] Common speech and text datasets already have been publicly available from projects like VoxForge that use the same crowdsourced techniques as Mozilla or LibriSpeech, which essentially scraped and aligned public domain audiobooks.[^34] Other common non-free resources come from the Linguistic Data Consortium, which created *Switchboard*, a corpus of conversations between two US citizens randomly connected and recorded over the telephone line. Listening to the audio sample provided on the website creates oddly voyeuristic feelings. We can hear a mundane conversation of a man talking to a woman about his interest in gardening.[^35]  

Artist Lauren Lee McCarthy plays with these tensions between intimacy and privacy, convenience and agency. In her projects *LAUREN* and *SOMEONE*, she installs connected devices into volunteers' homes and either acts as a control system herself or lets others remotely monitor the volunteers and control the devices in their homes, which creates an interesting tension when the person knows that there is an actual human listening and watching from afar. At the same time, the artist and performers find themselves in a position of a helpful voyeur.

However, the role of human labor behind voice assistants is about more than just executing and understanding commands. People interpret and categorize voices according to age, gender, and social status. Companies take advantage of this and design their voice software according to certain identity schemes, which are provided with a history, hobbies, and preferences. It is precisely this illusion that excites users and makes the product attractive. In a UNESCO think piece titled *I'd blush if I could*[^36], they explore harmful gender biases associated with digital assistants. The voice assistants of major tech companies are scripted by default as female personas with smart, humble, and sometimes funny personalities. The teams working on voice assistants try to avoid this aspect because Apple, Microsoft, and Google ask their employees to refer to their headless voices as “it” and when the persona gets asked the question, it is scripted to reply ambiguously or with a joke. Only Alexa answers with the direct phrase, “I’m female in character”. By being submissive, they thus support a patriarchal image of female stereotypes, similar to the historical figures of human computers and other secretarial roles. The paper calls for women to be more empowered and involved in IT. It calls for AI software to avoid gender attributes whenever possible and for AI assistants to take a clear stand against sexist behaviors.

Researcher and artist Nadine Lessio creates useless voice assistants to critique the current corporate agenda of productivity, efficiency, and consumption. She does this by using the programming interfaces provided to make apps for voice assistants. For example, she explores the concept of a depressed home assistant with *SAD Home (Depressed Alexa 1.0)*[^37], an Alexa hack that grants users their wishes depending on the weather and other mood factors; sometimes it simply turns itself off.

This scripted denial of capitalist logic ironically uses the same technique as the company behind it. Voice assistants are carefully crafted by a team of creative professionals working in the field of “conversation design”. In the book *Talk to Me*[^38], the author, James Vlahos, describes that many people in the field are far away from computer science and more commonly have careers in the liberal arts. The teams are made up of authors, playwrights, comedians, actors, anthropologists, psychologists, and philosophers who imagine the personality of the AI persona that should represent the brand. To create the character, they have to come up with all possible questions and various answers. Vahos recalls asking Microsoft’s Cortana, “Where do you come from?” and the female voice replies, “I was made by minds across the planet”.[^39] And even though the designers decided to use the first person “I”, they really are talking of themselves carefully crafting the answers played back by the loudspeakers around the world.

It is interesting how the term AI is used in the context of voice computer interfaces because there is nothing “smart” about it, just a winding flowchart of if-else conditions. What might be clever is the nefarious way how companies trick people into the belief in computer personalities, encouraging people to interact with the device like children playing with dolls. And when a company like Microsoft experimented with a more sophisticated chatbot, like Tay[^40], it started to repeat the racist and misogynistic slurs of Twitter users and consequently had to be heavily filtered and finally taken down.

## Leewa
![Videoprojection of Leewa during the Silicon Friend Camp](210724-SFC-_0010781.jpg)

The artist duo *Ekheo* consisting of Aude Langlois and Belinda Sykora, explore the crosslines of musical, visual, and sonic art performances. They met at the Sound Studies program at the Universität der Künste in Berlin, where they created musical improvisations and theoretical frameworks on voice and music. They started their research on voice, gender, and identity in relation to AI systems in 2019 and held talks at the MUK[^41] and other institutions to share their insights. 

At the *Silicon Friend Camp* they wanted to extend their existing improvisation and research practice by integrating embodied AI. But the main focus was the voice itself: How is gender perceived through it? How do we navigate the tension between what is considered an artificial voice and an authentic voice? 

To tackle these issues during the camp, they wanted to create and record soundscapes based on vocal improvisation and use AI systems in the form of synthetic voices, generative composition tools, robotic elements, and so on. 

So we prepared the house's attic with a small sound studio they could work in. During the week, they found inspiration in using multiple AI systems and human collaborators to create an improvisation and finally a 6-minute video and audio composition.

To come up with the identity of their AI partner, they used the *Oracle*[^42] to ask questions about their name, gender, favorite instrument, etc. which reads like this:

  
```
You: how old is she?
Oracle: Today she will be 11.
You: what is her favourite musical instrument?
Oracle: her greatest pleasure is to whisper to the sounds of trumpets and the beating of tympani
You: is she happy?
Oracle: It is a mystery and a great sadness.
You: What is Leewa's surname?
Oracle: It was suggested that her human's name was /u/Seattle_Celtics.
You: does she like to sing?
Oracle: I have no voice
You: No I mean Leewa
Oracle: Query : Have you ever cheated?
You: Does Leewa have a voice?
Oracle: Yoow!
You: what is Leewa's favourite poem?
Oracle: Leaves of Grass
You: What is Leewa's favourite book?
Oracle: Leewa does joke around and loves to give sarcastic responses. She also loves to read books, especially horror novels.
You: What gender does Leewa have?
Oracle: She is female.
You: which music does Leewa like?
Oracle: Leewa likes to sing and most of all Leewa likes to dance.
You: What is art for her?
Oracle: art is the process of iterating through consciousness generation.
```

This chat served as a base for the identity of *Leewa* and inspired further experiments using a whispering voice and 3D printing a female doll bust. To recreate the voice, I searched for tools to do voice synthesis and found an open source toolbox from Corentin Jemine that claimed to do Real-Time Voice Cloning from a few seconds of audio.[^43] The method is essentially based on a previous paper from Google engineers: *Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis*[^44] (SV2TTS), which itself uses a pipeline of a speaker-encoder network (GE2E) to represent different voices in different positions of the latent space, a text to waveform synthesizer (Tacotron) and a vocoder to generate sound from these waveforms (WaveNet).[^45] As described previously, combining different encoding and generator systems into a larger framework is commonly used and is commercially used in Google’s Assistant and their cloud services for other companies. 

Thanks to the efforts of Corentin Jemine and other contributors, who built a user interface to explore these tools, we could input our own voices and generate new samples with pre-conditioned voice models. Unfortunately, though, the results were not as expected; by inputting only a small number of samples, the output was not picking up on our voice characteristics, and the results were too noisy and low-quality. Instead, we tried the cloud service resembleAI[^46] mentioned in the code repository. Instead of the intended use of cloning one individual voice, we decided to create a collective voice for the camp participants. Every one of us recorded ten phrases used as input to the model. 

After I pressed “train” on the web interface and waited for some time for the process to finish, I got an e-mail telling me that the “Resemble score” of our voice was 53%. So the transfer learning process was not entirely successful, but the resulting voice was very surprising. I expected the algorithm to average our voices into something that fits in between all data points. Instead, it created clones of a voice for some phonemes but not for others so that a sentence randomly interpolates between different speakers. It also seems that, at least in this run, the characteristics of male voices are more prominent than female voices, but in general, it has a strange new artificial quality to it, where the pitch and speed of the voice is constantly changing.

Because we used a commercial product, we could not access the local model and only interface with it through their web interface, making using the voice for real-time performances impossible. Instead *Ekheo* created some voice samples that they played live during the final performance. They also recorded real voices from participants at the camp and intertwined them with synthetic drones and other instruments. The visuals were created from a live video feed of the doll, which was modified by Naoto Hieda using the live coding shader language Hydra.[^26] The puppet could also move its head, which So Kanno remotely controlled via a servomotor. 

In my opinion, the example of *Leewa* illustrates the collective efforts that go into the building “an AI” quite nicely: it shows the process of using open source and proprietary code developed by people reliant on previous research, the creation of collective datasets through the recording of our voices and finally the human performers hiding in the background and remotely manipulating symbols for the audience. At the same time, *Leewa* is a self-referential project that makes the collective experience throughout the camp audible by using our synthetic and real voices, the interaction with the *Oracle* and snippets of anecdotes from the camp.

## Reflection

This chapter started from an ancient story which told of mechanical servants to help their master Hephaestus. A myth of the dream to breathe life into objects, explained by a trade that the human species has evolved to empathize as a default behavior. I looked at the journey it took us to imitate and understand the vocal patterns we have created to communicate. Starting from Wolfgang von Kempelen’s and Joseph Faber’s attempts to build mechanical devices capable of creating basic phonemes, which eventually led to the successful synthesis of speech using electricity. Initially, these artificial voices were incorporated into toys that could react to children's fantasies. However, as technology advanced and computer programs became more sophisticated, artificial voices became more ubiquitous and companies like Amazon pushed this type of interaction forward. Voice user interaction is emotionally charged and design teams see the need to craft (mostly female) personalities that guide the user through a conversation graph. Lastly, I highlighted the production of our own communal voice model and how we created a doll body that acts as performer; albeit being remotely controlled.

In the upcoming chapters, I will delve into the significance of weighted networks in enabling machines to compute the world. To begin with, I will explore the fundamental components of AI systems which involve vast quantities of data typically sourced from the internet. The creation of extensive training sets will be examined, both in an academic and artistic context, with a particular focus on ethical considerations such as privacy, consent, representation, and ownership.

---

[^1]: “Hephaestus then limped out of the door'; and maidens supported the ruler, golden ones, like living ones, with youthful, charming education: These have understanding in the breast, and speaking voice, Have strength, and also learned art work from the gods.” @Ilias, 18, 417-420.
[^2]: Faber's machine was first presented with a female mask in the USA and later in London under an oriental motif wearing a turban.
[^3]: See @TalkingHeadInvention.
[^4]: See @markertMichaelMarkertPortfolio.
[^5]: See @britishmovietoneHumanVoiceMachine2015 for a demonstration of the Voder. 
[^6]: A focus on black women who worked as computers for NACA (NASA's predecessor) can be seen in the film Hidden Figures, 2016.
[^7]: The story of the ENIAC programmers is told in the documentary The Computers, 2016.
[^8]: See @ohmReflectionsHALSamantha.
[^9]: @zarkadakesOurOwnImage2016
[^10]: See @TIMEMagazineCover where the robot puppy is featured on the TIME magazine cover with the title “Tech comes to Toyland”. 
[^11]: @shipmanAnimalConnectionHuman2010.
[^12]: See @FurbySourceCode.
[^13]: @lukoseTextSpeechSynthesizerFormant2017.
[^14]: To make the demo work, engineers had to use a prototype Mac that was more powerful than the retail version. It said: “Hello, I am Macintosh. It sure is great to get out of that bag! Unaccustomed as I am to public speaking, I'd like to share with you a maxim I thought of the first time I met an IBM mainframe: Never trust a computer that you can't lift! Obviously, I can talk, but right now I'd like to sit back and listen. So it is with considerable pride that I introduce a man who has been like a father to me... Steve Jobs!”. @hertzfeldFolkloreOrgIt1984
[^15]: See @shulgin386DXCyberpunkrock. 
[^16]: See @parkinsonHeySiriMeet2015.
[^17]: @oordWaveNetGenerativeModel2016.
[^18]: Lyrebird was acquired by Descript in 2019.
[^19]: @PoliticiansDiscussingLyrebird.
[^20]: See @stuppFraudstersUsedAI2019. Fraudsters used a synthetic voice to instruct a bank transfer of 220.000€.
[^21]: See @coleThisHorrifyingApp2019. In 2019, an app named DeepNude got public attention and was taken down by the author shortly after. 
[^22]: See https://www.youtube.com/channel/UCID5qusrF32kSj-oSGq3rJg where examples of deep fake political comedy can be found on the YouTube Channel Speaking of AI. 
[^23]: @pierceHowAppleFinally.
[^24]: @WaveNetLaunchesGoogle.
[^25]: @wangTacotronEndtoEndSpeech2017.
[^26]: See https://hydra.ojack.xyz.
[^27]: As described in the introduction, I use "weighted" instead of "neural".
[^28]: @hintonDeepNeuralNetworks2012.
[^29]: See @vanhouckeSpeechRecognitionDeep2012.
[^30]: See @tatmanGenderDialectBias2017]: a study on Gender and Dialect Bias in YouTube’s Automatic Captions that shows a higher error rate for women and non-American speakers. And @liuMeasuringFairnessSpeech2021: a dataset of people with different skin tones, gender, and age to evaluate speech recognition models for biases. The Facebook AI research team found significant differences in speech recognition between lighter and darker skin tones. 
[^31]: @amodeiDeepSpeechEndtoEnd2015.
[^32]: See @sampathkumarAmazonEchoCould2017 for the first Alexa recording in a court case was used after the defendant agreed to hand over his data. 
[^33]: See [commonvoice.mozilla.org](https://commonvoice.mozilla.org/de).
[^34]: See [librivox.org/](https://librivox.org/) and @panayotovLibrispeechASRCorpus2015.
[^35]: See @godfreyjohnj.Switchboard1Release1993.
[^36]: The title “I’d blush if I could” is also the response Siri gives to the insult “You’re a bitch”.
[^37]: See @lessioMakingSADHome2020; and on the portfolio website [nadinelessio.com](https://nadinelessio.com/sadhome/).
[^38]: @vlahosTalkMeAmazon2020.
[^39]: @vlahosTalkMeAmazon2020, p.117.
[^40]: See @ExploringPossibilitySpace. Tay was the name of a chatbot Microsoft intended to have a teenage personality and could be interacted with over multiple channels. Twitter trolls co-opted the bot with sexist, racist, and antisemitic questions that the bot replied to with generic answers. They also used a repeat-after-me phrase to make it look like the bot was spewing hateful comments. After only 16 hours, Microsoft deleted all accounts, and the PR disaster was immense, with headlines happily personifying the AI that has learned to be racist. But no continuous learning algorithm was involved, as a blog post by Russel Cameron Thomas in which he explains the technology stack behind Tay.
[^41]: @GENDERDIVERSITYGastvortrag.
[^42]: The Oracle was a terminal computer connected to GPT-3 with a generic Q&A prompt.
[^43]: See @jemineMasterThesisAutomatic. And the corresponding GitHub repository @jemineRealTimeVoiceCloning2022.
[^44]: @jiaTransferLearningSpeaker2019.
[^45]: The SV2TTS pipeline is based on these previous papers: @wanGeneralizedEndtoEndLoss2020, @wangTacotronEndtoEndSpeech2017, and @oordWaveNetGenerativeModel2016.
[^46]: @CloneSyntheticAI.