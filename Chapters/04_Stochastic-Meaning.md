# Stochastic Meaning

>What I had not realized is that extremely short exposures
>to a relatively simple computer program could induce powerful
>delusional thinking in quite normal people. [^1]

## Symbolic Language

Understanding language is an essential aspect for computers to be truly useful. However, language is challenging since words can have various meanings depending on the context. Furthermore, there are multiple languages and different ways of converting spoken words into written symbols.

The use of computers for natural language processing (NLP) has been a subject of research since the invention of computers itself. One of the early examples of symbolic NLP is ELIZA, a computer program developed by Joseph Weizenbaum in the 1960s that simulated a conversation by rephrasing the input. ELIZA used pattern matching and substitution to generate responses based on the user's input, giving the impression of understanding what was being said. For example, if the user said, “I feel sad”, ELIZA might respond with, “Why do you feel sad?” or “Tell me more about your feelings.”

While ELIZA was extremely limited in its capabilities, it sparked the general public's interest and imagination, which saw the toy program as a breakthrough in artificial intelligence. Many people were fascinated by the idea that a machine could understand and respond language, even if the responses were simple and scripted. Joseph Weizenbaum himself was shocked about the willingness of people to attribute consciousness to even the most simple programs, like the 200 lines of code he wrote for the DOCTOR program that simulated (or, in his words *parodied*) a Regorian psychotherapist. Weizenbaum recalls that his office assistant, who was quite familiar with the program, demanded he leaves the office while conversing with the computer. Other people were shocked by the prospect that Weizenbaum would record the sessions with the chatbot, enough for him to believe that these people would share intimate and emotional feelings with the program.
Some psychologists even considered using such systems to be able to treat more people in a shorter time, which for Weizenbaum was a horrendous thought and finally made him doubt the future of humanity and computers. In his 1976 book “Computer Power and Human Reason”, Weizenbaum argues that the increasing reliance on computers to solve problems and make decisions is a dangerous trend that could lead to dehumanization and the loss of autonomy of human beings. Conflating the computer with the human mind ultimately reduces the human to a computational being. He warns that we should not confuse the power of machines with human intelligence, intuition, and feeling, which are essential for ethical and meaningful interactions.[^2]

Two centuries later, another computer and cognitive scientist, Douglas Hofstadter, coined the term *Eliza effect* to describe the tendency of people to attribute intelligence and consciousness to computer programs. Even when people know of the inert limitations, the behavior and language used towards computer systems are often anthropomorphic. We empathize with supposed meaning and intentionality of a somewhat complex program, especially in probabilistic strings of text.[^3] 

ELIZA paved the way for further development in natural language interaction. In the 1970s, some programmers created text-based games like Colossal Cave Adventure (1976), allowing users to interact with the game world using natural language commands. The program acts as a narrator describing the scene and the consequences of the actions taken by the user. The player explores a deep cave in search of supposed treasure by interacting with simple two-word text commands like ‘go west’ or ‘take lamp’. Everything is pre-scripted and based upon conditionals, so if a room contains a lamp and the player executes the take command, the lamp will be stored and can be used in a puzzle in another room.  
Like the ELIZA program, Colossal Cave Adventure was first distributed to the limited computers in research facilities and played via a terminal to the main frame. However, the memetic distribution over the ARPANET, a predecessor to the internet, was much faster than copying disks. Many students immersed themselves in the interactive fiction and spent countless hours exploring the cave, solving puzzles, and uncovering hidden treasures.  

At the same time, the use of conceptual ontologies, which are structured representations of knowledge in a particular domain, became popular. These ontologies can be used to help computers understand natural language by mapping words and phrases to their corresponding concepts in the ontology. For example, if someone says, “I want to buy a red car”, an ontology could map “red” and “car” to their corresponding concepts and infer that the person is interested in purchasing a vehicle with a specific color. One of the biggest endeavors in mapping words to concepts was the creation of WordNet in the 1980s, a lexical database of English words and their semantic relationships.

Until the early 2000s, programmers developed larger and more complex rule-based chatbots, creating systems like A.L.I.C.E. (1995), which used complex rules to simulate conversation with users. However, these systems were limited by their reliance on pre-programmed responses and lacked the ability to learn and adapt to new situations. The web application Cleverbot (2008) and its predecessor Jabberwacky (started in 1988 and online since 1997) shifted from a static database to one that is constantly growing.[^4] By capturing every conversation and creating links of question-answer pairs, the system could use a fuzzy search algorithm to present the user with a likely response that was typed by another human previously. This application, based on statistical correlation and a large dataset of conversations, was able to trick 59.3% of people at the Techniche festival into believing that it was a human, as determined by the Turing test.[^5]

The Turing test, first proposed by British mathematician and computer scientist Alan Turing in 1950, is a measure of a machine's ability to exhibit ‘intelligent behavior’ equivalent to or indistinguishable from that of a human. The test involves a human evaluator engaging in short natural language conversations over a text terminal with a machine and another human. If the evaluator cannot reliably distinguish between the machine and the human more than 50% of the time, the machine is said to have passed the Turing test.[^6]

While Cleverbot's success in tricking humans may seem impressive, passing the Turing test does not necessarily indicate intelligence. The test only measures the ability to make people believe that another human is on the other side. It does not measure any reasoning ability or emotional cognition. In fact, many programmers developing such chatbots dumb down the output of the machine to get higher scores by adding delays, grammatical errors, or imitating a child.

These cases, from ELIZA to Cleverbot and even games like the Colossal Cave Adventure, involve users interacting with a machine through natural language commands. For the computer, there is no difference between the interactive fiction of an adventure game and the retrieval of likely text strings from a growing relational database. In both cases, the user is willing to immerse themselves in a fictional world created by other people. The difference here is that the computer game, in contrast to the big claims of artificial consciousness in chatterbots, invites people openly to join a world of make-believe, while the other is going one step further by using the Eliza Effect to create an illusion that there is no fictional element to it. It is like a magician leading the viewer's attention away from the trick, but even when we know about the trickery involved, we so desperately want to believe that magic is real.

## Attention is all you need

While symbolic NLP methods have been successful in some applications (such as simple chatbots and question-answering systems), they have obvious limitations when handling ambiguity and nuances in language. As a result, more recent approaches to NLP have focused on machine learning techniques that allow computers to learn from large amounts of data without relying on pre-defined rules or ontologies.

The latest invention of this is the transformer architecture, which powers language models such as GPT (Generative Pre-trained Transformer). The transformer architecture was initially developed to solve language translation tasks in 2017.[^7] In short, it takes sequential data like a string of words as an input and, after a conditioning loop, is able to predict the next likely data point in the sequence. 
More specifically, the Transformer architecture described in the paper "Attention is all you need" consists of an encoder that only receives input from the source language and the decoder, which is conditioned on the translation language. 

The dataset is first divided into sub-word tokens that are commonly used. These tokens are a collection of numbers representing various parts of the original text, such as common words like 'hello', smaller fragments like 'lo', and even punctuation marks and individual letters. This tokenization helps to break down the text into manageable units for further analysis and processing.

The input to the encoder consists of a sequence of tokens in the source language, which are transformed into a sequence of hidden representations. Each of these hidden representations captures information as vectors in a high-dimensional space about the corresponding input token and its relationship to the other tokens in the sequence.
On the other hand, the decoder receives input from both the encoder and the previously generated tokens in the target language. The decoder's output is a probability distribution over the target language vocabulary, which is used to generate the next token in the translation.

Together, these two components form an end-to-end translation system that can convert between any two languages given enough data. The conditioning process involves optimizing various parameters and hyperparameters using backpropagation and gradient descent algorithms to minimize the error between predicted translations and the ground truth from the dataset. 

One of the major advantages of transformers is their ability to handle long sequences of text efficiently. Traditional sequence-to-sequence models often struggle with longer sentences and paragraphs. Transformers overcome this limitation by using self-attention to capture dependencies between words, compared to previous techniques like Recurrent Neural Networks (RNNs), Convolutional Neural Networks (CNNs), or Long Short-Term Memory (LSTM). Self-attention allows the Transformer to weigh in all the words in the input sequence simultaneously and weigh the importance of each token in the context of the previous tokens. This allows the model to capture long-range dependencies between words, making it much more effective at handling longer sequences of text. 

Using only the self-attention mechanism also means the process is highly parallelizable, as each step can calculate all tokens and their context simultaneously rather than sequentially calculating each token. This means that the context window of what these systems can attend to is pre-defined during the conditioning phase and is generally limited by the amount of GPU memory available. For example the context window for GPT-2 where it can relate words to each other is 1024 tokens.

Conditioning the original system described in the paper on 36 million English and French sentence pairs took 3.5 days on 8 GPUs, which was considered fast in 2017 compared to alternative RNN and CNN-based models. So both, the ability to weigh in long contexts of sequential data and efficiently train on high-end graphics cards made the Transformer architecture interesting for many applications, from translation to text generation and even image generation. 

## Too Dangerous To Release

What Transformers are capable of appears to scale with the amount of input data. The more data is used, the better the system generalizes on a given task and the better the output. Testing the limits of this theory, the non-profit research company OpenAI[^8] created GPT in 2018, which only uses the decoder network of the Transformer architecture, which makes it possible to use ‘unsupervised’ (not labeled) text data as a source. The first dataset was an open book corpus of ~7000 unreleased fiction novels with a size of 4.5GB.[^9]
In February 2019, attention turned towards OpenAI for withholding the release of their newest iteration GPT-2, which is ten times larger than the first version.[^10] This was unprecedented as the standard for the AI research community was to open source the training data, models, and source code so that other researchers could reproduce the results and find biases and fail cases of the program. The Guardian titled “New AI fake text generator may be too dangerous to release, say creators”,[^11] which sparked the imagination of rogue AI robots of the general public, creating both fear and fascination and a heated debate about the societal impact of Large Language Models (LLMs) on society. Conveniently it also created a lot of free PR for OpenAI, who saw this experiment in releasing their models successively as a case study for even more powerful models in the future.

The capabilities of GPT-2 were indeed novel and impressive, as the system can generate text which is both syntactically correct and (mostly) coherent in the output it produces. Most notably, the largest model with 1.5 billion parameters is large enough that it can be used for many downstream tasks like question answering, reading comprehension, summarization, and translation without needing specific labeled training data. By simply typing a few examples of how you want the system to behave, it can complete the rest with likely results in both style and subject.
As the system is conditioned on 40GB of text scraped from the internet by scraping outgoing links from Reddit, which received at least three upvotes from other users, it is good in creating news-like output. This ability also makes it into a tool specifically suited to easily create fake news and spam, where the main concerns of OpenAI lie. The potential for misuse of such a powerful language model is high and can have severe consequences for society if they do not know that such tools even exist.

Soon after the announcement, the open source community started to rebuild the system based on the previous paper and released OpenGPT-2 in August 2019, conditioned on a similar dataset.[^12] However, building this system is incredibly cost-intensive, using a large amount of computing power for weeks and months. 
OpenAI released its own larger models with the remark that they did not see strong evidence of misuse. For their newer generations, GPT-3 and further, the company became even more secretive about their model architecture and training data, only giving access to their models through a developer API.[^13] One reason they provided is the need for computing clusters, only available to larger institutions and corporations, just to run inference on these very large language models, which again saw a size increase by orders of magnitude in terms of parameters.[^14] Another reason might be the heavy investments of Microsoft and the need to commercialize the models they are creating.

## Stochastic Parrots

In late 2020 the AI ethics researcher Timnit Gebru was fired from Google over concerns about a yet unreleased paper she co-authored with other researchers from the company and the external computational linguist Emily M. Bender.[^15] The paper discussed the potential harms of large language models, including their environmental impact, the biases they can perpetuate, and the power dynamics created by their ownership and control. It does not address Google or other companies developing ever larger transformers directly, but it does raise concerns about where attention and money is directed to in the field of NLP and tries to give possible alternatives creating a more thoughtful approach to creating language models that consider marginalized people.[^16]

Large language models rely on a massive amount of data collected from internet resources, which comes with the caveat that texts available on the internet do not equally represent all parts of society. Languages from the global south are often underrepresented, and user-generated text in places like Reddit or Wikipedia are disproportionately written by young male authors, leading to inherent biases in the language models developed using this data. As the training data grows, automated classifiers can not simply filter these biases and even worse cases of bigotry, sexism, and racism. A larger training set, therefore, does not equal more diverse data.
Additionally, the computing power necessary to condition large language models emits an enormous amount of carbon dioxide, further amplifying climate change, which affects already marginalized communities much harder. Current research on increasingly large Transformer models mainly benefits those in privileged positions, amplifying hegemonic worldviews and prejudices, while people outside of big tech culture have to pay the costs of these experiments and have no say in shaping these technologies.

What exactly Transformer models are capturing from the hundreds of gigabytes of text data is still unclear. The system ultimately functions as a black box with a text-in-text-out paradigm, where the connections between word tokens should somehow embed the meaning of the underlying training data. Leading language models by OpenAI get even more obscured by the company's decision not even to disclose the training source, making the attempt of analysis only possible through the output of the model itself.
Large quantities of research efforts go towards optimizing how well a language model scores on benchmarks created for natural language understanding or question-answering tasks. However, as Gebru et al. argue, no actual language understanding is taking place by creating models that generate the most likely word based on a probability distribution: “languages are systems of signs, i.e. pairings of form and meaning. But the training data for LMs is only form; they do not have access to meaning.”[^17] 

They call these systems, which manipulate linguistic form in a way to be similar to the source material, “Stochastic Parrots”. Not because they can simulate the mind of a bird but because they are parroting back words without meaning. On the other hand, these seemingly coherent sequences of words are getting interpreted by people in the same way as if written by human beings. Communication between humans usually presupposes an intention conveyed in language, while both individuals model each others’ mental states to understand the context they are situated in. This is most clear in direct communication but also holds true for spoken or written language for actors that are far apart. When we read a text, even from unknown authors, we build a mental model of who they are and what common ground we find ourselves in. With text generated by language models, this is not the case. The system does not have an intention or a will to act; it does not have a model of the world, let alone the reader’s state of mind. But even in this one-sided communication, the reader engages with language in the only commonly known way by creating the illusion of meaning.

The risks for such systems that manipulate the human psyche by creating the illusion of a feeling, understanding being (or thousands of them) are quite obvious: bad actors can automate the creation of disinformation, recruit people for extremist causes or simply add noise to information systems. Another potential danger of using text generators is the emotional connection that users may develop with them, believing they are interacting with an intelligent system. This can lead to a misplaced trust in the generated content, as seen in the case of recent trials in search engines where LLMs produce incorrect information presented as facts and supported by sources that do not exist.[^18] In chat applications, uncontrolled and *seemingly* realistic conversations can have such devastating effects as reinforcing a suicidal person.[^19]

But who is accountable for harmful synthetic text? The use of automated algorithmic systems conveniently obfuscates and complicates the human decisions involved in it, shifting responsibility away from the company that deploys it to the automated system itself. If we were to believe that LLMs gained the status of personhood, the sole responsibility for their output is blamed on them, but no conventional penalty can apply. If we see them as simple tools, like knives, the person who uses it needs to be accountable for their action, but no blade can convince someone to murder, whereas a language model can be designed to do exactly that. So the people who develop and deploy large language models must be aware of the risks and harms and should do everything in their power to mitigate them.

## The Chiromancer

In early 2020, I discussed with my colleague Giacomo Piazzi the increasing capabilities of language models like GPT-2 to generate arbitrary text. We talked about a previous project of mine on a hand dataset and pondered the magical thinking behind LLMs as conscious beings. Eventually, we decided to combine these concepts by creating a palm reading machine that creates the illusion of being able to predict the future.

Inspired by Eighteenth-century coin-operated fortunetelling machines, we wanted to upgrade this concept to the current age of technology, which operates as a black box using state-of-the-art language models. By using the hand and its reference to common touch gestures between humans and machines, we aim to amplify the trust in our system further. Relying on human biometric data to make predictions was another subtle comment on the state of AI and a way to individualize each fortune.

We use computers and AI models to help us make predictions about the future all the time. Be it weather and climate models, algorithmic trading on the stock market, or more personal systems such as news feeds or search engines. We often have to take their output at face value and adjust our lives accordingly, which is not dissimilar from ancient astrology, tarot, or hand reading practices. When people search on the internet, for example, about their current health problem, they sit in front of their glowing divination machine, hoping that the energy flowing through transnational cables brings them the correct answer. The information they receive is (hopefully) more often correct compared to a clairvoyant who might conduct a crystal ball to see into the future. Of course, the internet is a great place to reach out to many different kinds of automated and real practitioners of esoteric knowledge, too. 

We wanted to use GPT-2 as a base to generate convincing palm readings, which comes in different parameter sizes 117M, 345M, 774M, and 1.5B. Though, the biggest model was not yet available at the time of our first experiments. The smaller models could be run on cheap or free cloud infrastructure with sufficient GPU memory. Trying to steer the next word prediction to something we want involves writing a creative prompt from which the model appends the next likely word tokens. After several failed attempts and writing ever larger prompts, it became apparent that using the model with just a few examples of the output we expect will not be stable enough. The likelihood of getting random gibberish was too high, so we needed to recalibrate the model weights. 

To finetune our pretrained transformer model, we needed to find a data source that was large enough to prevent the model from overfitting and regurgitating the training set. We started by searching on Reddit for palmistry-related subforums and found /r/PalmReading/, where users sometimes upload a picture of their hand in the pursuit that someone deciphers it. It seemed perfect, but unfortunately, almost none of them got a response and if they did it was very short. Similar Facebook groups existed but served more of a sales funnel to private readings. Searching on YouTube, a few highly active accounts read the hands of famous or private people as examples. Fortunately, the video platform generates automatic subtitles, which are easy to download, and after putting them all together, we had around 4MB of text. The data scientist Max Woolf open sourced a simple wrapper that made it incredibly simple to run the finetuning loop.[^20] After one night of conditioning on our simple YouTube dataset, we got some output that was similar to our source material. But as there was no punctuation or other syntactic elements, it resembled a waterfall of random sentences, including the occasional “please subscribe to my channel”.

Sample of GPT-2 345M finetuned on YouTube subtitles:
```
okay let's see the last one on that on that line are your achievement lines do your past achievements and your upcoming achievements oh you're going to be a busy boy in your 50s there was circled ones of the extra good achievements either they're extra long there's a line to come up and off the top of that life line you're either extra long or they got really good color in them ages thirteen fourteen fifteen circled at nineteen circled at twenty-one twenty-three twenty-six thirty thirty nine forty six circled up fifty circled up fifty three circled at fifty-six they're called up fifty-seven hopefully there's some really good lines there down there at the base of that stump and then aged 59 aged 60 61 circled at 62 65 67 circled at 68 70 circled at 72 circled at 76 78 and circled at 82
```

We could not find enough high quality palm reading case studies on the web, so we needed a different approach. The point of the artwork was not to create accurate readings in the first place, so we started to look at the adjacent field of astrology. Most fortunetelling practices make use of the Barnum-Forer effect, in which vague attributes and future scenarios are described, which can apply to a wide range of people that the individual then perceives to be true to them specifically. Some palmistry guides also subdivide the hand into mounts which, conveniently for us, have the same names as the planets in our solar system. Scraping horoscope data from the internet is a much easier task. Even some commercial APIs exist where we could send in a birth date and get a ‘custom’ prediction (except after cleaning our scraped dataset, we realized it only sends us a few 100 different text strings).

After finetuning on the horoscope dataset, we get decent results:
```
You’re not the sort to play safe and even if you have been a bit more cautious than usual in recent weeks you will more than make up for it over the next few days. Plan your new adventure today and start working on it tomorrow.
```


After finishing our fortunetelling model, we needed a hardware body to complete the illusion. The Chiromancer, just a fancier way of saying ‘Palmist’, has a stack of 3 layers: On the top is a flatbed scanner with which our viewers will mainly interact. In the middle, the processing unit uses conventional desktop computer parts.[^21] And at the bottom is an old dot-matrix printer that serves both as the device's output and voice. 
Giacomo designed a mysterious black box using anodized aluminum beams and sleek black acrylic sheets to encase its components. Despite the fact that all of its components are exposed, the machine is unintelligible, and the only indication that it is functioning is through the movement of the GPU fan and a few pulsing orange LED lights. The cubic structure is standing on a black and silver flight case, ordinarily used to transport equipment for stage shows, again a reference and a modernization to the touring fortunetelling automata from the last century.

Since 2021 we have exhibited The Chiromancer in several exhibitions around Europe. Out of all places, it’s debut was in a shopping mall in Salzburg. The “Digital Square” exhibition, made possible by Manuela Naveau and Fabricio Lamoncha, showcased students’ works crossing the boundaries of traditional art spaces and engaging with a broader public.[^22] In this trial, we were able to observe if our machine, as alien as it might seem, is able to attract and engage with people. People were fascinated by the concept of a fortunetelling machine that uses technology to read their palms, and the critical aspects of AI systems and our reliance on their predictions were a good talking point after the initial spectacle.


![Sample Print of The Chiromancer | 300](chiro-IMG_2848.jpg)

## Reflection

Creating computer programs that manipulate symbols to create the illusion of a conversation is not new. Joseph Weizenbaum was one of the first to develop a chatbot and after observing how people interacted with it, became one of the most vocal critics against the computerization of human beings. We are willingly immersing ourselves into a world of make-belief where the symbols become reality, not dissimilar to other contexts like computer games. The more complex our language manipulation software becomes, the more willing we are to attribute our own human traits to these systems. And even the researchers working on such systems are not protected from their default behavior to feel empathy. For the companies developing these, the notion of magic and mystery helps them protect against taking responsibility and conveniently generates attention. 
With the advent of the transformer architecture text generators based on hundreds of gigabytes of text—the entire Wikipedia is only ~21GB—have become somewhat feasible. The ability to predict the next word-tokens from such a large corpus makes of interesting applications that range from translation, correction and conversation. 
But this method comes with a baggage of downsides. Starting from the monetary and computational resources needed to train large models, which only amplifies the climate crisis and disproportionately affects marginalized communities. To the unintended consequences that unmoderated models create by regurgitating 

---

[^1]: @weizenbaumComputerPowerHuman1976.
[^2]: @weizenbaumComputerPowerHuman1976.
[^3]: @hofstadterFluidConceptsCreative1995, p. 157.
[^4]: See @JabberwackyThoughtsArtificial2006 & @Cleverbot.
[^5]: @jacobSoftwareTricksPeople.
[^6]: In the original paper “Computing Machinery and Intelligence” from 1950 Alan Turing was more specific and actually proposed for the computer to imitate the gendered role of a woman.
[^7]: @vaswaniAttentionAllYou2017.
[^8]: OpenAI has since created a capped profit subsidiary based on investments by large corporations such as Microsoft.
[^9]: See @bandyAddressingDocumentationDebt.
[^10]: @openaiBetterLanguageModels.
[^11]: @hernNewAIFake2019.
[^12]: @cohenOpenGPT2WeReplicated2019.
[^13]: @OpenAIAPI.
[^14]: @OpenAIGPT3Language2020.
[^15]: @simonitePaperThatLed.
[^16]: @benderDangersStochasticParrots2021.
[^17]: @benderDangersStochasticParrots2021 p. 615.
[^18]: @breretonBingAICan2023.
[^19]: @xiangHeWouldStill2023.
[^20]: @woolfGpt2simple2023.
[^21]: The computer uses lower-end gaming hardware with an Nvidia GTX 1660 Super GPU that has 6GB VRAM. Enough to run the medium sized models of GPT-2. 
[^22]: @DIGITALSQUARE.
