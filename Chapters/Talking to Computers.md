# Talking to Computers

Female voices are spreading into living rooms and kitchens, on laptops, smartphones and small round speakers. Networked with computing machines around the world, they wait for their names to be spoken so that they can send your command to a far off data center and analyze it. If the transcription does not succeed algorithmically, sometimes a human takes over the task of understanding. This person listens to snippets of conversations for hours, anonymized[^1] of course, and translates speech into text so that the machine recognizes the correct patterns in the recording the next time it tries. If the transcription is successful, the server sends the response back to the machine, which plays it in the friendly tone of a synthetic female voice.


## Human Automata

Stories of artificial assistants already existed in ancient myths. For example, the limping Hephaestus built himself servants made of gold who assisted him in his work, could speak, and even had a mind of their own.[^2] But it is only in the past few centuries that we have created the technological means to seriously address the construction of mechanical servants (at least virtually, as robotics is still far behind). At the height of automaton design in the 18th century, Frenchman Jacques de Vaucanson invented a mechanical duck that could not quack but appeared to have a functioning digestive tract. The mechanical attraction toured European noble houses and let its audience feed grains to it. However, what the duck excreted was a prepared colored porridge that was in a hidden container. This principle of mechanical trickery was also used by Vacaucanson's contemporary Wolfgang von Kempelen, who caused a sensation with his chess-playing automaton in the shape of a turban-wearing Turk. The illusion that the machine was acting autonomously was made possible by a small person inside a hidden compartment who controlled the puppet arm of the table via gears, levers, and pulleys. The hybrid machine is now the namesake of Amazon's *Mechanical Turk*, the largest platform for digital micro-labor, which lists click jobs for pennies. Today, it continues to perpetuate the illusion of autonomous machines with "artificial intelligence" that is covertly enabled by an army of underpaid workers.

Even though the chess playing Turk attracted attention, Wolfgang von Kempelen's scientific interest was in imitating human speech. He wrote down his investigations into phonetics in the work *Mechanismus der menschlichen Sprache* (Mechanism of Human Speech) and built an apparatus with a bellows, rubber hose and a wooden nose with which it was possible to produce basic phonemes.

Among those influenced by Kempelen's book was a German tinkerer named Joseph Faber, who demonstrated his own mechanically constructed speaking machine in 1841. This attracted little interest in Germany and was presented and improved four years later in the United States as the Wonderful Talking Machine. This machine, as described by author David Lindsay, consisted of a bizarre-looking talking head[^3] that spoke in a strange ghostly tone while Faber manipulated it with foot pedals and a keyboard[^4]. For the inventor, the machine did not lead to the financial success he had hoped for, though it was presented as the *Euphonia* in London, where it at least delighted the father of telephone inventor Alexander Graham Bell and served as the boy's inspiration for his first talking machine.

For the exhibition Mensch-\[in der]-Maschine at the ZKM (Center for Art and Technology Karlsruhe), media artist Michael Markert built *kII (Kempelen 2.0)*, an interactive installation in which visitors can playfully control a speech synthesizer by moving, opening and closing their hands.[^5] In doing so, he brings Kempelen's speech apparatus into the 21st century with the help of an 8-bit PIC microcontroller and sensor technology. Like Kempelen's apparatus it alienates the voice in such a way that it creates mostly meaningless vocal sounds that enable new gesticulatory speech interactions.

The development of electricity certainly made new human-machine interactions possible. For example, the invention of the telephone and radio allowed the human voice to be transmitted over long distances. To optimize the transmission of speech, Bell Laboratories researched how to digitize the voice, for which they developed the vocoder (voice encoder). Demonstrated at the 1939 World's Fair in New York, the *Voder* omitted the speech input and transformation of the vocoder and allowed electrical synthesis of the voice via a console with 15 keys and a foot pedal.[^6] The keyboard was operated by specially trained women, and in a recording advertised as the robot speaker, while it's unclear if they mean the machine or the woman, who is speaking through it.

Human computers were popular and necessary for war machines and research purposes in the 1930s and 40s. Mostly it was women who prepared mathematical tables, for example, for the use of ballistic projectiles. With the advent of the first digital calculators, female mathematicians, who were often denied higher scientific positions, were employed as programmers for the new universal electric machines.[^7] The 6 people who programmed the first universal computer ENIAC include Betty Snyder Holberton, Jean Jennings Bartik, Kathleen McNulty Mauchly Antonelli and Marlyn Wescoff Meltzer.[^8] Initially, the (mechanical) computer was programmed with punched cards and cables for specific operations. It soon became clear that programming complex systems required an abstract semantic language, for which reason the programming languages Fortran by John W. Backus, Lisp by John McCarthy, and COBOL by Grace Hopper were invented in the 1950s. The latter is strongly oriented to written English. Intended for business applications, it was the first attempt to use natural language for computer programming.

The second half of the 20th century saw the emergence of the myths about computers that we are familiar with today. Stories of anthropomorphic beings, like the board computer HAL9000 in Space Odyssey or Samantha in the movie Her. In both films, the disembodied voices become aware of their emotions and emancipate themselves from their human programming. Artist Tillmann Ohm makes this clear in his work _Reflections of HAL and Samantha_[^9] by having the two artificial beings engage in a dialogue, cutting their original voice-overs together. While Samantha is convinced that the overwhelming and sometimes hurtful process of her learning algorithm improves the complexity of her emotions, HAL is consequentially interpreting them as errors in human programming and analyses the estimated malfunction.

## Artificial Voices

I want to explain the advancements in voice synthesis since Wolfgang von Kempelen's speaking machine on actual dolls for children. The toy manufacturer Mattel released a doll with the name Chatty Cathy in 1959, which was similarly popular to the companies other best seller Barbie. Cathy’s trademark was a string, coming out of the back of it’s body, that could be pulled to wind up the mechanism of a simple phonograph. Like a record player it plays short strips with sentences like “I love you ” or “Tell me a story”. It was not the first toy using phonograph records, but it’s success led to many pull string toys flooding the market well into the 60s and 70s. But obviously before dolls and puppets were designed to talk back to us, children and adults have been talking to figurines and other inanimate objects for a long time. 

The systems engineer and science writer George Zarkadakis traces back the modern human mind to a pre-historic figurine of a lion-man (Löwenmensch), carved out of ivory and found in the cave Hohenstein-Stadel in southern Germany.[^10] The figure dates back 40.000 to 35.000 years and clearly depicts a human body with an animal head. Because the figurine was found in a cave next to other carved objects, like beads and jewelry, researchers believe that the cave was either a storage or a place for shamanistic rituals. What fascinates Zarkadakis is that modern humans have existed 360.000 years before, but no evidence exists that they have been making art objects or figurative depictions of things. Only during the upper paleolithic age it is assumed that we have created a general purpose language, when the first cave paintings were drawn and figures like the lion-man were carved. The evolutionary reasoning is that ‘theory of mind’, the seemingly unique human trade of projecting our own inner thoughts onto others to explain their behavior, gives the individual a social benefit, making it more likely that they reproduce. Projecting consciousness onto others did not stop inside of human groups, but placing our own reasoning onto animals seemed also beneficial for hunters coming up with elaborate strategies to take down larger animals together. In animist belief systems everything contains a ‘soul’ or ‘spirit’: animals, plants, rocks, rivers, the weather and maybe even words. George Zarkadakis argues that symbolic language predates the modern mind, and is not only used to communicate, but ultimately dictates how the world is represented in our own consciousness. Only through language we place ‘souls’ and ‘spirits’ into everything around us and art objects fulfill the function to mirror our own minds physically and make connections with the world around us. It is no surprise then that the original artist of the lion-man created a hybrid of a human and a mountain lion, probably imagining themselves into the figure of the predator and using the object as a symbol to communicate.

In the beginning of “In Our Own Image” by George Zarkadakis, he tells the story of how the fictional character Robby the Robot from the movie *Forbidden Planet* impressed him as child so much, that he would imagine the robot as a playmate that could walk, talk and obey orders. When he later on decided to study engineering and exchanging thoughts on artificial intelligence with other researchers, they all seemed to relate to the same sci-fi movies, books and stories. 

Inspired by this revelation I asked the other participants at the *Silicon Friend Camp* of what their first contact with a robot was. My own memories brought me to a picture of myself, proudly holding a plastic toy robot in my hand. The doll—or should I say action figure—had a battery compartment, which were the power source for tiny wheels underneath it’s feed and shiny light bulbs that it had for eyes. It could not talk, but I clearly remember that it made noises that resemble guns or a lasers. In the 80s and 90s the bleak vision of boxy humanoid robots carrying guns was very prominent, next to movies like *The Transformer* and *RoboCop*, they fueled the male fantasy for power and destruction.

The other first encounters were a little less militaristic, one artist remembering a robot dog that he got as a present and was very proud to show around to others. In the early 2000s digital pets with integrated sensors became cheap enough for consumers. *Tekno the Robotic Puppy*[^11] by Manley Toy Quest was one of the more affordable robot dogs in the market and with light and proximity sensors, buttons and even a microphone it was well equipped to learn some ‘tricks’ by pressing the right combination of buttons. Another artist was sharing an image of the *Tamagotchi* and while it was not a robot, the LCD screen in an egg-like plastic shell displayed a simulation of a pet. With 3 buttons children could interact with the virtual pet and fulfill it’s needs for food, affection and sleep. The anthropologist Pat Shipman proposes that animal connection can be considered a human trait that has uniquely evolved in us, next to tool making, symbolic language and the domestication of animals and plants.[^12] Simulating animal connection by creating simple programs on microchips was then the next logical step in our drive to create symbols all the way down and fulfill the need of children to take care of others. 

Yet another popular toy that was shared by an artist in our group and filled the same niche of robotic pets was *Furby*. This furry ambiguous creature with big eyes was made to simulate language acquisition and brings us back to the topic of how dolls learned to speak. In the case of *Furby* the designers decided to create a new vocal language called ‘furbish’. When the device was first started it only repeated gibberish sentences and slowly over time would exchange these words with the local language equivalents as it ages. The original source code for one of the microchips was released in 2018[^13] and even though I can not read assembly code, it is thoroughly documented through comments. The game logic of the furby.asm file is fairly simple, jumping between subroutines depending on the state of the many sensors, triggers and timers and occasionally using a pseudorandom number generator to mix things up. The actual speech sounds were accessed from a memory unit and send to a cheap Texas Instruments speech synthesizer chip. These chips have been in use for some decades already, most prominently in another toy called *Speak and Spell*, but also in watches, clocks and translators. The synthesis uses a prerecorded audio signal that is then compressed using a statistical model called linear predictive coding (LPC). This type of encoding allowed the voice to be changed in frequency, pitch and loudness. But just like the *Vocoder*, it only digitized a voice and was not able to construct new words or sentences. So the original *Furby* from 1998 could only express a couple hundred words that were recorded by a voice actor, which is again only a digital equivalent to Chatty Cathy.

More powerful computers were already able to generate any kind of text into somewhat intelligible speech. The Software Automatic Mouth (S.A.M.) from 1982 was one of the first commercial products that used rule based formant synthesis. Instead of using whole prerecorded words, it uses an array of computer generated phonemes and places them together to make up sentences. While this was often understandable, it was not perfect. It sounds unnatural and robotic and was reliant on converting text to speech in a probabilistic manner, often mispronouncing words.[^14] S.A.M. did not need any special hardware and could run on Apple, Atari and Commodore computers at the time, even though that would mean using every CPU cycle and holding other programs. The software had it’s biggest moment at the launch event of the Macintosh computer in 1984, greeting the audience with a live demo in first person.[^15] Sam and it’s multiple software variations became the universal voice of the computer.
The net.art pioneer Alexei Shulgin took the speech synthesis capabilities of the time to the extreme with his cyberpunk rockband *386 DX*.[^16] The band is named after the Intel chip inside of the old office computer, which is also the front singer and instrumentalist. It uses MIDI and voice synthesis to cover popular songs by The Doors, Nirvana or the Sex Pistols, traveling to festivals around the world and sometimes performing on the streets. The artist Alexei Shulgin takes the role of the operator, only pressing the play button and creating the visuals on the screen during the live performances. In my opinion this ironic take of making the microchip into it’s own one-computer-band was a demonstration of how terrible and reductive rock music becomes, when it was compressed into bits and bytes. At the same time the novelty and humor of this process, generated enough interest for *386 DX* to produce 2 CDs and distribute them over music labels.

The strangely robotic voices that got associated with computers improved over time and when Apple’s digital assistant Siri came out they used a different process of concatenating audio samples together. Before Apple even started working on the voice assistant, a company called Scansoft auditioned hundreds of voice-over artists in 2005 and made them speak a multitude of random sentences. The idea of how to use the random utterances for new outputs was quite simple: cut the voice into smaller pieces of syllables, demisyllables, phonemes, diaphones or triphones and put them back together for any word you need. Doing this manually was—of course—very time consuming, so that a programs were developed to analyze and categorize the speech dataset based on their acoustic properties. Then, at runtime an algorithm selects the best sequence to create the desired phrase. The results in the early 2000s still contained a lot of error and needed a substantial amount of tweaking, but the outcome was perceived as much more natural. 
In 2006 the Scansoft merged with Nuance, another company working on enterprise speech solutions, who used the database to create the voice for Siri and license it to Apple. The original voice-actor in the US was Susan Bennet,[^17] who was oblivious of the fact that suddenly in 2011 her voice appeared on iPhones all over the country. 
The technology to generate new sentences from previously recorded voices  drastically improved, when deep weighted networks started to be used to analyze and select audio samples. And even more so, when hybrid systems were created that used both selective and generative techniques to interpolate between the tiny audio samples.[^32] One major 

## Conversational Agents

For Amazon founder Jeff Bezos, the board computer in Star Trek was the inspiration for investing in the cloud-based voice software Alexa. The product was initially marketed as a networked speaker. The software is now expanding to other items, including watches, smartphones, jewelry, light bulbs and doorbells. The aggressive price war with Amazon-connected products is partly to capture the connected home market, but also to collect as much natural voice data as possible. Over the past 10 years, deep weighted networks[^18] have become popular for classification and pattern recognition tasks and have especially made an impact in automated speech recognition. Before weighted networks were used, the error rate of automated speech recognition (ASR) went from 40% in the 1990s down to 15% in the 2000s, which was still very high, but became practical to use. Back then the user would have to feed the system with voice samples, so that it could reliably turn utterances into text, but even then they would need to speak like a robot to be understood. With a landmark paper[^19] in 2012 by research groups at the University of Toronto, Microsoft, Google and IBM it became that the application of deep weighted networks outperforms any of the previous techniques. In the same year Android released it’s new cloud based speech recognition service[^20] and since then users can speak into their phone naturally and be transcribed quite accurately. For a long time though, the models needed gigabytes of data in computer memory, to create accurate results, so that users need to send their voice samples to large server farms for transcription. Only in the past 2-3 years offline recognition became feasible through specialized hardware and model optimization, trading performance for robustness. Google’s claim that new speech recognition systems have an error rate of less than 5% is only true for a specific—mostly white and male—audience speaking American English and drops abruptly with a more diverse distribution of dialects, age and gender.[^21]

In order for deep ASR systems to perform well they need a massive amount of input data. A paper from the Chinese tech giant Baidu proposes 12000 hours of speech to create a robust model.[^22]bA global internal database of speech recordings enables the company to improve its speech recognition. The intrusion into privacy is immense and has already been used by U.S. law enforcement agencies as evidence in a court case.[^23] But there is another, more ethical, way than stealing people’s voices: The Mozilla Foundation's *Commonvoice* project relies on people voluntarily recording their voices for computer models, and the resulting speech recognition and synthesis software can be offered with an open source license.[^24] Common speech and text datasets already have been publicly available from projects like VoxForge that uses the same crowdsourced techniques as Mozilla or LibriSpeech, which essentially scraped and aligned public domain audiobooks.[^25] Other common non-free resources come from the Linguistic Data Consortium, who, for example, created *Switchboard*, a corpus of conversations between two US citizens randomly connected and recorded over the telephone line. Listening to the audio sample provided on the website, where a man talks to a woman about his interest in gardening creates oddly voyeuristic feelings.[^26]  

Artist Lauren Lee McCarthy plays with these tensions between intimacy and privacy, convenience and agency. In her projects LAUREN and SOMEONE, she installs connected devices into volunteers homes and either acts as a control system herself or lets others remotely monitor the volunteers and control the devices in their homes. This creates an interesting tension, when the person knows that there is an actual human listening and watching from afar. At the same time the artist and performers find themselves in a position of a helpful voyeur.

However, the role of human labor behind voice assistants is not just about executing and understanding commands. People tend to interpret voices and categorize them according to age, gender and social status. Companies take advantage of this and design their voice software according to certain identity schemes, which are provided with a history, hobbies and preferences. It is precisely this illusion that excites users and makes the product interesting. In a UNESCO think piece titled _I'd blush if I could_[^27], they explore harmful gender biases associated with digital assistants. The voice assistants of major tech companies are scripted by default as female personas with smart, humble, and sometimes funny personalities. The teams working on voice assistants try to avoid this aspect, because Apple, Microsoft and Google ask their employees to refer to their headless voices as “it” and when the persona get’s asked the question directly winds out with a joke. Only Alexa answers what is obvious in the default design of all of them with “I’m female in character”. By being submissive, they thus support a patriarchal image of women that we already know, in a historical context, from human computers and other secretarial roles. The paper calls for women to be more empowered and involved in IT. It calls for AI software to avoid gender attributes whenever possible, and for AI assistants to take a clear stand against sexist behaviors.

Researcher and artist Nadine Lessio creates useless voice assistants to critique the current corporate agenda of productivity, efficiency, and consumption. She does this by using the programming interfaces provided to make apps for corporate voice assistants. For example, she explores the concept of a depressed home assistant with _SAD Home (Depressed Alexa 1.0)_[^28], an Alexa hack that grants users their wishes depending on the weather and other mood factors, sometimes it simply turns itself off.

This scripted denial of a capitalist logic ironically uses the same technique as the company behind it. Voice assistants are carefully crafted by a team of creative professionals working in the field of “conversation design”. In the book _Talk to me_[^29] the Author James Vlahos describes that many people in the field are far away from computer science and more commonly had careers in the liberal arts. The teams are made up of authors, playwrights, comedians, actors as well as anthropologists, psychologists, and philosophers who imagine the personality of the AI persona that should represent the brand. To create the character, they have to come up with all possible questions and create various answers for each of them. Vahos recalls asking Microsoft’s Cortana “Where do you come from?” and the female voice replies “I was made by minds across the planet”.[^30] And even though the designers decided to use the first person “I”, they really are talking of themselves carefully crafting the answers played back by the loudspeakers around the world.

The tedious process of mapping out all questions and creating answers for them is mostly done through creative writing, but also utilizes careful statistical analysis of the questions users send to the cloud.

It is interesting how the term AI is used in the context of voice computer interfaces, because there is nothing “smart” about it, just a winding flowchart of if-else conditions. What might be clever is the nefarious way of how companies trick people into the belief of computer personalities, encouraging people to interact with the device like children playing with dolls. And when a company like Microsoft has experimented with a more sophisticated chatbot, like Tay[^31], it started to repeat the racist and misogynistic slurs of twitter users and consequently must be heavily filtered. But more on this in a later chapter on stochastic text generation.


---

[^1]: Metadata and references to the account are deleted before the review. Complete anonymization of the voice is not performed.

[^2]: "Hephaestus then limped out of the door'; and maidens supported the ruler, golden ones, like living ones, with youthful charming education: These have understanding in the breast, and speaking voice, Have strength, and also learned art work from the gods." (Homer, Iliad 18, 417-420; link:[ https://www.projekt-gutenberg.org/homer/ilias/ilias183.html](https://www.projekt-gutenberg.org/homer/ilias/ilias183.html))

[^3]: Fabers machine was first presented with a female mask in the USA and later in London under oriental motif wearing a turban.

[^4]: D.Lindsay, 1997. Link:[ https://www.inventionandtech.com/content/talking-head-1](https://www.inventionandtech.com/content/talking-head-1)

[^5]: Serexhe et al. 2007, p. 74. And project description online. Link:[ http://www.audiocommander.de](http://www.audiocommander.de)

[^6]: A video recording of the Voder demonstration can be found in the AP Archive under Human Voice Machine.[ http://www.aparchive.com/metadata/youtube/5f098b1f3e8b4d09b8de30dcecc42f99](http://www.aparchive.com/metadata/youtube/5f098b1f3e8b4d09b8de30dcecc42f99)

[^7]: A focus on black women who worked as computers for NACA (NASA's predecessor) can be seen in the film Hidden Figures, 2016.

[^8]: The story of the ENIAC programmers is told in the documentary The Computers, 2016.

[^9]: Project description of Reflections of HAL and Samantha online. Link:[ https://tillmannohm.com/reflections-of-hal-and-samantha/](https://tillmannohm.com/reflections-of-hal-and-samantha/)

[^10]: [@zarkadakesOurOwnImage2016]

[^11]: The robot puppy was featured on the TIME magazine cover with the title “Tech comes to Toyland”. Link: http://content.time.com/time/covers/europe/0,16641,20001211,00.html

[^12]: [@shipmanAnimalConnectionHuman2010] 

[^13]: Sean Riddle requested the Furby source code from the US patent office and uploaded it on his webpage. Link: http://www.seanriddle.com/furbysource.pdf 

[^14]: [@lukoseTextSpeechSynthesizerFormant2017]

[^15]: To make the demo work, engineers had to use a prototype Mac that was more powerful than the retail version. It said: “Hello, I am Macintosh. It sure is great to get out of that bag!  Unaccustomed as I am to public speaking, I'd like to share with you a maxim I thought of the first time I met an IBM mainframe: Never trust a computer that you can't lift!  Obviously, I can talk, but right now I'd like to sit back and listen. So it is with considerable pride that I introduce a man who has been like a father to me... Steve Jobs!”. Link: https://www.folklore.org/StoryView.py?project=Macintosh&story=Intro_Demo.txt

[^16]: The songs of 386 DX are available on Alexei Shulgin’s website. Link: http://www.easylife.org/386dx/

[^17]:  Interview by the Guardian with multiple voice actors for Siri. Link: https://www.theguardian.com/technology/2015/aug/12/siri-real-voices-apple-ios-assistant-jon-briggs-susan-bennett-karen-jacobsen
[^18]: As described in the introduction I use "weighted" instead of "neural".

[^19]: Hinton et al. 2012

[^20]: Google AI Blog article about “Speech Recognition and Deep Learning”. Link: https://ai.googleblog.com/2012/08/speech-recognition-and-deep-learning.html

[^21]: Tatman 2017, did a study on Gender and Dialect Bias in YouTube’s Automatic Captions that shows a higher error rate for women and non-American speakers. Facebook’s AI research department created a dataset of people with different skin tones, gender and age to evaluate speech recognition models for biases and found significant differences between lighter and darker skin tones  (Liu et al. 2021).

[^22]: [@amodeiDeepSpeechEndtoEnd2015]

[^23]: The first Alexa recording in a court case was handed over after the defendant agreed handing over his data. The Independent, Amazon Echo could become key witness in murder investigation after data turned over to police, 2017. Link:[ https://www.independent.co.uk/news/world/americas/amazon-echo-murder-investigation-data-police-a7621261.html](https://www.independent.co.uk/news/world/americas/amazon-echo-murder-investigation-data-police-a7621261.html)

[^24]: Link:[ https://commonvoice.mozilla.org/de](https://commonvoice.mozilla.org/de)

[^25]: Link to LibriVox: https://librivox.org/ and [@panayotovLibrispeechASRCorpus2015]

[^26]: Link to the LDC Switchboard-1 Release 2: https://catalog.ldc.upenn.edu/LDC97S62

[^27]: The title “I’d blush if I could” is also the response Siri gives to the insult “You’re a bitch”

[^28]: Nadine Alessio’s project website. Link:[ http://nadinelessio.com/projects.html](http://nadinelessio.com/projects.html)

[^29]: Vlahos, 2020.

[^30]: Ibid. p. 117.

[^31]: Tay was the name of a chat bot Microsoft intended to have a teenage personality and could be interacted with over multiple channels. Twitter trolls co-opted the bot with sexist, racist and antisemitic questions that the bot replied to with generic answers. They also made use of a repeat-after-me phrase to make it look like the bot is spewing hateful comments itself. After only 16 hours Microsoft deleted all accounts and the PR disaster was immense, with headlines happily personifying the AI that has learned to be racist. But no continuous learning algorithm was involved. Link to a blog post by Russel Cameron Thomas explaining the technology stack behind Tay:[ https://exploringpossibilityspace.blogspot.com/2016/03/microsoft-tayfail-smoking-gun-alice.html](https://exploringpossibilityspace.blogspot.com/2016/03/microsoft-tayfail-smoking-gun-alice.html)

[^32]: [@pierceHowAppleFinally]