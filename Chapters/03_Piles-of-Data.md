# Piles of Data

## Self-Learning Networks

The idea behind Artificial Neural Networks has a long standing history. Using our understanding of the brain as a blueprint for mathematical operations dates back to the 1950s when the psychologist Frank Rosenblatt developed the *perceptron*.[^1] Inspired by nerve cells and their connections (synapses), the perceptron takes multiple input values, sums them up and outputs a 0 or 1 depending if a predefined threshold is reached. This system can be ‘trained’ by using positive and negative reinforcement to define the weights for each connection. With an apparatus, the Mark-I Perceptron,[^2] that uses photoreceptors to turn light into ‘bits’ (today we would say pixels) the perceptron could ‘sense’ shapes in the form a binary matrix and distinguish between circles, squares and triangles. He proposed that a network of perceptrons could possibly even recognize faces and objects. Even though he developed the perceptron in 1964, Frank Rosenblatt never got to see his invention take off.[^3] 
Another engineer, Kunihiko Fukushima, kept refining his methods in the 70s by adding multiple layers, effectively creating the first ‘deep neural network’ where deep just means the depth of ‘hidden’ or inbetween layers connecting the input signal to the output classifier.[^4] He called this self-organizing system Cognitron[^5] which was successful at accurately recognizing numbers and letters on a 12x12 grid. It’s successor the Neocognitron[^6] took further inspiration from the visual cortex and a discoveries by Hubel & Wiesel made in the 1950s that some biological neurons selectively respond to local features like lines, edges or color and others to figures like circles, squares or even human faces. This is also the core idea behind convolutional neural networks (known as ConvNet or CNN) which separate an image into a smaller grid and apply a certain filter to them, e.g. checking for edges. The french computer scientist Yann LeCun came up with ConvNets in the 1980s which are *the* driving force for AI systems today. Additionally Geoff Hinton, a cognitive psychologist and computer scientist, popularized the backpropagation algorithm in 1986 which finally made it possible for filters to tune themselves instead of relying on predefined rules.

Most conceptual ideas behind current deep weighted networks were already present in Frank Rosenblatt papers,[^7] but weighted networks were often outperformed by rule-based systems. So what changed when Alex Krizhevsky, a student of Hinton, made a phenomenal leap in 2012 on the ImageNet classification competition?
The main innovation is outlined by Krizhevsky in the paper itself: “To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation.”[^8] Until AlexNet was released it was incredibly time consuming to run the conditioning process on the CPU which can only do one operation of matrix multiplication at the time. The GPU as the name Graphics Processing Unit suggests was originally designed to calculate 3D scenes and render them on a display. This involves a lot of matrix and vector operations and to accelerate them GPUs are capable at calculating large blocks of data in parallel. This means that conditioning AlexNet on 1.2 million pictures took only 6-9 days on two consumer graphics cards compared to probably weeks or months without them. However it was not the first system that utilized the GPU, it is similar to a CNN by Dan C. Cireȿan et al. released a year prior which has a reported speedup of 60 times for large weighted networks.[^9]
The other roadblock for deep weighted networks is ‘overfitting’. In the case of the ImageNet competition that would mean that the model adapts to the image so closely that it would simply reproduce the categories of the image without being able to identify new pictures which were not inside the training set. The most common way to reduce overfitting is to have a sufficiently large dataset with a high amount of variance. For AlexNet the 1000 classes and 1.2 million images were still not enough and they used data augmentation which transforms, flips or changes the color of an image to increase the training set by a factor of 2048. This means that—in theory—a larger dataset increases the robustness of the weighted network.

In conclusion, the conceptual framework of how weighted networks (or artificial neural nets, or perceptrons, or cognitrons) work was mostly established in the past century. Their performance today comes down to increased computing power and larger training sets. In fact datasets have become bigger and bigger and we haven’t seen the limits of what weighted networks are capable of, they seem to be mainly limited by data and computation.
In the following chapter I will have a closer look at how large training sets are constructed in an academic and artistic context—both my own and a few other examples—and the ethical issues and responsibilities regarding privacy, consent, representation and ownership.

## Making Data, 11076 hands

My first encounter with a large dataset in the field of computer vision was in late 2017 when I found *11k Hands*.[^10] As the name suggests it is a dataset of 11076 human hands compiled by the researcher Mahmoud Afifi for gender recognition and biometric identification. The images show the hands of 190 individuals, in both their palm and dorsal orientation, placed on a white background with uniform lighting. Each image is accompanied by the following metadata: age, binary gender, skin color on a scale of “very fair”, “fair”, “medium” and “dark”, left or right hand, palm or dorsal, if it has nail polish and if there are “irregularities”. A statistical analysis of the dataset shows that it contains more female than male hands, mainly people in there 20s and a majority of “medium” and “fair” skin tones. The gender bias is addressed in the paper and mitigated by filtering the training set to have an equal amount of males and females. They report that the CNN conditioned on this dataset had on average a 87% accuracy recognizing the correct gender on an image of the palm and a 90% accuracy for the dorsal side.
But it is not the ‘state-of-the-art’ results that drew me to the paper, it was the gender and skin-tone classification itself that appalled me. It reminds me of phrenology in the 19th and 20th century, a popular pseudoscience that claimed that persons character and mental abilities could be determined by the shape of their skull. Phrenologists like Franz Joseph Gall went to a great length measuring and categorizing human skulls and associating certain regions to human traits. While it didn’t take scientists to debunk the idea that bumps on a head could indicate the characteristics of a person, it had a comeback in the early 20th century when it was applied to racist and sexist stereotypes and used to justify Nazi eugenics. The underlying assumption that the shape of the head has anything to do with the mental state of the person was simply wrong.[^11]
Considering that gender is a social construct and not necessarily a binary choice trying to use a computer to identify if a person is male or female by analyzing their hand seems arbitrary at best and reinforcing gender stereotypes at least.[^12] 
Another aspect of the dataset stood out to me when I started to look through the images. These were not pictures in the traditional sense, their aesthetical value did not matter as they are simply a tool to to accomplish a task. They are not made ‘to look at’ instead these images of hands are produced for a computer to analyze. Harun Faroki called these types of images ‘operative’ in his three-part series *Machine/Eye* where he examined how military technologies like guided weapons produce images that serve only the utility of the machine. As Aud Sissel Hoel puts it: “operative images are devoid of social intent, that they are not meant for edification, and nor for contemplation.” [^13]
But what happens when you contemplate on the hand dataset was remarkable to me. When I looked through the images quick enough I could not only see the motion of the test subject as the images were selected frames from a video, but I also started to imagine the person behind the hand and apply my own stereotypes and biases that far expanded the labels of the dataset. I could easily imagine the scene around the camera with a couple researchers who assembled a make-shift photo studio in the office of their lab. They greet the subject with a handshake, which is probably a fellow student, and explain the procedure quickly to get them to sign the paper that their anonymized personal data will be published for scientific research. Then the person puts both their hands under the camera spread their fingers and leave. 
This dichotomy between the label and my own narrative inspired me to create a video piece featuring the unaltered dataset. I used the default mac computer speech synthesis to read out the labels corresponding to each hand, and sped it up to fit into a 26-minute-long video. As viewers witness the participants holding their hands into the camera and spreading their fingers, they are met with a monotone, beat-like soundtrack of repeating words like “fair” and “medium” and occasionally “nail polish”.[^14]
![Example of a frame from 11k hands](Matthias%20Schäfer%20-%2011k%20hands%20[snJsKFxPlJ8%20-%20892x669%20-%203m35s].png)

At the 36th Chaos Communication Congress I organized a workshop called “Palm Reading AI” where I invited visitors to read the hands of people from the 11k hands dataset. At first I only introduced them to palmistry using wikihow as a reference.[^15] Then I handed out some prints where a random hand from the dataset was depicted and participants had to fill in a couple of questions. Some questions were short guesses like age, gender, country of origin, for some other they had to come up with fictional stories with only the hand lines as a reference: what is the persons future? How was their childhood? How is their love life?
After they filled in the form some people shared their stories and then I revealed where these hands came from and how computer scientists are using them to create models that try to predict their gender. Afterwards we had a discussion about the practice of creating large datasets and their ethical considerations. I had a longer talk with one participant that did not want to guess the age or gender of that person and I had told them that this was exactly the point of the workshop: to reflect on our own biases and stereotypes and how they translate into science.

After long contemplation on 11k hands and finding datasets that are much more problematic than this one, I don’t think the type of work from Afifi et al. is ‘unethical’ or needs to be redacted. They got consent from their subjects and share the dataset to the scientific community for “reasonable academic fair use”. The work on biometric identification and comparing CNNs to previous methods is interesting and novel, however as I stated above I think the premise behind the gender recognition task is flawed. Unfortunately this is very common in the computer vision field where people are (mis-)labeled that reflect and amplify societal stereotypes.[^16]
My research on the hands dataset in conjunction with esoteric practices and fortune-telling informed a later work of mine “The Chiromancer” that I built together with Giacomo Piazzi.

## In the wild

The process of collecting and creating data has drastically changed since the wide adoption of the internet. Where the 11k hands dataset has invited participants to their instute to take a picture specifically for the dataset, other researcher started to search and download huge collections from the internet without any consent.
Take for example the ImageNet dataset, initiated by Stanford University’s AI professor Fei Fei Li, which was created to tackle object recognition tasks and eventually consisted of 14 million images.[^17] The team queried multiple search engines with common nouns and multiple translations to get around 500-1000 images per category. The categories are derived from an older project called WordNet, created in 1985 at Princeton University, that tried to achieve a hierarchical ontology of words. For example the noun *human* is synonym to *homo, man and human being* which are subcategories of the *hominid* class, which are *primates*, which are *mammals*, which are *animals*, which is an *organism* and finally part of the *entity* class. After downloading millions of images in thousands of categories they were first automatically deduplicated but the task of checking whether the image actually depicts the search term could not be done by an algorithm. Originally Fei Fei Li wanted to hire graduate students to sort the data which was too costly and time consuming.[^18]
One of her undergraduates suggested to use a new service called Amazon Mechanical Turk where people around the world complete small task for little compensation. Even with the help of Mechanical Turk it took 2,5 years to sort and validate the first dataset with 3.2 million images in over 5000 categories. At some point ImageNet was the largest academic client for Amazon’s Turk service and after the popularization of weighted networks they became a corner stone of data annotation. Similar platforms were established and Mechanical Turk grew to an estimate of half a million workers, which were rigorously exploited and alienated as a so called click workers. One paper estimates their median hourly wage to 2$ and only the top 4% earning more than 7.25$ per hour.[^19] If the task is rejected by the requester the worker does not get paid at all and has no way of appealing that decision. To advocate for better working conditions the designer and researcher Caroline Sinders built an open source tool for data annotation and a wage calculator to better estimate the cost of labeling.[^20]  
With so many different people working on labeling images, societal biases are eventually reflected in the dataset. In 2019 the artist Trevor Paglen and researcher Kate Crawford collaborated on an exhibition titled *Training Humans*, dedicated to human image datasets. One of the main exhibits was a vast collection of human images from ImageNet.[^21]   
As you go further down the category tree you might find images of people that fall into categories such as “Bad Person, Call Girl, Drug Addict, Closet Queen, Convict” and so on.[^22] The artists used these absurd, racist, and misogynistic labels to train *ImageNet Roulette*,[^23] a recognition algorithm that was accessible online and in an interactive installation. People online quickly picked up on the tool and shared images of themselves with sometimes amusing and often deeply problematic image captions.[^24] While some of the people in the research community defended ImageNet that these offensive labels are not part of competition and make up only a small fraction of the total dataset, as a result of the media attention that followed more than 600,000 images were removed. It is now only accessible after proving to be part of a scientific institute.

Another pair of artists and researchers, Adam Harvey and Jules LaPlace, have been exposing image sets which often get revoked and removed after publishing their investigation.[^25] Harvey and LaPlace focus on datasets with faces that are captured “in the wild”. One particular example was the *UnConstrained College Students (UCCS)* dataset captured at the University of Colorado Colorado Springs. According to the authors of the associated papers to the dataset they identified that current image sets created for face recognition research did not address the presence of unknown subjects. The authors of the papers associated with this dataset wanted to create a more realistic benchmark for face recognition research by introducing unknown subjects over time. To achieve this, they captured students on campus during breaks using a 800mm telephoto lense from a distance of 100-150 meters through an office window. The authors frame it as a benefit that the students are unaware of their capturing for the dataset as they casually focus on random activities and their faces are sometimes blurred and occluded. To make things worse this research was mainly funded by US defense and intelligence agencies. The public backlash was immense and as a result the dataset is no longer publicly available. The researchers in question did not apologize and argued that their subjects are anonymous as their names or other identifiers are not published.[^26]
However when researchers need unconstrained and non-consensual data, they do not often capture them directly using creepy surveillance tactics. Starting with “Labeled Faces in the wild” in 2007 the practice of collecting and labeling image data from internet sources has become normalized and is still largely unregulated. Often these image sets “in the wild” operate in a gray zone where they either depict public figures, arguing that privacy regulations do not apply to them, or that they simply link to the file and only store them temporarily for analysis. A third option uses media licensed under Creative Commons (CC) which is mostly considered free and legal data with no restrictions in the AI research community.[^27] In a detailed report Adam Harvey lists many datasets that exploit and often misuse CC licenses to create face and object recognition datasets.[^28] He identifies Flickr as a major source for collecting images, a popular image sharing website where users can choose from various Creative Commons, copyright and public domain licenses. Flickr actively promoted the use of CC licenses and offered unlimited free hosting if a CC license is used. Their strategy worked and by 2022 they amassed 467 million CC licensed images. In 2014 a joint research group including Yahoo Labs, the company that bought Flickr, shared one of the largest public media datasets[^29] with the name *Yahoo! Flickr Creative Commons 100 Million (YFCC100M)*.
A multitude of specified datasets were created from this corpus. One of them is the *MegaFace* face recognition dataset with 4,7 million faces from 672,057 identities. While all of the images fall under a CC license most of them prohibit their commercial use and require appropriate attribution, which was not given in the dataset. As Harvey and LaPlace verified, the *MegaFace* dataset was used by globally by commercial, military and academic organizations.[^30] As an investigative article from the New York Times explains, most people are unaware that their images are powering face recognition research around the world, including companies with ties to Chinas surveillance on the Uyghur population.[^31] 
To conclude the three common issues with CC licensed media is that they are not or wrongly attributed, the use of non-consensual biometric data is prohibited in some places (e.g. Illinois) and the use in commercial applications is often prohibited. Many of thes issues identified by Harvey and LaPlace remain until today and operate in a legal grey zone. While the notion behind uploading images under an open license for others to freely share, distribute and remix media is a noble goal, the CC license is legally weak and practically useless against opting out of statistical analysis in AI research. But we might be slowly moving into a direction where law makers catch up and create precedences that disallow the use of biometric data in certain applications, like the upcoming AI Act.[^32] 

## This Person Does Exist

A dataset using Creative Commons images that I examined more closely was *Flickr Faces HQ (FFHQ)*. In 2018 the research lab of Nvidia, one of the leading companies for visual computing, published a paper introducing a machine learning architecture called StyleGAN.[^33] They improved on generative adversarial networks (GAN) in such a way that it was possible to create controllable synthetic high-resolution images. In simple terms the system is able to abstract large amounts of images with a model that in turn outputs similar-looking pictures. In this case the model is able to generate realistic-looking photographs of human faces. 
In comparison to other datasets *FFHQ* is fairly small with only 70,000 images. As existing datasets were too low in resolution a new corpus was created by scraping Creative Commons, Public Domain or U.S. Government Works licensed images through Flickr’s API. The dataset itself is published under a CC-BY-NC-SA license and the instructions for use and download are very clear, making it manageable for me in terms of size and effort to discover the underlying characteristics. The dataset consists only of photographic images as it was checked by Amazon Mechanical Turk workers to make sure that statues, paintings, or photos of photos were removed. Using the open-source library dlib, the raw images were automatically aligned and cropped around the face to create a uniform square ratio of 1024px. This library also identifies 68 points outlining the chin, eyebrows, eyes, nose and mouth which are included in the metadata. So the final dataset used for training the GAN consists of a multitude of faces whe the eye and mouth positions are always in the same spot.

![Screenshot of This Person Does Exist](Screen%20Shot%202023-03-15%20at%2020.50.49.png)

One year after Nvidia released its StyleGAN paper the software developer Phillip Wang published the website *thispersondoesnotexist.com* which shows the capabilities of the generative model to create realistic looking photographs of people.[^34] The site quickly took off and alarmed people about potential impact of AI systems in generating cheap synthetic media.  
As a counter-narrative to the AI image creator, I wanted to showcase the people who were used to train this system. In 2020 I moved the cropped and aligned face images to my server and built my own website with the name *this-person-does-exist.com* which displays the faces from the training set alongside their metadata.[^35] Looking at the individuals faces facilitates an interpersonal connection with the unknown person and evokes a feeling for the images that were used to train the generative model. At the same time it shows the creepy and strange practice of AI researchers using personal images as raw data.
As Flickr is used mostly by hobbyists and professional photographers, one can find portraits of children and families, speakers at conferences or people on holiday. We can assume that, like other scraped datasets, the photographers of these images are unaware that their images are used for AI analysis. In contrast to other scraped datasets, the Nvidia researchers provide a tool to see if an image is part of the collection and allow the removal of the photograph.[^36] According to Adam Harvey the company does not disclose if any images were requested for removal and has not updated or removed any photos since its release in 2016.[^37]

![70k faces from FFHQ compiled into one image ](Fig2.png)
![The Flickr Face - averaged FFHQ dataset](Fig3.png)

To get a sense of scale of the dataset, I compiled all face images into a grid, reducing the size of each image to 16 by 16px. This simple montage makes it possible to get a feeling for the vast amount of normalized image data.
The authors claim that FFHQ includes more variety than other face image sets in terms “of age, ethnicity and image background, and also has much better coverage of accessories such as eyeglasses, sun-glasses, hats, etc.”.[^38] They admit to biases inherited from the Flickr platform but fail to mention them. Looking through the data it becomes clear that a majority of the images are taken with high resolution digital camera systems under good lighting conditions. Many of the pictures show separate the subject with a blurry background, so called ‘bokeh’, and conform to photographic aesthetic norms of the early 2000s until today.
Indeed, the dataset contains a variety of skin tones, age groups and background but they are not equally distributed. To visually find a bias in image sets I averaged the pixel values of the images. This suppresses outliers, but it allows us to see an overall trend of the dataset. The resulting composite image The Flickr Face, I belief, reveals a trend towards smiling and light-skinned people in the data set. All these are highly subjective impressions, but I would not presume any ethnicity or categorize people by skin tone.
I understand that this project is doing the same as the research labs in question, but I hope by making people uncomfortable and showing the human behind AI systems, we can get a better feeling for how creepy this harvesting of faces is.

As a part of a growing group of artists exploring and exposing research datasets, I coined a term for this genre: Dataset Art. My paper on this subject was published in _Temes de Disseny_ in 2021 and includes a couple more examples. Through their works, these artists are able to make large datasets understandable and captivating. Their art has sometimes even created enough attention to lead some institutions to remove questionable parts or entire collections. Whether through galleries or online, Dataset Art is providing an exciting new way to peek into the workings of AI systems.[^39]

## Scrapism

Web scraping is the technique of using computer programs to automatically visit links and aggregate data from the internet. It is the backbone for many of the current machine learning applications. The artist Sam Lavigne uses the practice web scraping “for artistic, emotional, and critical ends” and describes this form as ‘Scrapism’.[^40]
Instead of using and exposing datasets made for scientific research, Lavigne creates his own datasets by downloading and analyzing materials on the internet to revert common power structures. For example 

[^1]: [@rosenblattPerceptronProbabilisticModel1958]
[^2]: The Mark I was a electromechanical machine that used motor driven potentiometers to adjust the variable weights.
[^3]: Frank Rosenblatt died in a boating accident in 1971. A couple years prior Marvin Minsky heavily criticized the mathematics behind perceptrons and advocated for a symbolic approach. These turn of events might have lead to a lack of funding in the ‘connectionist’ AI research field and ultimately lead to a general disinterest when the symbolic approach could not keep their exaggerated promises.
[^4]: [@mitchellArtificialIntelligenceGuide2019], p. 114
[^5]: [@fukushimaCognitronSelforganizingMultilayered1975]
[^6]: [@fukushimaNeocognitronHierarchicalNeural1988]
[^7]: [@tappertWhoFatherDeep2019]
[^8]: [@krizhevskyImageNetClassificationDeep2017]
[^9]: “One epoch takes 35 GPU minutes but more than 35 CPU hours.” [@ciresanFlexibleHighPerformance]
[^10]: [@afifi11KHandsGender2018]. Data and source code available here: [@afifi11kHands]
[^11]: In 2018 Parker et al jokingly tested the Galls theory using 21st century scientific methods and MRI data. [@dempsey-jonesNeuroscientistsPutDubious2018]
[^12]: I didn’t do any testing of the system as I don’t know how to run MatLab code, but I can imagine that the slightly better results on the dorsal hand are the result of nail polish only applied on female hands. 
[^13]: [@hoelOperativeImagesInroads2018]
[^14]: See: [@matthiasschafer11kHands2018]
[^15]: [@HowReadPalms]
[^16]: One particular famous example of this is the work by Michael Kosinski and Yiluna Wang. Their flawed study tried to predict if a person is gay by scraping dating sites and training a classifier on these images. See:[@murphyWhyStanfordResearchers2017]
[^17]: ImageNet started with 3,2 million images and had the goal to collect 50 million by the end of 2011. [@dengImageNetLargeScaleHierarchical2009]
[^18]: [@gershgornDataThatTransformed2017]
[^19]: [@haraDataDrivenAnalysisWorkers2017]
[^20]: See: [@TechnicallyResponsibleKnowledge]
[^21]: See: [@KATECRAWFORDTREVOR]
[^22]: [@ExcavatingAI] 
[^23]: See: [@ImageNetRouletteTrevor]
[^24]: [@reaHowImageNetRoulette2019]
[^25]: See: [@harveyExposingAi]
[^26]: [@2ndUnconstrainedFace]
[^27]: See a longer analysis on the exploitation of CC media by A. Harvey: [@CreativeCommonsBiometrics]
[^28]: [@CreativeCommonsBiometrics]
[^29]: YFCC100M only contains links and metadata to images and videos under CC license onf Flickr
[^30]: [@harveyExposingAiMegaFace]
[^31]: [@hillHowPhotosYour2019]
[^32]: The Open Future Foundation is a think tank that actively tries to influence european digital policy debates. See [@OpenFutureOpen]
[^33]: [@karrasStyleBasedGeneratorArchitecture2019]
[^34]: At the time of this writing the url redirects to stability.ai, an archived version can be found. See [@ThisPersonDoes2019]  
[^35]: See [@schaferThisPersonDoes2021]
[^36]: [@FFHQDatasetSearch]
[^37]: See ‘FFHQ Dataset’ at [@CreativeCommonsBiometrics]
[^38]: [@karrasStyleBasedGeneratorArchitecture2019]
[^39]: [@schaferThisPersonDoes2021]
[^40]: [@lavigneScrapism]