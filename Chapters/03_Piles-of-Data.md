# Piles of Data

The idea behind Artificial Neural Networks has a long standing history. Using our understanding of the brain as a blueprint for mathematical operations dates back to the 1950s when the psychologist Frank Rosenblatt developed the *Perceptron*.[^1] Inspired by nerve cells and their connections (synapses), the perceptron takes multiple input values, sums them up and outputs a 0 or 1 depending if a predefined threshold is reached. With an apparatus that turns light to black and white ‘bits’ (today we would say pixels) the Perceptron could ‘sense’ shapes in the form a binary matrix. Using positive and negative reinforcement to define the weights for each connection it was possible for Rosenblatt to build a detector that can distinguish between circles and squares. He proposed that a network of perceptrons could possibly recognize faces and objects. While Frank Rosenblatt never got to see his invention take off[^2] another engineer, Kunihiko Fukushima, kept refining his methods by adding multiple layers in between the input signal and the output. He called these self-organizing systems Cognitrons[^3] which were successful at accurately recognizing numbers and letters on a 12x12 grid.  


## Scraping the web

## Informed Consent
- 11k Hands
- This Person Does Exist

[^1]: [@rosenblattPerceptronProbabilisticModel1958]
[^2]: Frank Rosenblatt died in a boating accident in 1971. A couple years prior Marvin Minsky heavily criticized the mathematics behind perceptrons and advocated for a symbolic approach. These turn of events might have lead to a lack of funding in the ‘connectionist’ AI research field and ultimately lead to a general disinterest when the symbolic approach could not keep their promises.
[^3]: [@fukushimaCognitronSelforganizingMultilayered1975]