# Piles of Data

## Self-Learning Networks

The idea behind Artificial Neural Networks has a long standing history. Using our understanding of the brain as a blueprint for mathematical operations dates back to the 1950s when the psychologist Frank Rosenblatt developed the *perceptron*.[^1] Inspired by nerve cells and their connections (synapses), the perceptron takes multiple input values, sums them up and outputs a 0 or 1 depending if a predefined threshold is reached. This system can be ‘trained’ by using positive and negative reinforcement to define the weights for each connection. With an apparatus, the Mark-I Perceptron,[^2] that uses photoreceptors to turn light into ‘bits’ (today we would say pixels) the perceptron could ‘sense’ shapes in the form a binary matrix and distinguish between circles, squares and triangles. He proposed that a network of perceptrons could possibly even recognize faces and objects. Even though he developed the perceptron in 1964, Frank Rosenblatt never got to see his invention take off.[^3] 
Another engineer, Kunihiko Fukushima, kept refining his methods in the 70s by adding multiple layers, effectively creating the first ‘deep neural network’ where deep just means the depth of ‘hidden’ or inbetween layers connecting the input signal to the output classifier.[^4] He called this self-organizing system Cognitron[^5] which was successful at accurately recognizing numbers and letters on a 12x12 grid. It’s successor the Neocognitron[^6] took further inspiration from the visual cortex and a discoveries by Hubel & Wiesel made in the 1950s that some biological neurons selectively respond to local features like lines, edges or color and others to figures like circles, squares or even human faces. This is also the core idea behind convolutional neural networks (known as ConvNet or CNN) which separate an image into a smaller grid and apply a certain filter to them, e.g. checking for edges. The french computer scientist Yann LeCun came up with ConvNets in the 1980s which are *the* driving force for AI systems today. Additionally Geoff Hinton, a cognitive psychologist and computer scientist, popularized the backpropagation algorithm in 1986 which finally made it possible for filters to tune themselves instead of relying on predefined rules.

Most conceptual ideas behind current deep weighted networks were already present in Frank Rosenblatt papers,[^7] but weighted networks were often outperformed by rule-based systems. So what changed when Alex Krizhevsky, a student of Hinton, made a phenomenal leap in 2012 on the ImageNet classification competition?
The main innovation is outlined by Krizhevsky in the paper itself: “To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation.”[^8] Until AlexNet was released it was incredibly time consuming to run the conditioning process on the CPU which can only do one operation of matrix multiplication at the time. The GPU as the name Graphics Processing Unit suggests was originally designed to calculate 3D scenes and render them on a display. This involves a lot of matrix and vector operations and to accelerate them GPUs are capable at calculating large blocks of data in parallel. This means that conditioning AlexNet on 1.2 million pictures took only 6-9 days on two consumer graphics cards compared to probably weeks or months without them. However it was not the first system that utilized the GPU, it is similar to a CNN by Dan C. Cireȿan et al. released a year prior which has a reported speedup of 60 times for large weighted networks.[^9]
The other roadblock for deep weighted networks is ‘overfitting’. In the case of the ImageNet competition that would mean that the model adapts to the image so closely that it would simply reproduce the categories of the image without being able to identify new pictures which were not inside the training set. The most common way to reduce overfitting is to have a sufficiently large dataset with a high amount of variance. For AlexNet the 1000 classes and 1.2 million images were still not enough and they used data augmentation which transforms, flips or changes the color of an image to increase the training set by a factor of 2048. This means that—in theory—a larger dataset increases the robustness of the weighted network.

In conclusion, the conceptual framework of how weighted networks (or artificial neural nets, or perceptrons, or cognitrons) work was mostly established in the past century. Their performance today comes down to increased computing power and larger training sets. In fact datasets have become bigger and bigger and we haven’t seen the limits of what weighted networks are capable of, they seem to be mainly limited by data and computation.
In the following chapter I will have a closer look at how large training sets are constructed in an academic and artistic context—both my own and a few other examples—and the ethical issues and responsibilities regarding privacy, consent, representation and ownership.

## Making Data, 11076 hands

My first encounter with a large dataset in the field of computer vision was in late 2017 when I found *11k Hands*.[^10] As the name suggests it is a dataset of 11076 human hands compiled by the researcher Mahmoud Afifi for gender recognition and biometric identification. The images show the hands of 190 individuals, in both their palm and dorsal orientation, placed on a white background with uniform lighting. Each image is accompanied by the following metadata: age, binary gender, skin color on a scale of “very fair”, “fair”, “medium” and “dark”, left or right hand, palm or dorsal, if it has nail polish and if there are “irregularities”. A statistical analysis of the dataset shows that it contains more female than male hands, mainly people in there 20s and a majority of “medium” and “fair” skin tones. The gender bias is addressed in the paper and mitigated by filtering the training set to have an equal amount of males and females. They report that the CNN conditioned on this dataset had on average a 87% accuracy recognizing the correct gender on an image of the palm and a 90% accuracy for the dorsal side.
But it is not the ‘state-of-the-art’ results that drew me to the paper, it was the gender and skin-tone classification itself that appalled me. It reminds me of phrenology in the 19th and 20th century, a popular pseudoscience that claimed that persons character and mental abilities could be determined by the shape of their skull. Phrenologists like Franz Joseph Gall went to a great length measuring and categorizing human skulls and associating certain regions to human traits. While it didn’t take scientists to debunk the idea that bumps on a head could indicate the characteristics of a person, it had a comeback in the early 20th century when it was applied to racist and sexist stereotypes and used to justify Nazi eugenics. The underlying assumption that the shape of the head has anything to do with the mental state of the person was simply wrong.[^11]
Considering that gender is a social construct and not necessarily a binary choice trying to use a computer to identify if a person is male or female by analyzing their hand seems arbitrary at best and reinforcing gender stereotypes at least.[^12] 
Another aspect of the dataset stood out to me when I started to look through the images. These were not pictures in the traditional sense, their aesthetical value did not matter as they are simply a tool to to accomplish a task. They are not made ‘to look at’ instead these images of hands are produced for a computer to analyze. Harun Faroki called these types of images ‘operative’ in his three-part series *Machine/Eye* where he examined how military technologies like guided weapons produce images that serve only the utility of the machine. As Aud Sissel Hoel puts it: “operative images are devoid of social intent, that they are not meant for edification, and nor for contemplation.” [^13]
But what happens when you contemplate on the hand dataset was remarkable to me. When I looked through the images quick enough I could not only see the motion of the test subject as the images were selected frames from a video, but I also started to imagine the person behind the hand and apply my own stereotypes and biases that far expanded the labels of the dataset. I could easily imagine the scene around the camera with a couple researchers who assembled a make-shift photo studio in the office of their lab. They greet the subject with a handshake, which is probably a fellow student, and explain the procedure quickly to get them to sign the paper that their anonymized personal data will be published for scientific research. Then the person puts both their hands under the camera spread their fingers and leave. 
This dichotomy between the label and my own narrative inspired me to create a video piece featuring the unaltered dataset. I used the default mac computer speech synthesis to read out the labels corresponding to each hand, and sped it up to fit into a 26-minute-long video. As viewers witness the participants holding their hands into the camera and spreading their fingers, they are met with a monotone, beat-like soundtrack of repeating words like “fair” and “medium” and occasionally “nail polish”.[^14]
![Example of a frame from 11k hands](Matthias%20Schäfer%20-%2011k%20hands%20[snJsKFxPlJ8%20-%20892x669%20-%203m35s].png)

At the 36th Chaos Communication Congress I organized a workshop called “Palm Reading AI” where I invited visitors to read the hands of people from the 11k hands dataset. At first I only introduced them to palmistry using wikihow as a reference.[^15] Then I handed out some prints where a random hand from the dataset was depicted and participants had to fill in a couple of questions. Some questions were short guesses like age, gender, country of origin, for some other they had to come up with fictional stories with only the hand lines as a reference: what is the persons future? How was their childhood? How is their love life?
After they filled in the form some people shared their stories and then I revealed where these hands came from and how computer scientists are using them to create models that try to predict their gender. Afterwards we had a discussion about the practice of creating large datasets and their ethical considerations. I had a longer talk with one participant that did not want to guess the age or gender of that person and I had told them that this was exactly the point of the workshop: to reflect on our own biases and stereotypes and how they translate into science.

After long contemplation on 11k hands and finding datasets that are much more problematic than this one, I don’t think the type of work from Afifi et al. is ‘unethical’ or needs to be redacted. They got consent from their subjects and share the dataset to the scientific community for “reasonable academic fair use”. The work on biometric identification and comparing CNNs to previous methods is interesting and novel, however as I stated above I think the premise behind the gender recognition task is flawed. Unfortunately this is very common in the computer vision field where people are (mis-)labeled that reflect and amplify societal stereotypes.[^16]
My research on the hands dataset in conjunction with esoteric practices and fortune-telling informed a later work of mine “The Chiromancer” that I built together with Giacomo Piazzi.

## In the wild

The process of collecting and creating data has drastically changed since the wide adoption of the internet. Where the 11k hands dataset has invited participants to their office to take a picture specifically for the dataset, other researcher started to search and download huge collections from the internet without any consent.
Take for example the ImageNet dataset, initiated by Stanford University’s AI professor Fei Fei Li, which was created to tackle object recognition tasks and eventually consisted of 14 million images.[^17] The team queried multiple search engines with common nouns and multiple translations to get around 500-1000 images per category. The categories are derived from an older project called WordNet that tried to create a hierarchical ontology of words. WordNet starts with broad categories like  


In 2019 the artist Trevor Paglen and researcher Kate Crawford collaborated on an exhibition titled Training Humans, dedicated to human image databases. One of the main exhibits was a vast collection of human images from the ImageNet dataset,  
Some images of people fall into categories such as “Bad Person, Call Girl, Drug Addict, Closet Queen, Convict” and so on (Crawford and Paglen 2019). The artists used these absurd, racist, and misogynistic labels to train “ImageNet Roulette”, a recognition algorithm that was accessible online and in an interactive installation. A result of the media attention that followed was that 600,000 images were removed from ImageNet and, due to maintenance (Yang et al. 2019).

[i](#sdendnote1anc)See _Fondazione Prada, “KATE CRAWFORD | TREVOR PAGLEN: TRAINING HUMANS”,_ [_http://www.fondazioneprada.org/project/training-humans/?lang=en_](http://www.fondazioneprada.org/project/training-humans/?lang=en)

[ii](#sdendnote2anc)See _Trevor Paglen,_ ImageNet Roulette, [_https://paglen.studio/2020/04/29/imagenet-roulette/_](https://paglen.studio/2020/04/29/imagenet-roulette/)

[iii](#sdendnote3anc)On March 11 the team updated [https://image-net.org/](https://image-net.org/). They addressed the criticism by removing 2,702 synsets and experiment with blurring faces to preserve people’s privacy.

## Informed Consent
- This Person Does Exist

[^1]: [@rosenblattPerceptronProbabilisticModel1958]
[^2]: The Mark I was a electromechanical machine that used motor driven potentiometers to adjust the variable weights.
[^3]: Frank Rosenblatt died in a boating accident in 1971. A couple years prior Marvin Minsky heavily criticized the mathematics behind perceptrons and advocated for a symbolic approach. These turn of events might have lead to a lack of funding in the ‘connectionist’ AI research field and ultimately lead to a general disinterest when the symbolic approach could not keep their exaggerated promises.
[^4]: [@mitchellArtificialIntelligenceGuide2019], p. 114
[^5]: [@fukushimaCognitronSelforganizingMultilayered1975]
[^6]: [@fukushimaNeocognitronHierarchicalNeural1988]
[^7]: [@tappertWhoFatherDeep2019]
[^8]: [@krizhevskyImageNetClassificationDeep2017]
[^9]: “One epoch takes 35 GPU minutes but more than 35 CPU hours.” [@ciresanFlexibleHighPerformance]
[^10]: [@afifi11KHandsGender2018]. Data and source code available here: [@afifi11kHands]
[^11]: In 2018 Parker et al jokingly tested the Galls theory using 21st century scientific methods and MRI data. [@dempsey-jonesNeuroscientistsPutDubious2018]
[^12]: I didn’t do any testing of the system as I don’t know how to run MatLab code, but I can imagine that the slightly better results on the dorsal hand are the result of nail polish only applied on female hands. 
[^13]: [@hoelOperativeImagesInroads2018]
[^14]: See: [@matthiasschafer11kHands2018]
[^15]: [@HowReadPalms]
[^16]: One particular famous example of this is the work by Michael Kosinski and Yiluna Wang. Their flawed study tried to predict if a person is gay by scraping dating sites and training a classifier on these images. See:[@murphyWhyStanfordResearchers2017]
[^17]: ImageNet started with 3,2 million images and had the goal to collect 50 million by the end of 2011. [@dengImageNetLargeScaleHierarchical2009]