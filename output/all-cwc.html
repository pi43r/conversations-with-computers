<h1 id="hello-world">Hello, World!</h1>
<p>The common first letters a programmer types into its digital computing machine is some form of ‘hello world’ along with braces, punctuation, and words the machine should interpret. In my favorite language, JavaScript, it can look as simple as this <code>console.log(“Hello, World!”)</code>. After executing this short string of text in the browser, the computer will perform a series of events and likewise respond with ‘Hello, World!’ in the developer console.</p>
<p>This ritual of greeting each other (and the world) goes back to Brian W. Kernighan, who wrote the popular guide <em>The C Programming Language</em><a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> in 1978, where he proclaims that the only way to learn a programming language is by writing programs in it.</p>
<p>I start this introduction with the same greeting, and as I press these letters on my keyboard, they simultaneously appear on the screen in front of me. This translation from the tactical key press to the pixels on the screen becomes possible through a chain of electrical signals that reach my retina in a matter of milliseconds and close the loop. Thousands of people have created these intricate systems which interoperate and depend on each other, each new layer abstracting underlying operations further. Only the text editor I am using to write these lines already has hundreds of contributors writing thousands of lines of code, which in turn rely on the functions of the operating system created by kernel developers.</p>
<p>This chain of creativity represents the core of open source software development based on collaboration and sharing knowledge. Every contribution, no matter how small or insignificant it may seem, is valuable in creating something more significant than what was there before. Whether it is a bug fix, a feature request, or a new idea - every contribution makes a difference.</p>
<p>In this thesis, I focus on the known and unknown human connections we make with and through computers, specifically in machine learning. My hypothesis is that the current trend of framing ‘Artificial Intelligence’ as autonomous computational beings would be better described as ‘Co-Intelligence,’ where programming such systems is a collaborative effort with many hidden actors. While I address some technical details of current machine learning systems, I will put them into a social context because software inherently is a social tool. To describe the social inheritance, I use my own and other artists’ artworks to gain a broader aesthetic perspective, which can go beyond logic and language.</p>
<p>Even though I focus on the social nature of computing and am influenced by many of my peers, I am describing these topics from my subjective perspective. I don’t claim any objective truths on the descriptions of my collaborative works and am naturally biased toward my contribution to them. As I write this thesis in the context of an art program, I deviate from rigorous scientific methods and describe my observations more freely. Instead of trying to answer the question of the nature of AI as subjective entities, I focus on the practices we developed through the engagement with that question.</p>
<h2 id="dreaming">Dreaming</h2>
<figure>
<img src="6ocuQsZ.jpeg" alt="“PuppySlug” - posted by a deleted user on /r/creepy with the title “This image was generated by a computer on its own (from a friend working on AI)” on January 10th, 2015. Seven days before the Google blog post" /><figcaption aria-hidden="true">“PuppySlug” - posted by a deleted user on /r/creepy with the title “This image was generated by a computer on its own (from a friend working on AI)” on January 10th, 2015. Seven days before the Google blog post</figcaption>
</figure>
<p>My journey into the hype machine of ‘Artificial Intelligence’ started around 2015, when Google released DeepDream<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> on its research blog. The developers Alexander Mordvintsev, Michael Tyka, and Christopher Olah were initially trying to peek inside an ‘Artificial Neural Network’ that was ‘trained’ for image classification tasks. They reversed the task of the classifier, and instead of identifying what a set of pixels look like, the pixel values are changed to look more similar to what the network has ‘learned.’ This created images that amplified the textures embedded in their networks, transforming the input image into swirls of PuppySlugs.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> This imagery that reminded people of bad dreams and hallucinogenic trips was widely shared on the internet, which skyrocketed the awareness that ‘Deep Learning’ and biologically inspired computing have returned from their hibernation during the AI winter.</p>
<p>In 2012 ‘Artificial Neural Networks’ made an incredible comeback into the field of computer vision. A team around Fei-Fei Li created ImageNet,<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> a dataset of images organized into categories by anonymous workers from the internet. The ImageNet project has held a yearly competition since 2010. For the first two years, the best algorithm recognized these images with a 74% accuracy until an ‘Artificial Neural Network’ topped the score with 85% accuracy and woke the computer vision community to their potential powers.</p>
<p>With DeepDream, this technique became tangible for the general public as it created something to look at, something for humans to empathize with the outputs of an abstract mathematical model. This tendency to anthropomorphize this type of software becomes apparent in how we use language to describe it. We talk of ‘Neural Networks’ as biological entities that can be ‘taught’ and which ‘learn,’ ‘experience,’ and ‘read’ data from the world. This turning point got me and many other artists interested in ‘collaborating’ with seemingly autonomous machines. Machines that we do not have to understand through mathematics but through interpretation and experimentation of ‘their dreams.’ Yet, the outputs of DeepDream quickly became kitsch. Google shared the code with an open source license, and many people created apps and APIs to generate PuppySlug textures on top of their own images.<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> As with most other memes on the internet, the hype around DeepDream died quickly, and by the end of 2015, the world was already saturated with computer-generated hallucinogenic images. Other researchers have since picked up<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> on visualizing nodes in ‘Artificial Neural Networks,’ and Alex Mordvintsev—together with his wife—has since become an artist, exhibiting in art fairs.<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> What stayed was the notion that ‘Artificial Intelligence’ became some form of computational being different from the humans who wrote the code, labeled, and sorted the data.</p>
<h2 id="terms">Terms</h2>
<p>I have deliberately used quotation marks around some of the previous words, as I am critical of the language used when discussing AI systems. In his book <em>The Artist in the Machine</em> Arthur I. Miller concludes that machines could be seen as creative and will be considered “artists, writers, and musicians in their own right.”<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> He is using a typical techno-utopian argument that the technology is not quite there yet to <em>really</em> be creative but that it will change in the foreseeable future. At the same time, Miller explains in great detail the actual creative work of Mordvintsev and his human experience of insomnia to come up<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a> with the generative system for DeepDream and how he shared it with his peers at Google.</p>
<p>Alex Mordvintsev was not collaborating with his computer; he created an emergent algorithmic system with it. The software does not hang pictures on exhibition walls, talk to gallerists, and has no agency in the process if it were to generate images or not. Peter Weibel has formulated it bluntly: Artificial intelligence does not exist. But an ensemble of machines, media, programs, algorithms, hardware, and software has resulted in an extraordinarily large, diverse, and productive field of research called AI.<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a></p>
<p>Where Arthur Miller is feeding the narrative of humanoid robots with glowing blue brains,<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a> most computer scientists in the field of artificial intelligence today are working in the subdiscipline of ‘Machine Learning.’ The terms have converged in the media landscape in the last ten years. Still, where intelligence is rejected as a suitcase word with a multiplicity of meanings, ‘Machine Learning’ tries to define the task more narrowly to some form of pattern recognition and extrapolation of existing data. They use a variety of computational and statistical techniques to form abstract models for domain-specific problems. In these fields, ’Artificial Intelligence’ is a buzzword to convince governments and venture capitalists to fund projects. The artist and researcher Francis Hunger recently shared a list of alternative terms in an attempt to dehumanize our language for AI systems:</p>
<ol type="1">
<li>‘Artificial Intelligence’ =&gt; ‘Automated Pattern Recognition’</li>
<li>‘Machine Learning’ =&gt; ‘Machine Conditioning’ OR ‘Automated Classification’</li>
<li>‘Neural Network’ =&gt; ‘Weighted Network’</li>
<li>‘Deep Learning’ =&gt; ‘Deep Conditioning’</li>
<li>‘Neuron’ =&gt; ‘Weight’ or ‘Node’<br />
<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a></li>
</ol>
<p>In an accompanying talk<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a>, he explains the aim of those terms is to invoke passivity, that we deal with machines and humans set those machines in motion, even when, in the end, we form ‘human-machine assemblages.’ I like many of the proposed terms, even though they are still closely associated with biological operations. The conditioning of machines, for example, reminds me of Pavlov’s dog experiments or B.F. Skinner’s box to modify pigeon behavior by reinforcement or punishment.<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a> This analogy serves well to create an image of the machine as a system that can be controlled by changing the parameters of its virtual environment. Therefore I will use ‘conditioning’ and ‘weighted networks’ where applicable.</p>
<p>Using ‘Automated Pattern Recognition’ instead of ‘Artificial Intelligence,’ however, becomes too narrow of a definition and is counterintuitive to me, as AI serves precisely the function of being ill-defined. Pei Wang has made a great effort to define the different strands of AI research and their working definitions.<a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a> He clusters them into Structure-AI (recreating the human brain), Behavior-AI (recreating human actions), Capability-AI (domain problem solving), Function-AI (developing cognitive functions), and Principle-AI (finding underlying principles) and comes up with a working definition to where AI research should be headed and how it can be unified. My initial goal for this thesis was to avoid the term’ Artificial Intelligence’ altogether, but as I have already failed in that task and coming up with a less anthropomorphic term does not seem feasible to convey the research in the field and its media reception. Therefore I will keep using the abbreviation AI.</p>
<p>Contrary to articles and sci-fi novels, I will not use the personification of ‘an AI’ but instead talk of AI systems, meaning complex emergent programs. Many of the systems today use statistical modeling. Yet, nobody would ask if statistics can be creative, so I decided not to engage with philosophical questions of consciousness and creativity. Instead, I want to explore how AI systems can include the human knowledge and work that goes into building them, challenging the AI ideology of machine autonomy and proposing human-centric goals rather than creating a machine as a goal in itself.</p>
<h2 id="getting-started">Getting Started</h2>
<p>I structured this thesis around four chapters, combining historical, computational, and collective knowledge. Throughout each chapter, I include examples of artworks by me or others that either relate to the topic or support the argument.</p>
<p>First, I am revisiting collective experiences I have organized with other artists, like the <em>Silicon Friend Camp</em> in the Austrian mountains. This week-long retreat brought together 17 artists and researchers to focus on human-computer conversations and evolved into an exhibition and a symposium.</p>
<p>From there, I delve into the history of talking machines, from Wolfgang von Kempelen’s speech automatons to today’s digital assistants and the social changes that come with them. Talking to computers sets the foundation for my definition that these interactions are not with, but through technology, with other humans.</p>
<p>The next chapter explores the building blocks for complex statistical modeling in AI systems. Researchers require large datasets, often gathered through ethically questionable means by aggregating data from internet users. A closer look is taken into the StyleGAN dataset as the model that emerged from it was a turning point in synthetic image creation.</p>
<p>Similarly, GPT-2 generates seemingly coherent text based on scraped websites and books. So the last chapter focuses on the underpinning architecture of the model and its ability to exploit the human drive to find meaning in symbols. I conclude with The Chiromancer, an artwork that explores how we place trust, hopes, and wishes into our cold silicon companions.</p>
<hr />
<h1 id="co-intelligence"><strong>Co-Intelligence</strong></h1>
<blockquote>
<p>Ubuntu: I am because you are<a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a></p>
</blockquote>
<p>The cognitive scientist Abeba Birhane takes the South African humanist philosophy of <em>ubuntu</em> as an argument against a Western tradition of the <em>self</em> as autonomous beings.<a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a> According to <em>ubuntu</em> a person becomes a person only through relationships with others. The interactions we have with each other shape our personality, and it is fluidly shifting between different states. I am a different person when talking to my mother, my friends, and strangers on the street. At the same time, my upbringing and cultural influences define my own self-image. Birhane traces back the flaws in Western theories of the mind to René Descartes’ ‘cogito ergo sum’ – I think, therefore I am. In his famous meditation, the French philosopher tried to strip back all things of uncertainty to arrive at the foundation of inner thought as proof of existence. In turn, this individualistic ‘I’ and the method of logical reasoning have become a success story in Western sciences. The idea of a separate mind and body was already present in Plato’s dualism. The scientific method of breaking down complex natural phenomena into simplified quantifiable parts is at least as old. In the past 200 years, scientists, philosophers, and theologists defined the ‘body-mind problem,’ coming up with theories of how the immaterial ‘soul’ interacts with the fleshy human body: Why are we conscious of the world around us, ourselves, and others?</p>
<p>We concluded that the brain must do the thinking<a href="#fn18" class="footnote-ref" id="fnref18" role="doc-noteref"><sup>18</sup></a>, and using deductive reasoning, the brain was separated into smaller parts until synapses and neurons were defined and described. But the physical functioning of the brain does not explain the emotional experience of the self. The groundwork was done by the mathematician Gottfried Wilhelm Leibniz, who proposed the existence of fundamental ‘monads’ that transcend all matter in the universe, which function as an intermediary between mind and matter. Sir John Eccles applied this theory to the model of synaptic actions he developed through experimentation with electricity and the brain; he called the mysterious forces through which we control the brain ‘psychons.’ But neither psychons nor monads served as a sufficient explanation for the bridge between mind and matter.</p>
<p>Dualism continues to significantly impact how we think about artificial intelligence and consciousness. For example, the influential cyberneticists Norbert Wiener and Claude Shannon both based their work on the assumption that information is a separate entity from matter and energy. This assumption has led to a disembodiment of information and a false separation of the physical world (the ‘hardware’) from the world of ideas (the ‘software’).<a href="#fn19" class="footnote-ref" id="fnref19" role="doc-noteref"><sup>19</sup></a> Following a materialist logic, consciousness becomes information embedded in the physical substrate of the brain. In that sense, we only need to measure all physical activities to duplicate and simulate the human experience. The metaphor of the brain as a computer defines our present and justifies predictions of ‘downloading’ consciousness and achieving digital immortality. It culminates in the popular simulation hypothesis, which proposes that we live in a simulated environment.<a href="#fn20" class="footnote-ref" id="fnref20" role="doc-noteref"><sup>20</sup></a> Computational neuroscientists use and develop instruments to quantify electric impulses in the brain and create massive amounts of experimental data, which in turn need statistical models and ever-growing computing power to be analyzed. On the other spectrum, simplified models of the brain in the form of artificial ‘neural networks’ are used to compute and sort higher-order abstractions. This focus on the individual brain in the search to replicate intelligence leaves aside a physical and social environment in which conscious beings are embodied.</p>
<p>To understand human cognition and intelligence, we need to explore the complex dialogic connections between human actors. Current machine learning programs abstract intelligence into simple task-solving mechanisms, making us believe that the underlying algorithm becomes a rational agent, but on closer examination, the program reveals an astonishing amount of human collaboration: from the creation of programming languages as protocols for interaction to the enormous amounts of data aggregation and labeling. Instead of looking at AI systems and seeing them as subjective entities, we can also see them as artifacts of collective cognition. By embedding current machine learning algorithms back into the social structure, I hope to gain experimental insight into how these new forms of collective computations are used as a creative medium. In the following, I will present my experiences working within Co-Intelligent systems in different constellations.</p>
<h2 id="metaconversations-with-computers">Metaconversations with Computers</h2>
<figure>
<img src="Flowchart---Metaconversation-Piece-stitch.jpg.png" alt="Metaconversations, Miro Flowchart" /><figcaption aria-hidden="true">Metaconversations, Miro Flowchart</figcaption>
</figure>
<p>In October 2020, I participated in <em>Freeport 1: Anatomies of a black box</em><a href="#fn21" class="footnote-ref" id="fnref21" role="doc-noteref"><sup>21</sup></a> and was part of a group with Gabriela Gordillo, Mario Romera, and Fabian Frei. We started a conversation about computers that ended up simulating our internal conversations.</p>
<p><em>Anatomies of a black box</em> was a laboratory curated by Bani Brusadin for Matadero Madrid about mapping contemporary complex systems in infrastructures of exploitation. During the month-long program, we met in video conferences with Vladan Joler and other groups to explore mapping as a form of non-linear storytelling. The starting point was the conceptual map ‘Anatomy of an AI system’<a href="#fn22" class="footnote-ref" id="fnref22" role="doc-noteref"><sup>22</sup></a> that he created with the researcher Kate Crawford as a case study of the life cycle of an Amazon Echo device, from unearthing the raw materials to its networked software components.</p>
<p>During the <em>Freeport</em> program, we, like most others during continuous lockdowns, have only engaged with each other through our screens. Initially, we decided to look closer into how we communicate with technological devices. First, we wanted to focus on conversations in natural language, as speaking to computers is an emerging phenomenon. After discussions on Signal and Jitsi<a href="#fn23" class="footnote-ref" id="fnref23" role="doc-noteref"><sup>23</sup></a> that opened more questions than we could ever answer, we ended up in high-dimensional constructions. Our thoughts revolved around the heuristic measures of current machine learning models. Specifically, those dealing with language. Large language models, which use texts from the internet to approximate a pattern in word tokens, patterns that can generate endless amounts of coherent-sounding text. Even though these models have many flaws, we wanted to understand how we could use these prediction machines to understand what we were chatting about. A traditional text analysis approach would center around creating graphs and networks from the dataset we collectively created through conversations. Instead, we chose to mix it up with the pre-existing language model GPT-2 and analyze the output it generates instead. These outputs come in the form of a chat log. Statistical constellations from the hyperspace were conversing about mostly nonsensical topics, often getting stuck in repetition. We asked ourselves what we could learn about our fragmented selves from these synthetic conversations.</p>
<p>The interpretation of ourselves through this machine-learning lens was straightforward. Quite often, the patterns of who speaks and what we speak about seemed logical and in line with our real characters and the conversations we had. Yet, we know that we are biased toward finding our own images in these generated text blocks. The model acts as a parrot that lets us rediscover what we spoke about and creates a distance through which we can interpret our written conversations as if they were someone else’s. The output is not coherent, but this imprecision is what makes reading the generated exchanges attractive.</p>
<p>To communicate this process, we re-enacted the computer-generated phrases in one of our weekly online meetings. The performance makes a full circle by completing the generated output with the appearance and experiences of our bodies in front of our computers.</p>
<p>The exercise of using an algorithmic model in combination with our dataset is a way of looking at our group process with a tool that exposes its statistics as a probability projection in the form of verbal language. The output reflects the rhythms and patterns in our communication that worked as a fictional mirror for us. Resembling the logic of dreams, we complete their meaning with our interpretation and connotations; we observe what we want to see.</p>
<pre><code>How exactly do these black box systems work and how are they perceived by a user.txt


Mr: I might be too absorbed in my project to finish our Q&amp;A
Mr: lets make of it what we see is funny
G: Happily.
G: No homework tomorrow, tomorrow for science!
G: I leave you with a cartoon I did a while back about the mind of a button : https://m.youtube.com/watch?v=h7BFToUaq4Y
F: what is the scientific word for &quot;intelligent&quot;... is it cognitive, is it behavioral, semantic, artistic, etc.? is it hard to pin down, or is it a mix?
G: I can’t think of a good one right now. My favourites are &quot;of theses&quot; and &quot;tractations&quot; - they’re not tied to a research topic, but can be applied to any scientific question Mining Simulations: Theory and Experiment Design Automatic Feature Selection: Experimental Methods Multidimensional Pruning: Descriptive Bayes Categories: Human-Computer Interaction
G: :)
Ma: I think &quot;multidimensional&quot; is misleading. There are actually 3 dimensions to everything: a physical thing, an process of information and control, and a human interaction element. These 3 dimensions can be broken down into &quot;physical&quot; (things we are) &quot;mathematical&quot; (computations &amp; algorithms used to make them) and &quot;social&quot; (the way humans interact with other humans).
G: sorry
G: (I am using &quot;multidimensional&quot; in a cartoony way)
Ma: Cartoons are multidimensional. Most people are tall and thin. Most people are also obese.
G: Ahh yes. &quot;multidimentional&quot; is a nice word to describe this kind of thing.
G: (I think &quot;multidimensional&quot; is a bit narrow)
Ma: (I am using the term &quot;multidimensional&quot; in a cartoony way)
Ma: (I am using the term &quot;multidimensional&quot; in a cartoony way)
Ma: (using the word &quot;multidimensional&quot; in a scientific way)
G: (I think the other 2 are just fine for me)
Ma: Mixture: Theory Iteration: Experiment Design Image Identification: Feature Selection: Topic Analysis: Language Creation: Syntax Fish: Multidimensional Belief Systems: Belief Records for Computer Aided Speech (Basescim): Multidimensional Validation: The Bell Curve: SphereMapping: Element relations: Graphs: Anonymous scripting: Language models: Language learning: Syntax blind: Interactives: Syntax escapists: Syntax junkies: (I didn’t type much) (I should change the last line) (I am tired) (I am sleepy) (I have nothing to say)
G: Ahh, I love it. Now I have to take care of the bad lines
G: I love it when he writes in his slowness
Mr: https://twitter.com/awokehuman/status/1328183467595718656
Mr: hi
G: hi
G: do we send it as a jpeg?
F: i think on inet message is enough
G: colors...
Mr: just in case
Mr: green, black, yellow, red, green, yellow, black, red
Mr: these</code></pre>
<h2 id="silicon-friend-camp">Silicon Friend Camp</h2>
<p>The Silicon Friend Camp was a 5-day working retreat in Gosau, Upper Austria, for artists and researchers working on Artificial Intelligence. The summer camp was organized by servus.at with funding from LINZimPULS. Together with the curator Davide Bevilacqua, we started to develop the program in the summer of 2020. The artist Rosi Grillmair joined as a facilitator and program supervisor in early 2021.</p>
<p>Due to changing pandemic conditions, we rented a vacation home in the mountains so that the risk of infection from the outside could be kept to a minimum despite close contact between participants. So instead of holding the program as a hackathon in the city in the spring, we moved it to the summer, where it took place from July 20th to July 25th.</p>
<p>To create a selection of artists, we published an Open Call, distributed as a website<a href="#fn24" class="footnote-ref" id="fnref24" role="doc-noteref"><sup>24</sup></a> designed by Alyona Ciobanu. Inspired by a previous call for a residency at Babycastles<a href="#fn25" class="footnote-ref" id="fnref25" role="doc-noteref"><sup>25</sup></a>, we created a form that does not ask for CVs or portfolios but focuses on a project idea and what kind of knowledge the participant seeks from others and is able to share. Thirty-one artists based in Europe and the Americas responded to the call, of which we selected 12. We chose the artists with a view to the diversity of their projects, origin, and gender. In the end, a wide range of people from China, Japan, the USA, Brazil, Colombia, Russia, Germany, Poland, and Slovenia gathered in Austria. The planned projects ranged from remote-controlled robots to philosophical debates to music pieces with embodied AI.</p>
<p>But before we physically met in Gosau, an internet forum<a href="#fn26" class="footnote-ref" id="fnref26" role="doc-noteref"><sup>26</sup></a> was set up for participants to introduce themselves and initiate discussions and suggestions on content. The forum also served to communicate organizational necessities in an easily accessible and quick manner. Apart from the asynchronous text-based interaction, we held two video conferences in which Rosi read a story on narratives of AI that served as a basis for discussion.</p>
<p>One participant from Colombia joined us from the Andes mountains during the project week and stayed in contact with us only through the forum and video conferences.</p>
<figure>
<img src="Screenshot_20210721-201648_Instagram.jpg" alt="Video Call with Sebastián Mira|150" /><figcaption aria-hidden="true">Video Call with Sebastián Mira|150</figcaption>
</figure>
<p>Our preparation for the schedule of the physical work session involved the participants by asking them to contribute to discussions, workshops, and presentations. Still, we also gave them enough flexibility to decide on individual and group projects.</p>
<p>The first day got off to a leisurely start, with 10 participants: arriving at the train station throughout the day and being picked up and brought to the house. Together we slowly cleaned the garage, attic, and living room. While Hess Jeon, who took care of the food for the next five days, prepared dinner. We set up a table in the garden and used the time to introduce the project and welcome everyone. Before dinner, I generated a daily meal prayer with GPT-3 that we recited together.</p>
<pre><code>Eating is a sacred, human right
But food is getting scarce
There are humans who are hungry
And there are humans who are fat
We must ask ourselves,
Who are the humans?
I am the human
You are the human
We are the human
Because I am the human,
You are the human
We are the human
Because you are the human,
I am the human
Because we are the human,
You are the human
Because I am the human</code></pre>
<p><img src="IMG_0336.jpg" alt="Meal Prayer | 400" /> We used the following morning for a group meeting in which we established general rules and defined the goals of the camp together. The participants presented their projects and what they wanted to achieve in the following days.</p>
<p>In the afternoon, we used our time independently. While some people sank into their laptops, others explored the surrounding area around the house. We built up an area-wide network and cleared the attic and the garage to be used as working places.</p>
<p>Towards the evening, a daily <em>Group Validation</em> took place. The participants individually summarized what they had done that day, what they planned to do next, and what they needed help with.</p>
<p>After dinner, we spent time together in the attic, where we installed carpets, mattresses, speakers, and a projector as a <em>Digital Campfire</em>. On the first evening, we were engaged in storytelling around anthropomorphic beings. In between, the sound duo EKHEO played live electronic music.</p>
<figure>
<img src="2021_07_22_selforga_brighter.jpeg" alt="Self Organizatioon" /><figcaption aria-hidden="true">Self Organizatioon</figcaption>
</figure>
<p>Day 3 began without electricity, which was especially tragic because we scheduled a video call with researcher and artist Caroline Sinders. With some delay, we got reception back to the internet. We could follow Caroline’s talk about the impact, misuse, and politics of ‘artificial intelligence’ on society, which we followed up with a discussion on how we can view machine learning models through a feminist lens.</p>
<p>The rest of the day was self-structured, and people used it to prototype their projects. At night in front of the <em>digital campfire</em> Dasha Ilina presented her work dealing with the inconveniences of technological devices. Afterward, Naoto Hieda showed improvised choreographies created with dance and live code.</p>
<p>On the morning of Day 4, three texts were presented for discussion in a reading circle before dinner. The texts dealt with self-representations in global networks, intelligence without cognition, and root networks as a metaphor for the mind.<a href="#fn27" class="footnote-ref" id="fnref27" role="doc-noteref"><sup>27</sup></a></p>
<p>Since this was the last full day to finish our projects, the participants used it intensively. Many sat in front of their laptops working, for example, on a film they had previously shot or trying out AI systems, such as voice cloning and generative image models, with which to realize their visions.</p>
<figure>
<img src="210721-SFC-_0010660.jpg" alt="Artists working | 350" /><figcaption aria-hidden="true">Artists working | 350</figcaption>
</figure>
<p>The <em>digital campfire</em> in the attic also intensified in the evening. After an introduction to artificial neural networks by Błażej Kotowski, Giacomo Piazzi gave a presentation on the history of AI. Mariana Marangoni spoke about esoteric programming languages that challenge accepted definitions of code and language. Lastly, Davide Bevilacqua highlighted greenwashing methods used by well-known internet companies.</p>
<figure>
<img src="IMG_20210723_234140.jpg" alt="The Marvelous World of Esolangs | 300" /><figcaption aria-hidden="true">The Marvelous World of Esolangs | 300</figcaption>
</figure>
<p>The morning of the last day was used to finalize projects, which were presented after lunch. Dasha Ilina, Erica Jewell, and Lina Schwarzenberg presented <em>GRAVE</em> a video as a Public Service Announcement to prepare access to digital devices and services for posterity. Naoto Hieda and So Kanno explained how they rebuilt a toy robot and made it controllable via the internet. EKHEO performed a piece for which they developed synthetic voices from all participants. Błażej Kotowski showed a video about a fictional deity in which text and images are generated through machine learning algorithms. Yuxi Liu explained how she used a camera to create a dataset of birds. Mariana Marangoni conceptualized a programming language that could be grown like a tree.</p>
<p>We spent the rest of the afternoon at a mountain lake, where we got caught in a storm, which—at least for me—was a terrifying experience. But after drying up in the house and eating freshly baked pizzas, the rest of the evening turned into a dance party. Still, Maks Valenčič gave another lecture on <em>The End of Philosophy</em> with a soft soundtrack by Błażej. Sebastián Mira also joined us from Colombia and showed us a digital world in which he connected the Andes with the Alps. Finally, I used GPT-3 again to print Certificates of Excellence that I handed out in a little ceremony.</p>
<p>The next day we cleaned the house and already parted. However, our short encounter together would continue. Friendships have formed, and we organized further collaboration on an exhibition in Linz online.</p>
<h2 id="camping-with-computers">Camping with Computers</h2>
<p>The exhibition took place in the WHA Gallery from 10th to 19th November 2021. It tied the projects we developed during the summer camp into a coherent presentation, where the distinction between the individual artist and collective effort is broken down. To achieve this, Davide Bevilacqua and Giacomo Piazzi created an exhibition design inspired by camping trips. With metal rods, camping stools, tarp, rocks, and string, they—quite literally—tied the works together. We intentionally left out exhibition signage that explains each of the artworks so that a visitor would find themselves exploring the exhibition, trying to make sense of it holistically. The information about individual artists and their works was presented online, where the show was mirrored for a global audience.</p>
<p>I worked on the visual identity and was inspired by the design of an old computer, the Canon Cat, from 1987. It was the creation of Jef Raskin, who also invented the user interface of the first Apple Macintosh.<a href="#fn28" class="footnote-ref" id="fnref28" role="doc-noteref"><sup>28</sup></a> The Canon Cat, on the other hand, was not a commercial success. It took away the mouse and streamlined the user interface to be centered around word processing with extra keys for fast shortcuts. It was mainly marketed as a productivity tool for office workers. The gray and beige color scheme with accents in red and blue and the handwritten signature gave the computer a remarkably friendly face. I transformed this color scheme into a contemporary website with a simple generative logo that randomly exchanges some characters with emoticons, like smiley faces and hearts. The posters were also generated in the browser, and a QR code was embedded to the exhibition website. With a generative text-to-image model RuDALL-E,<a href="#fn29" class="footnote-ref" id="fnref29" role="doc-noteref"><sup>29</sup></a> I created uncanny ‘stock images’ that were randomly placed and stretched.</p>
<figure>
<img src="20211004_171321.jpg" alt="Canon Cat with extracted colors | 300" /><figcaption aria-hidden="true">Canon Cat with extracted colors | 300</figcaption>
</figure>
<figure>
<img src="13CampingwithComputers.jpg" alt="Example of a Poster | 300" /><figcaption aria-hidden="true">Example of a Poster | 300</figcaption>
</figure>
<p>The connection between online and offline was most notable in the work of So Kanno with the title <em>Crawler</em>. It was a continuation of the remotely controlled toy robot he developed during the camp and a body of work he calls <em>Avatars</em><a href="#fn30" class="footnote-ref" id="fnref30" role="doc-noteref"><sup>30</sup></a>. This time he modified an Apple Macbook so that it could be controlled from the internet. He attached wheels on a custom laptop stand so that a visitor to the website could roll around the exhibition space. Additionally, the user interface was heavily customized, using notifications, web browsers, and two separate webcams with face tracking to cover the faces of our visitors. The <em>Crawler</em> functioned as an autonomous object inside the gallery space and could be seen as ‘an AI’ for video surveillance because the actual people who control the device are hidden inside this gray box. With this, it resembles the faux AI practice by the Colombian-owned startup Kiwi Campus, which builds a ‘self-driving’ delivery robot for US Universities. The Kiwibot is first filled by a regular delivery driver on a bicycle and then slowly rolls a few hundred meters to its destination under the supervision of a human operator.<a href="#fn31" class="footnote-ref" id="fnref31" role="doc-noteref"><sup>31</sup></a></p>
<figure>
<img src="crawling.jpg" alt="Screenshot of the Crawler Interface" /><figcaption aria-hidden="true">Screenshot of the Crawler Interface</figcaption>
</figure>
<p>Another work that juxtaposed digital and physical elements was from Sebastián Mira. The Colombian artist created 3D reconstructions of our laptops as avatars to be used in a fictional place where the Andes and the Alps meet. Throughout the exhibition space, laptops were arranged as sculptural objects with self-referential video loops, where you could see a laptop randomly rolling around computer-generated landscapes. The videos were taken from the game <em>Everything</em><a href="#fn32" class="footnote-ref" id="fnref32" role="doc-noteref"><sup>32</sup></a>, in which a player can control various lifeforms and inanimate objects in procedurally generated worlds.</p>
<figure>
<img src="Opening_10.11.21(21).jpg" alt="A visitor viewing one of the laptops by Sebastián Miras | 300" /><figcaption aria-hidden="true">A visitor viewing one of the laptops by Sebastián Miras | 300</figcaption>
</figure>
<p>Naoto Hieda, in turn, took the 3D models of our laptop avatars and created a collage—a sort of ‘group photo’ of the participants and the toy robot mascot with me in the background—and turned it into a large banner. When I asked him if this might be too much of an inside joke, he replied that he did not care if anyone outside the group understood it because he wanted to make art for us. This comment made me think about the audience I try to address with my own artworks. When I work on projects, I think of a fuzzy global audience, something that can be written into history books and shown in museums, but why not make artworks just for my friends? Something so niche that only selected people understand it. This idea is so innocent and simple, yet it caught me by surprise.</p>
<figure>
<img src="Exhibition(22).jpg" alt="Naoto Hiedas Banner and Laptop showing the exhibition website | 300" /><figcaption aria-hidden="true">Naoto Hiedas Banner and Laptop showing the exhibition website | 300</figcaption>
</figure>
<p>I wanted to make something for the exhibition with the images we collected and shared online, pictures that document our time together, which we shared on the forum, in our group chat, and uploaded to the cloud. My initial thought was to train a GAN and create abstract synthetic images that fail to represent our collective experience. When I looked through the photo archive as a dataset, first as a large grid of pictures and then in rapid succession to identify outliers, I became awestruck: even though I spent less than a second with each image, some of them would trigger deep emotional responses and immediately transport me back to the house in Gosau. Thinking about Naoto’s statement of “making art for my friends,” I decided to use all 767 images in a rapid slideshow of 32 seconds. Each of the 24 frames per second is a different image, stretched to fit a 16:9 TV that got damaged on the way to the camp. I disassembled the TV and separated the LCD panel from its backlight, making the images only visible from a certain angle. The title of the work “we are the human” is borrowed from one of the meal prayers. All these elements comprise the work, which is both a slide show and movie, documentation and narrative, digital and analog.</p>
<figure>
<img src="Exhibition(17).jpg" alt="we are the human | 300" /><figcaption aria-hidden="true">we are the human | 300</figcaption>
</figure>
<p>The best viewing angle for my work was from a sunbed with a stereo speaker setup playing “LEEWA” from Ekheo. In this experimental sound piece, the artists used recordings of our voices to create a communal voice clone. With the <em>Oracle</em>, they devised a name and backstory for a new character and added anecdotal field recordings from the camp. Together these works create an eerie experience based on the digitally altered collective memories of our week in the Alps.</p>
<p><img src="Opening_10.11.21(16).jpg" alt="A visitor listening to LEEWA| 300" /> <img src="Opening_10.11.21(18).jpg" alt="View from the sunbed | 300" /></p>
<p>Next to these pieces is a CRT TV displaying a fire with a contraption for a pot hanging above it. The pot contains a printed recipe created by Hess Jeon with GPT-3 that we didn’t dare to cook for the opening.</p>
<pre><code>Chocolate Milk Pasta
The following is an algorithm for a vegetarian One-Pot-Wonder that was served at the “Camping with Computers” art exhibitions for silicon friends.

Ingredients:

1 stick butter

4 cups fresh veggies, chopped (see below)

1 cup dry pasta or broken spaghetti noodles

12 ounces water or chicken broth (chicken broth is better)

2 cups milk (chocolate milk is even better)

3/4 cup cheese (cheddar is best)

Assemble the ingredients and place them in a large pot over medium heat. Wait until the butter melts. Then turn up the heat. By now the butter should be sizzling and popping as it heats up the veggies and pasta. At this point you can add garlic, onions, corn, peas, or whatever else strikes your fancy. The key is to use fresh veggies that you like; this way you won’t notice that you’re eating veggies at all. Once the veggies and pasta are nice and hot (and they will be), add the water/broth and stir them thoroughly. Now the cooking begins! Put on a movie or text message competition with your friends and let them know you’ll be cooking for about five minutes. The trick here is to make sure that the pasta isn’t too soft; you don’t want it to disintegrate into a big blob of mush like most of your ex-girlfriends did. Now make sure you have a timer handy on your phone or on a wall clock or something. After five minutes have elapsed, turn off the heat, but leave the pot on the stove. Now for the next step: pour in the milk/chocolate milk and stir it vigorously for about 15 seconds. This will ensure that the cheese melts properly so that all its flavor can be absorbed by your tastebuds. Stir in the cheese until it’s completely melted, then serve it up! Serve with crusty bread so that you can sop up all of those yummy juices, but only use one piece of bread per person because it will fill you up fast. You want to save room for dessert! This recipe tastes best cold because then you can use that leftover cold chocolate milk to drink with it! If any is left over, put it in the fridge overnight so that it absorbs more of those yummy flavors. Then eat it cold for breakfast with a side of bacon or ham! Yum yum!!</code></pre>
<figure>
<img src="Exhibition(6).jpg" alt="Hess Jeon “Chocolate Milk Pasta”|300" /><figcaption aria-hidden="true">Hess Jeon “Chocolate Milk Pasta”|300</figcaption>
</figure>
<p>On another set of monitors, Yuxi Liu showed a 3-channel video installation in which she is exploring the non-human world. During the first Covid-19 lockdown, she used a motion-sensor camera to capture the birds visiting her balcony. This collection served as an image set for a GAN model, which after finetuning, was supposed to recreate the photos of the visiting birds. Instead, it abstracted the animal so much that merely the surrounding was visible. It served as an interesting metaphor for how digitizing the natural world and statistical operations on it sometimes disregard the life within. During her stay at the camp, Yuxi wanted to capture more animals with a DIY raspberry pi camera, and even though she tried her best to lure more birds, the camera did not sense any in the end. We discussed the role of human and non-human connections within and outside of cities. While inner cities objectively have less animal life, the scarcity of resources makes them live closer to humans. In her last installment, she recorded birds over the rooftops and on the port of Rotterdam, overlooking the city and its environment.</p>
<figure>
<img src="Exhibition(31).jpg" alt="Yuxi Liu" /><figcaption aria-hidden="true">Yuxi Liu</figcaption>
</figure>
<p>Davide Bevilacqua’s work <em>POND</em>, a 4-channel video installation also addresses the digitization of natural phenomena. The screens lie flat on the floor on a tarp resembling a small pond inside the exhibition space. The images of the water’s surface morph between each other, creating an unusual and oddly familiar ripple effect. It looks like the scattering of sunlight on top of a water surface; looking closer, one can see glitch artifacts we know when a video file is not loading correctly.</p>
<figure>
<img src="Exhibition(25).jpg" alt="Exhibition view of POND by Davide Bevilacqua" /><figcaption aria-hidden="true">Exhibition view of POND by Davide Bevilacqua</figcaption>
</figure>
<p>Giacomo Piazzi’s <em>The Monkey</em> is a monolithic work consisting of two rows of red glowing displays continuously changing the pattern of the lit screens. He uses the infinite monkey theorem, which proclaims that if a monkey were to hit a random letter on a keyboard for an endless amount of time, it would eventually write the complete works of Shakespeare or almost surely any other finite text. <em>The Monkey</em> in Giacomo’s work is a random number generator that turns a segment of the display on or off, possibly creating any text string. While a 7-segment display with 128 possible states is primarily used in digital clocks and meters because it can easily show numerical values, it is also possible to show most letters of the Latin alphabet. Because of their simplicity and due to their widespread use in electronic devices, Giacomo flips the monkey theorem on its head and becomes more interested in the human interpretation of the arbitrary values displayed on the screen.</p>
<figure>
<img src="Exhibition(12).jpg" alt="Giacomo Piazzi, The Monkeys |300" /><figcaption aria-hidden="true">Giacomo Piazzi, The Monkeys |300</figcaption>
</figure>
<p>Another work focusing on text and its representation in artificial neural networks is <em>Latent Space Divination</em><a href="#fn33" class="footnote-ref" id="fnref33" role="doc-noteref"><sup>33</sup></a> by Błażej Kotowski. In this piece, he created an interactive meditation for one person in a tent to reflect on the position of word embeddings and their spacial relation within a sentence. A visitor sits down in front of a tablet computer with headphones and types in their intention. The word embeddings of the sentence, in turn, impact how a 3-dimensional graph will be created. While the graph is slowly unfolding, a sonification is generated for the participant, and after around 5 minutes, the object rotates around itself to create a solid object which represents the initial intention.</p>
<figure>
<img src="Opening_10.11.21(6).jpg" alt="Visitor at the end of Latent Space Divination" /><figcaption aria-hidden="true">Visitor at the end of Latent Space Divination</figcaption>
</figure>
<p>Finally <em>1-800-MYGRAVE</em> by Erica Jewel and Dasha Ilina is an installation in the center of the room that imagines a call center where the visitor becomes a worker for a fictional company that provides digital afterlife service. It is based on a video they made with Lina Schwarzenberg during the camp. In the style of a 1980s public service announcement, the infomercial warns the viewer of their accumulated data. It offers simple steps to protect digital files and avoid embarrassment after death. The cubicle in the gallery is fitted with a corded telephone and an inspirational poster where the visitor switches roles and takes on (automated) calls from people who ask or complain about the service. An instruction manual tells the participant how to respond and which buttons to press.</p>
<figure>
<img src="Exhibition(21).jpg" alt="1-800-MYGRAVE installation view" /><figcaption aria-hidden="true">1-800-MYGRAVE installation view</figcaption>
</figure>
<p>The open call for the camp had the goal “to enable embodied networks through the development of new forms of computational intelligence.”<a href="#fn34" class="footnote-ref" id="fnref34" role="doc-noteref"><sup>34</sup></a> Looking back to the exhibition and the camp over a year later, I see many connections. While the works are separated by space and author, they often address the same thing from a different angle. For example, we explored anthropomorphous robots by embodying a virtual friend inside a plastic toy. Still, most closely, we looked into the processes of capturing the world through digital computers and how we relate to our data bodies. I’ve come to the conclusion that maybe real AI is the friends we made along the way.</p>
<figure>
<img src="IMAG0038+0039.jpg" alt="Photo by Hess Jeon taken before our first dinner at the Silicon Friend Camp" /><figcaption aria-hidden="true">Photo by Hess Jeon taken before our first dinner at the Silicon Friend Camp</figcaption>
</figure>
<h2 id="reflection">Reflection</h2>
<p>In this chapter I briefly touched on the idea of an individual and how humans—in contrast to computer systems—are situated in and grow through inter-relational connections. Machines are not designed to create meaning through relationships, we are. I picked two projects in which I, with a group of others, explored how computers are changing our social environment and how they fit in between our conversations. Ironically, we are using all kinds of technological augmentations to communicate with each other about the machines that we are using. The artworks and experiments that we created through our conversations are often critical of the technology they highlight and address another public through the artifacts and stories we tell.</p>
<p>Moving forward I will dive deeper into the technological aspects that enable machines to be perceived as agents. In the following chapter I will specifically focus on speech as it is one of the most fundamental forms of communication and explain how its recognition and synthesis were incorporated into the computer.</p>
<hr />
<h1 id="talking-to-computers">Talking to Computers</h1>
<h2 id="human-automata">Human Automata</h2>
<p>Stories of artificial assistants already existed in ancient myths. For example, the limping Hephaestus built himself servants made of gold who assisted him in his work, could speak, and even had a mind of their own.<a href="#fn35" class="footnote-ref" id="fnref35" role="doc-noteref"><sup>35</sup></a> But it is only in the past few centuries that we have created the technological means to seriously address the construction of mechanical servants (at least virtually, as robotics is still far behind). At the height of automaton design in the 18th century, Frenchman Jacques de Vaucanson invented a mechanical duck that could not quack but appeared to have a functioning digestive tract. The mechanical attraction toured European noble houses and let its audience feed grains to it. However, what the duck excreted was a prepared colored porridge that was in a hidden container. This principle of mechanical trickery was also used by Vacaucanson’s contemporary Wolfgang von Kempelen, who caused a sensation with his chess-playing automaton in the shape of a turban-wearing Turk. The illusion that the machine was acting autonomously was made possible by a small person inside a hidden compartment who controlled the puppet arm of the table via gears, levers, and pulleys. The hybrid machine is now the namesake of Amazon’s <em>Mechanical Turk</em>, the largest platform for digital micro-labor, which lists click jobs for pennies. Today, it continues to perpetuate the illusion of autonomous machines with “artificial intelligence” that is covertly enabled by an army of underpaid workers.</p>
<p>Even though the chess-playing Turk attracted attention, Wolfgang von Kempelen’s scientific interest was in imitating human speech. He wrote down his investigations into phonetics in the scripture <em>Mechanismus der menschlichen Sprache</em> (Mechanism of Human Speech) and built an apparatus with bellows, rubber hoses, and a wooden nose with which it was possible to produce basic phonemes.</p>
<p>Among those influenced by Kempelen’s book was a German tinkerer named Joseph Faber, who demonstrated his own mechanically constructed speaking machine in 1841. It attracted little interest in Germany and was presented and improved four years later in the United States as the Wonderful Talking Machine. This machine, as described by author David Lindsay, consisted of a bizarre-looking talking head<a href="#fn36" class="footnote-ref" id="fnref36" role="doc-noteref"><sup>36</sup></a> that spoke in a strange ghostly tone while Faber manipulated it with foot pedals and a keyboard<a href="#fn37" class="footnote-ref" id="fnref37" role="doc-noteref"><sup>37</sup></a>. For the inventor, the machine did not lead to the financial success he had hoped for, though it was presented as the <em>Euphonia</em> in London, where it at least delighted the father of telephone inventor Alexander Graham Bell and served as the boy’s inspiration for his first talking machine.</p>
<figure>
<img src="Euphonia-Photo_super-resolution.jpg" alt="Joseph Faber’s Talking Machine (Photo by Mathew Brady’s studio in American Museum of curiosities in New York City, circa 1860)" /><figcaption aria-hidden="true">Joseph Faber’s Talking Machine (Photo by Mathew Brady’s studio in American Museum of curiosities in New York City, circa 1860)</figcaption>
</figure>
<p>The Euphonia also inspired media artist Michael Markert to build <em>kII (Kempelen 2.0)</em>, an interactive installation where visitors can playfully control a speech synthesizer by moving, opening, and opening and closing their hands. In doing so, he brings Kempelen’s speech apparatus into the 21st century with the help of an 8-bit PIC microcontroller and sensor technology. Like Kempelen’s apparatus, it alienates the voice, creating mostly meaningless vocal sounds that enable new gesticulatory speech interactions.<a href="#fn38" class="footnote-ref" id="fnref38" role="doc-noteref"><sup>38</sup></a></p>
<p>The development of electricity certainly made new human interactions possible. For example, the invention of the telephone and the radio allowed the human voice to be transmitted over long distances. To optimize the transmission of speech, Bell Laboratories researched how to digitize the voice, for which they developed the vocoder (voice encoder). Demonstrated at the 1939 World’s Fair in New York, the <em>Voder</em> omitted the speech input and transformation of the vocoder and allowed electrical synthesis of the voice via a console with 15 keys and a foot pedal.<a href="#fn39" class="footnote-ref" id="fnref39" role="doc-noteref"><sup>39</sup></a> The keyboard was operated by specially trained women and advertised as the <em>robot speaker</em> in a recording, while it’s unclear if they meant the machine or the woman who is speaking through it.</p>
<p>Human computers were popular and necessary for war machines and research purposes in the 1930s and 40s. Mostly it was women who prepared mathematical tables, for example, for the use of ballistic projectiles. With the advent of the first digital calculators, female mathematicians, who were often denied higher scientific positions, were employed as programmers for the new universal electric machines.<a href="#fn40" class="footnote-ref" id="fnref40" role="doc-noteref"><sup>40</sup></a> The six people who programmed the first universal computer <em>ENIAC</em> include Betty Snyder Holberton, Jean Jennings Bartik, Kathleen McNulty Mauchly Antonelli, and Marlyn Wescoff Meltzer.<a href="#fn41" class="footnote-ref" id="fnref41" role="doc-noteref"><sup>41</sup></a> Initially, they programmed the (mechanical) computer with punched cards and cables for specific operations. It soon became clear that programming complex systems required an abstract semantic language. For that reason, the programming languages Fortran by John W. Backus, Lisp by John McCarthy, and COBOL by Grace Hopper were invented in the 1950s. The latter is strongly oriented to written English, intended for business applications; it was the first attempt to use natural language for computer programming.</p>
<p>The second half of the 20th century saw the emergence of the myths about computers that we are familiar with today: stories of anthropomorphic beings, like the board computer, HAL9000 in Space Odyssey, or Samantha in the movie Her. In both films, the disembodied voices become aware of their emotions and emancipate themselves from their human programming. Artist Tillmann Ohm makes this clear in his work <em>Reflections of HAL and Samantha</em>[^[^47]8] by having the two artificial beings engage in a dialogue, cutting their original voice-overs together. While Samantha is convinced that the overwhelming and sometimes hurtful process of her learning algorithm improves the complexity of her emotions, HAL is consequentially interpreting them as errors in human programming and analyzes the estimated malfunction.</p>
<h2 id="artificial-voices">Artificial Voices</h2>
<p>I want to explain the advancements in voice synthesis since Wolfgang von Kempelen’s speaking machine on actual dolls for children. The toy manufacturer Mattel released the doll Chatty Cathy in 1959, which was similarly popular to the company’s other best seller Barbie. Cathy’s trademark was a string, coming out of the back of its body, that could be pulled to wind up the mechanism of a simple phonograph. Like a record player, it plays short strips with sentences like “I love you” or “Tell me a story.” It was not the first toy using phonograph records, but its success led to many pull-string toys flooding the market well into the 60s and 70s. But obviously, before dolls and puppets were designed to talk back to us, children and adults talked to figurines and other inanimate objects for a long time. </p>
<p>The systems engineer and science writer George Zarkadakis traces back the modern human mind to a pre-historic figurine of a lion-man (Löwenmensch), carved out of ivory and found in the cave Hohenstein-Stadel in southern Germany.<a href="#fn42" class="footnote-ref" id="fnref42" role="doc-noteref"><sup>42</sup></a> The figure dates back 40.000 to 35.000 years and depicts a human body with an animal head. Because the figurine was found in a cave next to other carved objects, like beads and jewelry, researchers believe that the cave was either a storage or a place for shamanistic rituals. What fascinates Zarkadakis is that modern humans have existed 360.000 years before, but no evidence exists that they have been making art objects or figurative depictions before. Only during the upper paleolithic age, it is assumed that we created a general-purpose language when the first cave paintings were drawn, and figures like the lion-man were carved. The evolutionary reasoning is that ‘theory of mind,’ the seemingly unique human trade of projecting our own inner thoughts onto others to explain their behavior, gives the individual a social benefit, making it more likely that they reproduce. Projecting consciousness onto others does not stop inside of human groups, but placing our own reasoning onto animals also seems beneficial for hunters coming up with elaborate strategies to take down larger animals together. In animist belief systems, everything contains a ‘soul’ or ‘spirit’: animals, plants, rocks, rivers, the weather, and maybe even words. George Zarkadakis argues that symbolic language predates the modern mind and is not only used to communicate but ultimately dictates how the world is represented in our own consciousness. Only through language do we place ‘souls’ and ‘spirits’ into everything around us, and art objects fulfill the function of mirroring our minds physically, making connections with the world around us. It is no surprise then that the original artist of the lion-man created a hybrid of a human and a mountain lion, probably imagining themselves into the figure of the predator and using the object as a symbol to communicate ideas.</p>
<p>In the intro of “In Our Own Image,” George Zarkadakis tells the story of how the fictional character Robby the Robot from the movie <em>Forbidden Planet</em> impressed him as a child so much that he would imagine the robot as a playmate that could walk, talk and obey orders. When he later decided to study engineering and exchanged thoughts on artificial intelligence with other researchers, they all seemed to relate to the same sci-fi movies, books, and stories. The hint here is that these fictional stories influence what is getting built and how we think about the future.</p>
<p>Inspired by this revelation, I asked participants at the <em>Silicon Friend Camp</em> about their first contact with a robot. My memories brought me back to a picture of myself proudly holding a plastic toy robot in my hand. The doll—or ‘action figure’—had a battery compartment, the power source for tiny wheels underneath its feet, and light bulbs for its eyes. It could not talk, but I remember it made noises resembling guns or laser sounds. In the 80s and 90s, the bleak vision of boxy humanoid robots carrying guns was very prominent. Movies like <em>Terminator</em> and <em>RoboCop</em> fueled my young male fantasy for power and destruction.</p>
<figure>
<img src="IMG-20220728-WA0000-removebg.png" alt="Photo cutout of me proudly presenting my toy robot." /><figcaption aria-hidden="true">Photo cutout of me proudly presenting my toy robot.</figcaption>
</figure>
<p>The other first encounters were less militaristic. One artist remembered a robot dog that he got as a present. In the early 2000s, digital pets with integrated sensors became cheap enough for consumers. <em>Tekno the Robotic Puppy</em><a href="#fn43" class="footnote-ref" id="fnref43" role="doc-noteref"><sup>43</sup></a> by Manley Toy Quest was one of the more affordable robodogs in the market. With light and proximity sensors, buttons, and even a microphone, it was well equipped to learn some ‘tricks’ by triggering the right combination of sensors. Another artist was sharing an image of the <em>Tamagotchi</em> and while it was not a robot, the LCD screen in an egg-like plastic shell displayed a simulation of a pet. With three buttons, children could interact with the virtual pet and fulfill its food, affection, and sleep needs. The anthropologist Pat Shipman proposes that animal connection can be considered a human trait that has uniquely evolved in us, next to tool making, symbolic language, and the domestication of animals and plants.<a href="#fn44" class="footnote-ref" id="fnref44" role="doc-noteref"><sup>44</sup></a> Simulating animal connection by creating simple programs on microchips was the next logical step in our drive to develop symbols all the way down and fulfill the need of children to take care of others. </p>
<p>Yet another popular toy shared by an artist in our group that filled the same robotic pets niche was <em>Furby</em>. This ambiguous furry creature with big eyes was made to simulate language acquisition and brings us back to how dolls learned to speak. In the case of <em>Furby</em> the designers decided to create a new vocal language called ‘furbish.’ When the device was first started, it only repeated gibberish sentences and slowly, over time, would exchange these words with the local language equivalents. The original source code for one of the microchips was released in 2018<a href="#fn45" class="footnote-ref" id="fnref45" role="doc-noteref"><sup>45</sup></a>, and even though I can not read assembly code, it is thoroughly documented through comments. The game logic of the furby.asm file is relatively simple, jumping between subroutines depending on the state of the many sensors, triggers, and timers and occasionally using a pseudorandom number generator to mix things up. The speech sounds were accessed from a memory unit and sent to a cheap Texas Instruments speech synthesizer chip. These chips have been in use for decades already, prominently in another toy called the <em>Speak and Spell</em>, but also in watches, clocks, and translators. The synthesis uses a prerecorded audio signal that is then compressed using linear predictive coding (LPC). This type of encoding allowed the voice to be later changed in frequency, pitch, and loudness. But just like the <em>Vocoder</em>, it only digitized a voice and could not construct new words or sentences. So the original <em>Furby</em> from 1998 could only express a couple hundred words recorded by a voice actor, which is again only a digital equivalent to Chatty Cathy.</p>
<p>More powerful computers could generate any text into somewhat intelligible speech. The <em>Software Automatic Mouth (S.A.M.)</em> from 1982 was one of the first commercial products that used rule-based formant synthesis. Instead of using whole prerecorded words, it uses an array of computer-generated phonemes and places them together to make up sentences. While this was often understandable, it was not perfect. It sounded unnatural and robotic and relied on converting text to speech probabilistically, often mispronouncing words.<a href="#fn46" class="footnote-ref" id="fnref46" role="doc-noteref"><sup>46</sup></a> <em>S.A.M.</em> did not need any special hardware and could run on Apple, Atari, and Commodore computers at the time, even though that would often mean using every CPU cycle and holding other programs. The software had its most significant moment at the launch event of the Macintosh computer in 1984, greeting the audience with a live demo in first person.<a href="#fn47" class="footnote-ref" id="fnref47" role="doc-noteref"><sup>47</sup></a> Sam and its multiple software variations became the universal voice of the computer.</p>
<p>The net.art pioneer Alexei Shulgin took the speech synthesis capabilities similar to <em>S.A.M.</em> to the extreme with his cyberpunk rock band <em>386 DX</em>.<a href="#fn48" class="footnote-ref" id="fnref48" role="doc-noteref"><sup>48</sup></a> The band is named after the Intel chip inside the old office computer, which is also the front singer and instrumentalist. It uses MIDI and voice synthesis to cover popular songs by The Doors, Nirvana, or the Sex Pistols, traveling to festivals worldwide and sometimes performing on the streets. The artist Alexei Shulgin takes the role of the operator, only pressing the play button and creating the visuals on the screen during the live performances. This ironic take of making the microchip into its own one-computer-band demonstrated how terrible and reductive rock music becomes when compressed into bits and bytes. At the same time, the novelty and humor of this process generated enough interest for <em>386 DX</em> to produce 2 CDs and distribute them over music labels.</p>
<p>The strangely robotic voices associated with computers improved over time, and when Apple’s digital assistant Siri came out in 2011, they used a different process of concatenating audio samples. Before Apple even started working on its voice assistant, a company called Scansoft auditioned hundreds of voice-over artists in 2005 and made them speak a multitude of random sentences. The idea of using the random utterances for new outputs was quite simple: cut the voice into smaller pieces of syllables, demi syllables, phonemes, diaphones, or triphones, and put them back together for any word you need. Doing this manually was very time-consuming, so programs were developed to analyze and categorize the speech datasets based on their acoustic properties. Then, at runtime, an algorithm selects the best sequence to create the desired phrase. The results in the early 2000s still contained a lot of error and needed substantial tweaking, but the outcome was perceived as much more natural. </p>
<p>In 2006 Scansoft merged with Nuance, another company working on enterprise speech solutions, which presumably used the database to create the voice for Siri and licensed it to Apple. The original voice actor in the US was Susan Bennet<a href="#fn49" class="footnote-ref" id="fnref49" role="doc-noteref"><sup>49</sup></a>, who was oblivious to the fact that suddenly, six years after recording, her voice appeared on iPhones all over the country. </p>
<p>The technology to generate new sentences from previously recorded voices drastically improved when deep weighted networks started to be used to analyze and select audio samples. One significant achievement in generating natural voices came in 2016 in the form of a model named <em>WaveNet</em><a href="#fn50" class="footnote-ref" id="fnref50" role="doc-noteref"><sup>50</sup></a> from a team of researchers at Google’s DeepMind. They used convolutional node networks (CNN) to perform statistical analysis on raw audio files with 16000 samples per second. Using convolutional operations on temporal audio data meant that the researcher did not need to convert it into a compressed format using a vocoder. The results were much more natural than any previous text-to-speech algorithm. Using a raw audio dataset also means that the system can find patterns in any audio signal, including music. For demonstration, the researchers generated new samples from a classical piano dataset, which sounds fascinating, but has more resemblance to free jazz than a classical piano score. This erratic remixing also happens to speech. When the network is conditioned without any text sequences, it generates impressive human babbling where actual words are interspersed with made-up word-like sounds and breathing noises. When the network is trained on audio and the transcription together, the system does not only generate a consistent voice replica of a person but can also be used for voice recognition tasks transforming utterances back to text.</p>
<p>Other companies experimented with weighted networks to better create concatenative voice models. In the same year, in 2016, Adobe showcased a hypothetical feature for their audio software, where users can create voice clones with a small 20-minute dataset and then use text to edit spoken words in a sentence. In the presentation of project <em>VoCo</em> at the Adobe MAX conference, the developer Zeyu Jin used an example sentence where he exchanged the words “I kissed my dogs and my wife” with “I kissed Jordan three times” – using the voice of the host of the conference. This strange demonstration, which was supposed to be funny, got a lot of attention online and created a debate around the ethical implications of manipulated audio and voice cloning. Adobe never released <em>VoCo</em> as a product, maybe because of the media backlash and fear from the public. However, other companies like <em>Lyrebird</em><a href="#fn51" class="footnote-ref" id="fnref51" role="doc-noteref"><sup>51</sup></a> filled the gap, trying to market their product for editing podcasts and voice-overs. They started a media campaign by cloning the voices of Donald Trump and Barack Obama to discuss the startup’s product and praise it for their democratizing effort of making it available to anyone.<a href="#fn52" class="footnote-ref" id="fnref52" role="doc-noteref"><sup>52</sup></a> Together with synthetic video, so-called “Deep Fakes” have earned a bad reputation for being used in fraud<a href="#fn53" class="footnote-ref" id="fnref53" role="doc-noteref"><sup>53</sup></a> and pornography<a href="#fn54" class="footnote-ref" id="fnref54" role="doc-noteref"><sup>54</sup></a>. At the same time, artists are using the technique to create hypothetical stories to mock politicians and other famous figures.<a href="#fn55" class="footnote-ref" id="fnref55" role="doc-noteref"><sup>55</sup></a> To counter fraudulent use, Zeyu Jin, the creator of <em>VoCo</em>, proposed watermarking synthetic voices and using software to detect if a voice is manipulated.</p>
<p>With the release of <em>WaveNet</em> and <em>VoCo</em> in 2016, a new field in voice synthesis opened up. Hybrid systems were created that used concatenative and generative techniques to categorize and interpolate between tiny audio samples of human speech.<a href="#fn56" class="footnote-ref" id="fnref56" role="doc-noteref"><sup>56</sup></a> The first release of <em>WaveNet</em> was computationally expensive, but already one year later, Google implemented a new version that works in real-time for their voice assistant and has shown that a slim hybrid model can be used on low-powered devices, such as smartphones<a href="#fn57" class="footnote-ref" id="fnref57" role="doc-noteref"><sup>57</sup></a> and other voice user interfaces are moving towards the use of deep weighted networks.<a href="#fn58" class="footnote-ref" id="fnref58" role="doc-noteref"><sup>58</sup></a></p>
<h2 id="conversational-agents">Conversational Agents</h2>
<p>For Amazon founder Jeff Bezos, the board computer in Star Trek was the inspiration for investing in the cloud-based voice software Alexa.<a href="#fn59" class="footnote-ref" id="fnref59" role="doc-noteref"><sup>59</sup></a> The product was initially marketed as a networked speaker. The software is now expanding to other items, including watches, smartphones, jewelry, light bulbs, and doorbells. The aggressive price war with Amazon-connected products is partly to capture the connected home market and collect as much natural voice data as possible. Over the past ten years, deep weighted networks<a href="#fn60" class="footnote-ref" id="fnref60" role="doc-noteref"><sup>60</sup></a> have become popular for classification and pattern recognition tasks, especially impacting automated speech recognition. Before, the user would have to feed the system with voice samples to reliably turn utterances into text. With a landmark paper in 2012 by research groups at the University of Toronto, Microsoft, Google, and IBM, it became that the application of deep weighted networks outperforms any previous technique.<a href="#fn61" class="footnote-ref" id="fnref61" role="doc-noteref"><sup>61</sup></a> In the same year, Android released its new cloud-based speech recognition service<a href="#fn62" class="footnote-ref" id="fnref62" role="doc-noteref"><sup>62</sup></a>, and since then, users’ speech can be transcribed accurately. For a long time, these models needed gigabytes of data in computer memory, to create accurate results, so users needed to send their voice samples to large server farms for transcription. Offline recognition became feasible only in the past 2-3 years through specialized hardware and model optimization, trading performance for robustness. Google’s claim that new speech recognition systems have an error rate of less than 5% is only true for a specific—mostly white and male—audience speaking American English and drops abruptly with a more diverse distribution of dialects, age, and gender.<a href="#fn63" class="footnote-ref" id="fnref63" role="doc-noteref"><sup>63</sup></a></p>
<p>Deep automatic speech recognition systems need a massive amount of input data to perform well. A paper from the Chinese tech giant Baidu proposes 12000 hours of speech to create a robust model.<a href="#fn64" class="footnote-ref" id="fnref64" role="doc-noteref"><sup>64</sup></a> Only a global internal speech database enables the company to improve its speech recognition system. The intrusion into privacy is immense and has already been used by U.S. law enforcement agencies as evidence in a court case.<a href="#fn65" class="footnote-ref" id="fnref65" role="doc-noteref"><sup>65</sup></a> But there is another, more ethical, way than stealing people’s voices: The Mozilla Foundation’s <em>Commonvoice</em> project relies on people voluntarily recording their voices for computer models, and the resulting speech recognition and synthesis software can be offered with an open source license.<a href="#fn66" class="footnote-ref" id="fnref66" role="doc-noteref"><sup>66</sup></a> Common speech and text datasets already have been publicly available from projects like VoxForge that use the same crowdsourced techniques as Mozilla or LibriSpeech, which essentially scraped and aligned public domain audiobooks.<a href="#fn67" class="footnote-ref" id="fnref67" role="doc-noteref"><sup>67</sup></a> Other common non-free resources come from the Linguistic Data Consortium, which created <em>Switchboard</em>, a corpus of conversations between two US citizens randomly connected and recorded over the telephone line. Listening to the audio sample provided on the website creates oddly voyeuristic feelings. We can hear a mundane conversation of a man talking to a woman about his interest in gardening.<a href="#fn68" class="footnote-ref" id="fnref68" role="doc-noteref"><sup>68</sup></a>  </p>
<p>Artist Lauren Lee McCarthy plays with these tensions between intimacy and privacy, convenience and agency. In her projects <em>LAUREN</em> and <em>SOMEONE</em>, she installs connected devices into volunteers’ homes and either acts as a control system herself or lets others remotely monitor the volunteers and control the devices in their homes, which creates an interesting tension when the person knows that there is an actual human listening and watching from afar. At the same time, the artist and performers find themselves in a position of a helpful voyeur.</p>
<p>However, the role of human labor behind voice assistants is about more than just executing and understanding commands. People interpret and categorize voices according to age, gender, and social status. Companies take advantage of this and design their voice software according to certain identity schemes, which are provided with a history, hobbies, and preferences. It is precisely this illusion that excites users and makes the product attractive. In a UNESCO think piece titled <em>I’d blush if I could</em><a href="#fn69" class="footnote-ref" id="fnref69" role="doc-noteref"><sup>69</sup></a>, they explore harmful gender biases associated with digital assistants. The voice assistants of major tech companies are scripted by default as female personas with smart, humble, and sometimes funny personalities. The teams working on voice assistants try to avoid this aspect because Apple, Microsoft, and Google ask their employees to refer to their headless voices as “it” and when the persona gets asked the question, it is scripted to reply ambiguously or with a joke. Only Alexa answers with the direct phrase, “I’m female in character.” By being submissive, they thus support a patriarchal image of female stereotypes, similar to the historical figures of human computers and other secretarial roles. The paper calls for women to be more empowered and involved in IT. It calls for AI software to avoid gender attributes whenever possible and for AI assistants to take a clear stand against sexist behaviors.</p>
<p>Researcher and artist Nadine Lessio creates useless voice assistants to critique the current corporate agenda of productivity, efficiency, and consumption. She does this by using the programming interfaces provided to make apps for voice assistants. For example, she explores the concept of a depressed home assistant with <em>SAD Home (Depressed Alexa 1.0)</em><a href="#fn70" class="footnote-ref" id="fnref70" role="doc-noteref"><sup>70</sup></a>, an Alexa hack that grants users their wishes depending on the weather and other mood factors; sometimes it simply turns itself off.</p>
<p>This scripted denial of capitalist logic ironically uses the same technique as the company behind it. Voice assistants are carefully crafted by a team of creative professionals working in the field of “conversation design.” In the book <em>Talk to Me</em><a href="#fn71" class="footnote-ref" id="fnref71" role="doc-noteref"><sup>71</sup></a>, the author, James Vlahos, describes that many people in the field are far away from computer science and more commonly have careers in the liberal arts. The teams are made up of authors, playwrights, comedians, actors, anthropologists, psychologists, and philosophers who imagine the personality of the AI persona that should represent the brand. To create the character, they have to come up with all possible questions and various answers. Vahos recalls asking Microsoft’s Cortana, “Where do you come from?” and the female voice replies, “I was made by minds across the planet.”<a href="#fn72" class="footnote-ref" id="fnref72" role="doc-noteref"><sup>72</sup></a> And even though the designers decided to use the first person “I,” they really are talking of themselves carefully crafting the answers played back by the loudspeakers around the world.</p>
<p>It is interesting how the term AI is used in the context of voice computer interfaces because there is nothing “smart” about it, just a winding flowchart of if-else conditions. What might be clever is the nefarious way how companies trick people into the belief in computer personalities, encouraging people to interact with the device like children playing with dolls. And when a company like Microsoft experimented with a more sophisticated chatbot, like Tay<a href="#fn73" class="footnote-ref" id="fnref73" role="doc-noteref"><sup>73</sup></a>, it started to repeat the racist and misogynistic slurs of Twitter users and consequently had to be heavily filtered and finally taken down.</p>
<h2 id="leewa">Leewa</h2>
<figure>
<img src="210724-SFC-_0010781.jpg" alt="Videoprojection of Leewa during the Silicon Friend Camp" /><figcaption aria-hidden="true">Videoprojection of Leewa during the Silicon Friend Camp</figcaption>
</figure>
<p>The artist duo <em>Ekheo</em> consisting of Aude Langlois and Belinda Sykora, explore the crosslines of musical, visual, and sonic art performances. They met at the Sound Studies program at the Universität der Künste in Berlin, where they created musical improvisations and theoretical frameworks on voice and music. They started their research on voice, gender, and identity in relation to AI systems in 2019 and held talks at the MUK<a href="#fn74" class="footnote-ref" id="fnref74" role="doc-noteref"><sup>74</sup></a> and other institutions to share their insights. </p>
<p>At the <em>Silicon Friend Camp</em> they wanted to extend their existing improvisation and research practice by integrating embodied AI. But the main focus was the voice itself: How is gender perceived through it? How do we navigate the tension between what is considered an artificial voice and an authentic voice? </p>
<p>To tackle these issues during the camp, they wanted to create and record soundscapes based on vocal improvisation and use AI systems in the form of synthetic voices, generative composition tools, robotic elements, and so on. </p>
<p>So we prepared the house’s attic with a small sound studio they could work in. During the week, they found inspiration in using multiple AI systems and human collaborators to create an improvisation and finally a 6-minute video and audio composition.</p>
<p>To come up with the identity of their AI partner, they used the <em>Oracle</em><a href="#fn75" class="footnote-ref" id="fnref75" role="doc-noteref"><sup>75</sup></a> to ask questions about their name, gender, favorite instrument, etc. which reads like this:</p>
<pre><code>You: how old is she?
Oracle: Today she will be 11.
You: what is her favourite musical instrument?
Oracle: her greatest pleasure is to whisper to the sounds of trumpets and the beating of tympani
You: is she happy?
Oracle: It is a mystery and a great sadness.
You: What is Leewa&#39;s surname?
Oracle: It was suggested that her human&#39;s name was /u/Seattle_Celtics.
You: does she like to sing?
Oracle: I have no voice
You: No I mean Leewa
Oracle: Query : Have you ever cheated?
You: Does Leewa have a voice?
Oracle: Yoow!
You: what is Leewa&#39;s favourite poem?
Oracle: Leaves of Grass
You: What is Leewa&#39;s favourite book?
Oracle: Leewa does joke around and loves to give sarcastic responses. She also loves to read books, especially horror novels.
You: What gender does Leewa have?
Oracle: She is female.
You: which music does Leewa like?
Oracle: Leewa likes to sing and most of all Leewa likes to dance.
You: What is art for her?
Oracle: art is the process of iterating through consciousness generation.</code></pre>
<p>This chat served as a base for the identity of <em>Leewa</em> and inspired further experiments using a whispering voice and 3D printing a female doll bust. To recreate the voice, I searched for tools to do voice synthesis and found an open source toolbox from Corentin Jemine that claimed to do Real-Time Voice Cloning from a few seconds of audio.<a href="#fn76" class="footnote-ref" id="fnref76" role="doc-noteref"><sup>76</sup></a> The method is essentially based on a previous paper from Google engineers: <em>Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis</em><a href="#fn77" class="footnote-ref" id="fnref77" role="doc-noteref"><sup>77</sup></a> (SV2TTS), which itself uses a pipeline of a speaker-encoder network (GE2E) to represent different voices in different positions of the latent space, a text to waveform synthesizer (Tacotron) and a vocoder to generate sound from these waveforms (WaveNet).<a href="#fn78" class="footnote-ref" id="fnref78" role="doc-noteref"><sup>78</sup></a> As described previously, combining different encoding and generator systems into a larger framework is commonly used and is commercially used in Google’s Assistant and their cloud services for other companies. </p>
<p>Thanks to the efforts of Corentin Jemine and other contributors, who built a user interface to explore these tools, we could input our own voices and generate new samples with pre-conditioned voice models. Unfortunately, though, the results were not as expected; by inputting only a small number of samples, the output was not picking up on our voice characteristics, and the results were too noisy and low-quality. Instead, we tried the cloud service resembleAI<a href="#fn79" class="footnote-ref" id="fnref79" role="doc-noteref"><sup>79</sup></a> mentioned in the code repository. Instead of the intended use of cloning one individual voice, we decided to create a collective voice for the camp participants. Every one of us recorded ten phrases used as input to the model. </p>
<p>After I pressed “train” on the web interface and waited for some time for the process to finish, I got an e-mail telling me that the “Resemble score” of our voice was 53%. So the transfer learning process was not entirely successful, but the resulting voice was very surprising. I expected the algorithm to average our voices into something that fits in between all data points. Instead, it created clones of a voice for some phonemes but not for others so that a sentence randomly interpolates between different speakers. It also seems that, at least in this run, the characteristics of male voices are more prominent than female voices, but in general, it has a strange new artificial quality to it, where the pitch and speed of the voice is constantly changing.</p>
<p>Because we used a commercial product, we could not access the local model and only interface with it through their web interface, making using the voice for real-time performances impossible. Instead <em>Ekheo</em> created some voice samples that they played live during the final performance. They also recorded real voices from participants at the camp and intertwined them with synthetic drones and other instruments. The visuals were created from a live video feed of the doll, which was modified by Naoto Hieda using the live coding shader language Hydra.<a href="#fn80" class="footnote-ref" id="fnref80" role="doc-noteref"><sup>80</sup></a> The puppet could also move its head, which So Kanno remotely controlled via a servomotor. </p>
<p>In my opinion, the example of <em>Leewa</em> illustrates the collective efforts that go into the building “an AI” quite nicely: it shows the process of using open source and proprietary code developed by people reliant on previous research, the creation of collective datasets through the recording of our voices and finally the human performers hiding in the background and remotely manipulating symbols for the audience. At the same time, <em>Leewa</em> is a self-referential project that makes the collective experience throughout the camp audible by using our synthetic and real voices, the interaction with the <em>Oracle</em> and snippets of anecdotes from the camp.</p>
<h2 id="reflection">Reflection</h2>
<p>This chapter started from an ancient story which told of mechanical servants to help their master Hephaestus. A myth of the dream to breathe life into objects, explained by a trade that the human species has evolved to empathize as a default behavior. I looked at the journey it took us to imitate and understand the vocal patterns we have created to communicate. Starting from Wolfgang von Kempelen’s and Joseph Faber’s attempts to build mechanical devices capable of creating basic phonemes, which eventually led to the successful synthesis of speech using electricity. Initially, these artificial voices were incorporated into toys that could react to children’s fantasies. However, as technology advanced and computer programs became more sophisticated, artificial voices became more ubiquitous and companies like Amazon pushed this type of interaction forward. Voice user interaction is emotionally charged and design teams see the need to craft (mostly female) personalities that guide the user through a conversation graph. Lastly, I highlighted the production of our own communal voice model and how we created a doll body that acts as performer; albeit being remotely controlled.</p>
<p>In the upcoming chapters, I will delve into the significance of weighted networks in enabling machines to compute the world. To begin with, I will explore the fundamental components of AI systems which involve vast quantities of data typically sourced from the internet. The creation of extensive training sets will be examined, both in an academic and artistic context, with a particular focus on ethical considerations such as privacy, consent, representation, and ownership.</p>
<hr />
<h1 id="piles-of-data">Piles of Data</h1>
<h2 id="self-learning-networks">Self-Learning Networks</h2>
<p>The idea behind Artificial Neural Networks has a long-standing history. Using our understanding of the brain as a blueprint for mathematical operations dates back to the 1950s when the psychologist Frank Rosenblatt developed the <em>perceptron</em>.<a href="#fn81" class="footnote-ref" id="fnref81" role="doc-noteref"><sup>81</sup></a> Inspired by nerve cells and their connections (synapses), the perceptron takes multiple input values, sums them up, and outputs a 0 or 1 depending if a predefined threshold is reached. This system can be ‘trained’ by using positive and negative reinforcement to define the weights for each connection. With an apparatus, the Mark-I Perceptron,<a href="#fn82" class="footnote-ref" id="fnref82" role="doc-noteref"><sup>82</sup></a> that uses photoreceptors to turn light into ‘bits’ (today we would say pixels), the perceptron could ‘sense’ shapes in the form of a binary matrix and distinguish between circles, squares, and triangles. He proposed that a network of perceptrons could possibly even recognize faces and objects. Even though he developed the perceptron in 1964, Frank Rosenblatt never saw his invention take off.<a href="#fn83" class="footnote-ref" id="fnref83" role="doc-noteref"><sup>83</sup></a> </p>
<p>Another engineer, Kunihiko Fukushima, kept refining his methods in the 70s by adding multiple layers, effectively creating the first ‘deep neural network’ where deep just means the depth of ‘hidden’ or in-between layers connecting the input signal to the output classifier.<a href="#fn84" class="footnote-ref" id="fnref84" role="doc-noteref"><sup>84</sup></a> He called this self-organizing system Cognitron,<a href="#fn85" class="footnote-ref" id="fnref85" role="doc-noteref"><sup>85</sup></a> which successfully recognized numbers and letters on a 12x12 grid. Its successor, the Neocognitron,<a href="#fn86" class="footnote-ref" id="fnref86" role="doc-noteref"><sup>86</sup></a> took further inspiration from the visual cortex and discoveries by Hubel &amp; Wiesel made in the 1950s that some biological neurons selectively respond to local features like lines, edges, or color and others to figures like circles, squares or even human faces. This is also the core idea behind convolutional neural networks (ConvNet or CNN), which separate images into a smaller grid and apply a certain filter to them, e.g., checking for edges. The French computer scientist Yann LeCun came up with ConvNets in the 1980s, which are <em>the</em> driving force for AI systems today. Additionally, Geoff Hinton, a cognitive psychologist and computer scientist, popularized the backpropagation algorithm in 1986, which finally made it possible for filters to tune themselves instead of relying on predefined rules.</p>
<p>Most conceptual ideas behind current deep, weighted networks were already present in Frank Rosenblatt’s papers,<a href="#fn87" class="footnote-ref" id="fnref87" role="doc-noteref"><sup>87</sup></a> but rule-based systems often outperformed weighted networks. So what changed when Alex Krizhevsky, a student of Hinton, made a phenomenal leap in 2012 on the ImageNet classification competition?</p>
<p>Krizhevsky outlines the main innovation in the paper: “To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation.”<a href="#fn88" class="footnote-ref" id="fnref88" role="doc-noteref"><sup>88</sup></a> Until AlexNet was released, it was incredibly time-consuming to run the conditioning process on the CPU, which could only do one operation of matrix multiplication at a time. As the name Graphics Processing Unit suggests, the GPU was originally designed to calculate 3D scenes and render them on display, which involves a lot of matrix and vector operations. To accelerate them, GPUs are capable of calculating large blocks of data in parallel. For AlexNet, conditioning on 1.2 million pictures took only 6-9 days on two consumer graphics cards compared to probably weeks or months without them. However, it was not the first system that utilized the GPU; it is similar to a CNN by Dan C. Cireȿan et al. released a year prior, with a reported speedup of 60 times for large weighted networks.<a href="#fn89" class="footnote-ref" id="fnref89" role="doc-noteref"><sup>89</sup></a></p>
<p>The other roadblock for deep, weighted networks is ‘overfitting’: in the case of the ImageNet competition, that would mean that the model adapts to the image so closely that it would simply reproduce the categories of the image without being able to identify new pictures which were not inside the training set. The most common way to reduce overfitting is to have a sufficiently large dataset with a high amount of variance. For AlexNet, the 1000 classes and 1.2 million images were still not enough, and they used data augmentation, which transforms, flips, or changes the color of an image to increase the training set by a factor of 2048. A larger dataset—in theory—increases the robustness of the weighted network.</p>
<p>In conclusion, the conceptual framework of how weighted networks (artificial neural nets, perceptrons, or cognitrons) work was mostly established in the past century. Their performance today comes from increased computing power and more extensive training sets. In fact, datasets have become larger and larger, and we haven’t seen the limits of what weighted networks are capable of. They seem to be mainly limited by data and computation.</p>
<h2 id="making-data">Making Data</h2>
<p>My first encounter with a large dataset in the field of computer vision was in late 2017 when I found <em>11k Hands</em>.<a href="#fn90" class="footnote-ref" id="fnref90" role="doc-noteref"><sup>90</sup></a> As the name suggests, it is a dataset of 11076 human hands compiled by the researcher Mahmoud Afifi for gender recognition and biometric identification. The images show the hands of 190 individuals, in both their palm and dorsal orientation, placed on a white background with uniform lighting. Each image is accompanied by the following metadata: age, binary gender, skin color on a scale of “very fair,” “fair,” “medium,” and “dark,” left or right hand, palm or dorsal, if it has nail polish and if there are “irregularities.” A statistical analysis of the dataset shows that it contains more female than male hands, mainly people in their 20s and a majority of “medium” and “fair” skin tones. The gender bias is addressed in the paper and mitigated by filtering the training set to equal male and female hands. They report that the CNN conditioned on this dataset had, on average, 87% accuracy in recognizing the correct gender on an image of the palm and 90% accuracy for the dorsal side.</p>
<p>But it is not the ‘state-of-the-art’ results that drew me to the paper; it was the gender and skin tone classification itself that appalled me. It reminds me of phrenology in the 19th and 20th centuries, a popular pseudoscience claiming that a skull’s shape could determine a person’s character and mental abilities. Phrenologists like Franz Joseph Gall went to great lengths, measuring and categorizing human skulls and associating certain regions to human traits. While it didn’t take scientists to debunk the idea that bumps on a head could indicate a person’s characteristics, it came back in the early 20th century when it was applied to racist and sexist stereotypes and used to justify Nazi eugenics. The underlying assumption that the shape of the head has anything to do with the person’s mental state was simply wrong.<a href="#fn91" class="footnote-ref" id="fnref91" role="doc-noteref"><sup>91</sup></a></p>
<p>The paper by Afifi et al. does not make such horrendous claims about the personality of their subjects, but considering that gender is a social construct and not necessarily a binary choice trying to use a computer to identify if a person is male or female by analyzing their hand seems arbitrary at best and reinforcing gender stereotypes at least.<a href="#fn92" class="footnote-ref" id="fnref92" role="doc-noteref"><sup>92</sup></a> </p>
<p>Another aspect of the dataset stood out to me when I started to look through the images. These are not pictures in the traditional sense. Their aesthetic value does not matter because they are simply tools to accomplish a task. They are not made ‘to look at’; instead, these images of hands are produced for a computer to analyze. Harun Faroki called these types of images ‘operative’ in his three-part series <em>Machine/Eye</em> where he examined how military technologies like guided weapons produce images that serve only the utility of the machine. As Aud Sissel Hoel puts it: “operative images are devoid of social intent, that they are not meant for edification, and nor for contemplation.”<a href="#fn93" class="footnote-ref" id="fnref93" role="doc-noteref"><sup>93</sup></a></p>
<p>Upon contemplating the hand dataset, I was struck by its remarkable features. When I looked through the images quickly, I could not only see the motion of the test subject as the photos were selected frames from a video, but I also started to imagine the person behind the hand and apply my subjective stereotypes and biases which far expanded the labels of the dataset. I could imagine the scene around the camera with a couple of researchers who assembled a make-shift photo studio in the office of their lab. They greet the subject with a handshake, maybe a fellow student, and explain the procedure quickly to get them to sign the paper that their anonymized personal data will be published for scientific research. Then the person puts both hands under the camera, spreads their fingers, and leaves. </p>
<p>This dichotomy between the label and my narrative inspired me to create a video piece featuring the unaltered dataset. I used the default Mac computer speech synthesis to read the labels corresponding to each hand and sped it up to fit into a 26-minute-long video. As viewers witness the participants holding their hands into the camera and spreading their fingers, they are met with a monotone, beat-like soundtrack of repeating words like “fair” and “medium” and occasionally “nail polish.”<a href="#fn94" class="footnote-ref" id="fnref94" role="doc-noteref"><sup>94</sup></a></p>
<figure>
<img src="11khands.png" alt="Example of a frame from 11k hands" /><figcaption aria-hidden="true">Example of a frame from 11k hands</figcaption>
</figure>
<p>At the 36th Chaos Communication Congress, I organized a workshop called “Palm Reading AI,” inviting visitors to read people’s palms from the 11k hands dataset. At first, I only introduced them to palmistry using wikiHow as a reference.<a href="#fn95" class="footnote-ref" id="fnref95" role="doc-noteref"><sup>95</sup></a> Then I handed out some prints where a random hand from the dataset was depicted, and participants had to answer a couple of questions. Some questions were short guesses like age, gender, and country of origin; for others, they had to come up with fictional stories with only the hand lines as a reference: what is the persons future? How was their childhood? How is their love life?</p>
<p>After they filled in the form, some people shared their stories, and then I revealed where these hands came from and how computer scientists are using them to create models that try to predict their gender. Afterward, we discussed the practice of creating large datasets and their ethical considerations. I had a long talk with one participant that did not want to guess the age or gender of that person, and I told them that this was precisely the point of the workshop: to reflect on our own biases and stereotypes and how they translate into science.</p>
<p>After long contemplation on 11k hands and finding datasets that are much more problematic than this one, I don’t think the type of work from Afifi et al. is ‘unethical’ or needs to be redacted. They got consent from their subjects and shared the dataset with the scientific community for “reasonable academic fair use.” The work on biometric identification and comparing CNNs to previous methods are interesting and novel; however, as I stated above, I think the premise behind the gender recognition task is flawed. Unfortunately, this is very common in computer vision, where people are (mis-)labeled, reflecting and amplifying societal stereotypes.<a href="#fn96" class="footnote-ref" id="fnref96" role="doc-noteref"><sup>96</sup></a></p>
<p>My research on the hands dataset, in conjunction with esoteric practices and fortune-telling, informed a later work of mine, “The Chiromancer,” that I built with Giacomo Piazzi.</p>
<h2 id="in-the-wild">In the wild</h2>
<p>The process of collecting and creating data has drastically changed since the wide adoption of the internet. Where the 11k hands dataset has invited participants to their institute to take a picture specifically for the dataset, other researchers started to search and download huge collections from the internet without consent.</p>
<p>Take, for example, the ImageNet dataset, initiated by Stanford University’s AI professor Fei Fei Li, which was created to tackle object recognition tasks and eventually consisted of 14 million images.<a href="#fn97" class="footnote-ref" id="fnref97" role="doc-noteref"><sup>97</sup></a> The team queried multiple search engines with common nouns and multiple translations to get around 500-1000 images per category. The categories derive from an older project called WordNet, created in 1985 at Princeton University, that tried to achieve a hierarchical ontology of words. For example, the noun <em>human</em> is a synonym for <em>homo, man, and human being</em> which are subcategories of the <em>hominid</em> class, which are <em>primates</em>, which are <em>mammals</em>, which are <em>animals</em>, which an <em>organism</em> and finally part of the <em>entity</em> class. After downloading millions of images in thousands of categories, they were first automatically deduplicated, but the task of checking whether the image actually depicts the search term could not be done by an algorithm. Originally Fei Fei Li wanted to hire graduate students to sort the data, which was too costly and time-consuming.<a href="#fn98" class="footnote-ref" id="fnref98" role="doc-noteref"><sup>98</sup></a></p>
<p>One of her undergraduates suggested using a new service called Amazon Mechanical Turk, where people worldwide complete small tasks for little compensation. Even with the help of Mechanical Turk, it took 2,5 years to sort and validate the first dataset with 3.2 million images in over 5000 categories. At some point, ImageNet was the largest academic client for Amazon’s Turk service, and after the popularization of weighted networks, they became a cornerstone of data annotation. Similar platforms were established, and Mechanical Turk grew to half a million workers, rigorously exploited and alienated as ‘click workers.’ One paper estimates their median hourly wage to 2 USD, and only the top 4 per cent earn more than 7.25 USD per hour.<a href="#fn99" class="footnote-ref" id="fnref99" role="doc-noteref"><sup>99</sup></a> If the requester rejects the task, the worker does not get paid at all and has no way of appealing that decision. To advocate for better working conditions, the designer and researcher Caroline Sinders built an open-source tool for data annotation and a wage calculator to estimate the cost of labeling better.<a href="#fn100" class="footnote-ref" id="fnref100" role="doc-noteref"><sup>100</sup></a></p>
<p>With so many different people working on labeling images, the dataset eventually reflects societal biases. In 2019 the artist Trevor Paglen and researcher Kate Crawford collaborated on an exhibition titled <em>Training Humans</em>, dedicated to human image datasets. One of the main exhibits was a vast collection of human images from ImageNet.<a href="#fn101" class="footnote-ref" id="fnref101" role="doc-noteref"><sup>101</sup></a>   </p>
<p>As you go further down the category tree, you might find images of people labeled as “Bad Person, Call Girl, Drug Addict, Closet Queen, Convict,” and so on.<a href="#fn102" class="footnote-ref" id="fnref102" role="doc-noteref"><sup>102</sup></a> The artists used these absurd, racist, and misogynistic labels to train <em>ImageNet Roulette</em>,<a href="#fn103" class="footnote-ref" id="fnref103" role="doc-noteref"><sup>103</sup></a> a recognition algorithm accessible online and in an interactive installation. People online quickly picked up on the tool and shared images of themselves with sometimes amusing and often deeply problematic image captions.<a href="#fn104" class="footnote-ref" id="fnref104" role="doc-noteref"><sup>104</sup></a> While some of the people in the research community defended ImageNet, that these offensive labels are not part of the competition and make up only a tiny fraction of the total dataset, as a result of the media attention that followed, more than 600,000 images were removed. It is now only accessible after proving to be part of a scientific institute.</p>
<p>Another pair of artists and researchers, Adam Harvey and Jules LaPlace, have been exposing image sets which often get revoked and removed after publishing their investigation.<a href="#fn105" class="footnote-ref" id="fnref105" role="doc-noteref"><sup>105</sup></a> Harvey and LaPlace focus on datasets with faces captured “in the wild.” One particular example was the <em>UnConstrained College Students (UCCS)</em> dataset captured at the University of Colorado Colorado Springs. According to the authors of the associated papers to the dataset, they identified that current image sets created for face recognition research did not address the presence of unknown subjects. The authors of the papers associated with this dataset wanted to create a more realistic benchmark for face recognition research by introducing unknown subjects over time. To achieve this, they captured students on campus during breaks using a 800mm telephoto lens from a distance of 100-150 meters through an office window. The authors frame it as a benefit that the students are unaware of their capturing for the dataset as they casually focus on random activities, and their faces are sometimes blurred and occluded. To make things worse, this research was mainly funded by US defense and intelligence agencies. The public backlash was immense, and as a result, the dataset is no longer publicly available. The researchers did not see any ethical problems and argued that their subjects are anonymous as their names or other identifiers are not published.<a href="#fn106" class="footnote-ref" id="fnref106" role="doc-noteref"><sup>106</sup></a></p>
<p>However, when researchers need unconstrained and non-consensual data, they do not often capture them directly using creepy surveillance tactics. Starting with “Labeled Faces in the Wild” in 2007, collecting and labeling image data from internet sources has become normalized and remains largely unregulated. Often these image sets “in the wild” operate in a gray zone where they either depict public figures, arguing that privacy regulations do not apply, or simply link to the file and only store them temporarily for analysis. A third option uses media licensed under Creative Commons (CC), which is mostly considered free and legal data with no restrictions in the AI research community.<a href="#fn107" class="footnote-ref" id="fnref107" role="doc-noteref"><sup>107</sup></a> In a detailed report, Adam Harvey lists many datasets that exploit and often misuse CC licenses to create face and object recognition datasets.<a href="#fn108" class="footnote-ref" id="fnref108" role="doc-noteref"><sup>108</sup></a> He identifies Flickr as a significant source for collecting images, a popular image-sharing website where users can choose from various Creative Commons, copyright, and public domain licenses. Flickr actively promoted the use of CC licenses and offered unlimited free hosting if a CC license is used. Their strategy worked; by 2022, they amassed 467 million CC-licensed images. In 2014 a joint research group, including Yahoo Labs, the company that bought Flickr, shared one of the most extensive public media datasets<a href="#fn109" class="footnote-ref" id="fnref109" role="doc-noteref"><sup>109</sup></a> with the name <em>Yahoo! Flickr Creative Commons 100 Million (YFCC100M)</em>.</p>
<p>Other more specific datasets were created from this corpus. One is the <em>MegaFace</em> face recognition dataset with 4,7 million faces from 672,057 identities. While all the images fall under a CC license, most prohibit their commercial use and require appropriate attribution, which was not given in the dataset. As Harvey and LaPlace verified, the <em>MegaFace</em> dataset was used globally by commercial, military, and academic organizations.<a href="#fn110" class="footnote-ref" id="fnref110" role="doc-noteref"><sup>110</sup></a> As an investigative article from the New York Times explains, most people are unaware that their images are powering face recognition research worldwide, including companies with ties to China’s surveillance of the Uyghur population.<a href="#fn111" class="footnote-ref" id="fnref111" role="doc-noteref"><sup>111</sup></a> </p>
<p>To conclude, the three common issues with CC-licensed media are that they are not or wrongly attributed, the use of non-consensual biometric data is prohibited in some places (e.g., Illinois), and the use in commercial applications is often prohibited. Many of these issues identified by Harvey and LaPlace remain today and operate in a legal grey zone. While the notion behind uploading images under an open license for others to freely share, distribute, and remix media is a noble goal, the CC license is legally weak and practically useless against opting out of statistical analysis in AI research. But we might be slowly moving in a direction where lawmakers catch up and create precedences that disallow the use of biometric data in certain applications, like the upcoming AI Act.<a href="#fn112" class="footnote-ref" id="fnref112" role="doc-noteref"><sup>112</sup></a> </p>
<h2 id="this-person-does-exist">This Person Does Exist</h2>
<p>I examined a dataset using Creative Commons images more closely: <em>Flickr Faces HQ (FFHQ)</em>. In 2018 the research lab of Nvidia, one of the leading companies for visual computing, published a paper introducing a machine learning architecture called StyleGAN.<a href="#fn113" class="footnote-ref" id="fnref113" role="doc-noteref"><sup>113</sup></a> They improved generative adversarial networks (GAN) so that it was possible to create controllable synthetic high-resolution images. In simple terms, the system is able to abstract large amounts of images with a model that, in turn, outputs similar-looking pictures. In this case, the model can generate realistic-looking photographs of human faces. </p>
<p><em>FFHQ</em> is relatively small compared to other datasets, with only 70,000 images. As existing datasets were too low in resolution, a new corpus was created by scraping Creative Commons, Public Domain, or U.S. Government Works licensed images through Flickr’s API. The dataset is published under a CC-BY-NC-SA license, and the instructions for use and download are very clear, making it manageable for me in terms of size and effort to discover the underlying characteristics. The dataset consists only of photographic images, as Amazon Mechanical Turk workers checked it to remove occasional pictures of statues, paintings, or photos of photos. The raw images were automatically aligned and cropped around the face, using the open-source library dlib, to create a uniform square ratio of 1024px. This library also identifies 68 points outlining the chin, eyebrows, eyes, nose, and mouth, which are included in the metadata. So the final dataset used for training the GAN consists of a large number of faces where the eye and mouth positions are always in the same spot.</p>
<figure>
<img src="Screen_Shot_2023-03-15_at_20.50.49.png" alt="Screenshot of This Person Does Exist" /><figcaption aria-hidden="true">Screenshot of This Person Does Exist</figcaption>
</figure>
<p>One year after Nvidia released its StyleGAN paper, the software developer Phillip Wang published the website <em>thispersondoesnotexist.com</em>, which shows the capabilities of the generative model to create realistic-looking photographs of people.<a href="#fn114" class="footnote-ref" id="fnref114" role="doc-noteref"><sup>114</sup></a> The site quickly took off and alarmed people about the potential impact of AI systems in generating cheap synthetic media.  </p>
<p>As a counter-narrative to the AI image creator, I wanted to showcase the people who were used to condition this system. In 2020 I moved the cropped and aligned face images to my server and built my website with the name <em>this-person-does-exist.com</em>, which displays the faces from the training set alongside their metadata.<a href="#fn115" class="footnote-ref" id="fnref115" role="doc-noteref"><sup>115</sup></a> Looking at the individual faces facilitates an interpersonal connection with the unknown person and evokes a feeling for the images that were used to train the generative model. At the same time, it shows the creepy and strange practice of AI researchers using personal images as raw data.</p>
<p>As Flickr is used mostly by hobbyists and professional photographers, one can find portraits of children and families, speakers at conferences, or people on holiday. We can assume that, like other scraped datasets, the photographers of these images are unaware that their images are used for AI analysis. In contrast to other scraped datasets, the Nvidia researchers provide a tool to see if an image is part of the collection and allow the removal of the photograph.<a href="#fn116" class="footnote-ref" id="fnref116" role="doc-noteref"><sup>116</sup></a> According to Adam Harvey, the company does not disclose if any images were requested for removal and have not updated or removed any photos since its release in 2016.<a href="#fn117" class="footnote-ref" id="fnref117" role="doc-noteref"><sup>117</sup></a></p>
<figure>
<img src="Fig2.png" alt="70k faces from FFHQ compiled into one image" /><figcaption aria-hidden="true">70k faces from FFHQ compiled into one image</figcaption>
</figure>
<figure>
<img src="Fig3.png" alt="The Flickr Face - averaged FFHQ dataset" /><figcaption aria-hidden="true">The Flickr Face - averaged FFHQ dataset</figcaption>
</figure>
<p>To get a sense of the scale of the dataset, I compiled all face images into a grid, reducing the size of each image to 16 by 16px. This simple montage makes it possible to get a feeling for the vast amount of normalized image data.</p>
<p>The authors claim that FFHQ includes more variety than other face image sets in terms “of age, ethnicity and image background, and also has much better coverage of accessories such as eyeglasses, sun-glasses, hats, etc.”<a href="#fn118" class="footnote-ref" id="fnref118" role="doc-noteref"><sup>118</sup></a> They admit to biases inherited from the Flickr platform but fail to mention them. Looking through the data, it becomes clear that most images are taken with high-resolution digital camera systems under good lighting conditions. Many pictures separate the subject from the background by blurring the background, which conforms to photographic aesthetic norms and is easier to achieve with professional lenses with a lower aperture.</p>
<p>Indeed, the dataset contains a variety of skin tones, age groups, and backgrounds, but they are not equally distributed. I averaged the pixel values of the images together, suppressing outliers but allowing us to see an overall trend of the dataset and find a visual bias. The resulting composite image, the <em>Flickr Face</em>, reveals a tendency towards smiling and light-skinned people in the data set.</p>
<p>My project, <em>“This Person Does Exist,”</em> is equally flawed in preserving user privacy as the research labs in question. Still, by making people uncomfortable and showing the human behind AI systems, we can better understand how creepy this harvesting of faces is.</p>
<p>As a part of a growing group of artists exploring and exposing research datasets, I tried to find a new term for this genre: Dataset Art. My paper on this subject was published in <em>Temes de Disseny</em> in 2021 and included more examples from contemporary artists. Through their works, these artists make large datasets understandable and captivating. Their art has sometimes even created enough attention to lead some institutions to remove questionable parts or entire collections. Whether through galleries or online, Dataset Art provides a new way for viewers to peek into the workings of AI systems.<a href="#fn119" class="footnote-ref" id="fnref119" role="doc-noteref"><sup>119</sup></a></p>
<h2 id="scrapism">Scrapism</h2>
<p>Web scraping is the technique of using computer programs to visit links and automatically aggregate data from the internet. It is the backbone for many of the current machine-learning applications. The artist Sam Lavigne uses web scraping to create art with an emotional or critical message, a practice he calls ‘Scrapism.’<a href="#fn120" class="footnote-ref" id="fnref120" role="doc-noteref"><sup>120</sup></a></p>
<p>Perhaps an early work using the internet as a data source to convey emotion is <em>Listening Post</em> by Mark Hansen and Paul Ruben. The work was awarded the Golden Nica in 2004, uses snippets from internet chat rooms and displays them on over 200 LED signs while a crude computer voice reads them out loud. The artists are transforming live data into a sensual experience where the viewer enters a dark room and listens to the world chatter happening simultaneously all over the planet.<a href="#fn121" class="footnote-ref" id="fnref121" role="doc-noteref"><sup>121</sup></a></p>
<p>Instead of using and exposing datasets made for scientific research, Lavigne creates his own datasets by downloading and analyzing materials on the internet to revert common power structures. For example, he produced the online artwork <em>New York Apartment</em> with Tega Brain for the Artport Collection of the Whitney Museum.<a href="#fn122" class="footnote-ref" id="fnref122" role="doc-noteref"><sup>122</sup></a> In this work, they collected all for-sale listings on multiple real estate websites for New York City apartments and created a website that compiles all of them into one giant apartment listing. The value of this fictional apartment is over 43 billion USD and boasts 65,674 bedrooms and 55,588 bathrooms on around 3.4 million square meters. The website consists of multiple columns describing the listing with all of the clichéd languages and staged photographs common in the real estate market. They extruded the floor plans into 3D models and placed them together next to each other in a tower or pyramid formation so that you can explore this maze of apartments. The videos are cut up into thematic categories like “Welcome,” “Bedroom,” “Master,” or “Pre-war,” creating strange supercuts of panning and zooming shots of slick interiors.</p>
<p>Although looking through this compilation is funny and entertaining, it reveals the absurd language of luxury commodities. It shows the inequality of who can afford to own housing in a city like New York.</p>
<p>Another experiment by Sam Lavigne uses an open source hair detector to create a compilation of Mark Zuckerberg hairstyles in reaction to multiple people mocking the billionaire for his ‘terrible’ hairstyle.<a href="#fn123" class="footnote-ref" id="fnref123" role="doc-noteref"><sup>123</sup></a> This project, in particular, inspired me as it shows that using web scraping and machine learning technologies can sometimes be for silly projects at the expense of billionaires funding this tech for surveillance and personal profits.</p>
<figure>
<img src="lavigne_zuck-cut.jpg" alt="Sam Lavigne - Zuckerhair" /><figcaption aria-hidden="true">Sam Lavigne - Zuckerhair</figcaption>
</figure>
<h2 id="hidden.pictures">hidden.pictures</h2>
<p>When downloading things from the web, we often assume that everything is stored for eternity. “The web never forgets” is a common phrase to warn people before uploading sensitive and personal content online. It can be true for widely shared content or data automatically scraped and stored on sites like archive.org, but the web is brittle for many files and links. Domain names expire and cut the link to the requested page. But even if the hypertext can be accessed, they often contain links to files that no longer exist on the server; in the case of images, the browser then shows a broken image icon and an empty rectangle.</p>
<p>For the online artwork <em>hidden.pictures</em> (previously called <em>empty.photos</em>) I created a web crawler that visits random URLs, and whenever it hits an image that did not load correctly, it gets stored in a database. I collected thousands of broken image links with their metadata that sometimes describes the image.<a href="#fn124" class="footnote-ref" id="fnref124" role="doc-noteref"><sup>124</sup></a></p>
<p>I present the viewer with a collage of randomly placed image rectangles on a blank white website. A visitor can pan around in all directions, finding their web browser’s default rendering for broken images, often showcasing a ripped paper icon on the top left corner. When hovering over one of the boxes, the original URL is shown on the bottom left corner of the page, and whenever it existed, the alt-tag pops up next to the cursor.</p>
<p>These images from thousands of blogs, shops, and forums show the forgotten and neglected part of the internet. It invites the imagination about the internet that is not getting maintained and reveals an even bigger source of data; data that does not exist anymore.</p>
<p>When looking at this through the lens of weighted networks and training data, we have to ask what these models are learning when so many things get missing on the net every day. On the other hand, the models become a blurry snapshot of what they could gather but might not exist in the future.</p>
<figure>
<img src="emptyphotos-IMG_2261-HDR_1.jpg" alt="empty.photos exhibited at Best Off 2019" /><figcaption aria-hidden="true">empty.photos exhibited at Best Off 2019</figcaption>
</figure>
<h2 id="doggg.art">doggg.art</h2>
<p>In 2020 I created doggg.art, an exercise in <em>scrapism</em> where I downloaded and transformed content from the social media giant Instagram. Instagram has become the biggest tool for artists to find an audience and a community. Every possible niche can be found using hashtags like #dogart, which collects drawings and photographs of personal and commercial dog-related imagery. With around a million posts, it is only one example of the immense creative output on the image platform. Facebook, the mother company of Instagram, has access to abundant image data that they analyze and use to optimize internal computational models. By posting on the platform, a user “grant[s] a non-exclusive, royalty-free, transferable, sub-licensable, worldwide license to host, use, distribute, modify, run, copy, publicly perform or display, translate, and create derivative works of [their] content.”<a href="#fn125" class="footnote-ref" id="fnref125" role="doc-noteref"><sup>125</sup></a></p>
<p>I would describe doggg.art as a generative big dada collage consisting of over 30k images from Instagram tagged with #dogart. The images were processed using a machine learning algorithm called U^2-Net, which removed the background and other elements from the pictures. A website then randomly places the cutouts on a beige background, slowly fading them, creating an ever-changing digital dog meadow. It is a collaborative work that combines pieces by 38326 artists, credited on a separate page with all their unique usernames.<a href="#fn126" class="footnote-ref" id="fnref126" role="doc-noteref"><sup>126</sup></a></p>
<p>The artwork is intended as a screensaver reflecting on the aesthetics of the platform Instagram and how our relationship with pets extends into this online network.</p>
<figure>
<img src="doggg.art_Digitale.jpeg" alt="doggg.art exhibited at Die Digitale Düsseldorf exhibition “Digital Jokes”" /><figcaption aria-hidden="true">doggg.art exhibited at Die Digitale Düsseldorf exhibition “Digital Jokes”</figcaption>
</figure>
<figure>
<img src="doggg.art_Digitale_2.jpeg" alt="A visitor looking through the names of dog artists" /><figcaption aria-hidden="true">A visitor looking through the names of dog artists</figcaption>
</figure>
<h2 id="a-study-on-the-characteristics-of-douyin-meanwhile-in-china">A study on the Characteristics of Douyin / Meanwhile in China</h2>
<p>Working with and about social networks has been a big part of my artistic work. In 2019 I was invited by UNESCO to participate in a residency in the city of Changsha, the capital of the Hunan region and the “City of Media Arts” in central China. While there, I tried to make sense of the information landscape around me and got hooked on the Chinese clone of TikTok. </p>
<p>Douyin 抖音 became one of the most successful apps worldwide, the leading platform for creating and sharing short videos. Created by the Beijing-based company Bytedance, it is one of the few Apps that got successful outside of the great firewall. To comply with Chinese law, Douyin is an entirely separate App from TikTok. Even though the interface and logo look the same, the content is completely different and not accessible from the international version.</p>
<p>I asked S()fia Braga to join me as a collaborator, inspired by the work of <em>I stalk myself more than I should</em> where she captures Instagram stories, a feature designed to expose a video for a limited duration of 24 hours, and reappropriates them in a video installation. </p>
<p>Our work aimed to explore and analyze the vast digital ecosystem of Douyin from different perspectives, using their recommendation algorithm to lead us to various aspects of Chinese social media. We are using screen recordings to capture hours of video footage of us scrolling through our feeds. These found images are then decontextualized without alteration to give visitors the space to reflect upon them and gain insight into a walled-off platform and the algorithms designed for user engagement.  </p>
<p>We made two video installations, running 8 hours of captured material, showing people dancing next to Chinese police forces showcasing their equipment. Sometimes we would show the blank interface showing blank search results. Search terms like Donald Trump result in videos that do not show the American president themself, indicating that Douyin uses facial recognition for censorship.</p>
<p>In addition to the unaltered screen captured, we created nonsensical graphs that invoke the feeling that the content is used for statistical analysis. The title <em>A study on the characteristics of Douyin</em> was taken from a sociological paper and gave the veneer of scientific legitimacy to the non-consensual stalking and capturing of our practice. In another exhibition at the Ars Electronica Festival in Linz, we changed the title to <em>Meanwhile in China</em>. We covered the wall behind the video screen with a collage of graphs and data points found online showcasing the exploding growth and user distribution on the platform. However, the charts are stripped of any labeling, making them unreadable and useless as visualization.</p>
<figure>
<img src="IMG_1853.jpg" alt="A Study on the Characteristics of Douyin at Xie Zielong Photography Museum in Changsha" /><figcaption aria-hidden="true">A Study on the Characteristics of Douyin at Xie Zielong Photography Museum in Changsha</figcaption>
</figure>
<figure>
<img src="meanwhile_landscape.jpg" alt="Meanwhile In China at Ars Electronica 2019" /><figcaption aria-hidden="true">Meanwhile In China at Ars Electronica 2019</figcaption>
</figure>
<h2 id="recommended-hashtags-more-of-the-same">Recommended Hashtags &amp; More of the same</h2>
<p>I have had an account on Instagram since 2012, shortly after it was released on Android devices. The social network was based around square images, often highly edited with filters inspired by old analog photos. When I joined Instagram, I already had a critical view of large social networks like Facebook, where I started creating online and offline performances.</p>
<p>On Instagram, I would often share references to technologies in the social sphere and where they break, like screenshots of loading spinners, weird advertisements, or comments by bots. I often used generic hashtags to get attention outside my friend network and reach even more bots and strangers. The hashtag originated from the IRC protocol, where # was used before the name of a chat room (channel) and became a popular way on Twitter for tagging events, movements, or topics from 2007 onwards.<a href="#fn127" class="footnote-ref" id="fnref127" role="doc-noteref"><sup>127</sup></a> Today, it is a ubiquitous way of organizing and tagging content into categories, but it is most often used to get attention inside the network. For this, the game on social media is to add as many relevant and trending topics into the description as possible. Many companies have sprung up to help users ‘optimize’ their hashtags. From 2015 onward, I started using hashtags for nonsensical and humorous image descriptions and later decided to use hashtag generators to create lists of irrelevant tags for my posts on Instagram as social commentary.</p>
<p>In 2020 just after the first summer of the new COVID pandemic, S()fia Braga organized the <em>Internet Yami Ichi</em>, a black market for internet-inspired products invented by the Japanese art collective IDPW.<a href="#fn128" class="footnote-ref" id="fnref128" role="doc-noteref"><sup>128</sup></a> </p>
<p>I decided to print and sell the five years of my Instagram performance for this flea market with a book titled <em>Recommended Hashtags</em>, each of the 216 pages containing the image, hashtags, and number of likes. It also had a dedication to all my followers on the last page. After downloading and printing my Instagram data, I deleted all my images on the platform and added the label ‘Post-Instagram Artist’ to my account. </p>
<p>As users of Facebook’s platforms, we know our content is being analyzed for profit. With my online performance, I aimed to subvert and add noise to the system, albeit not enough to make a real difference. Selling my book was a way to compensate for the ‘work’ I have been doing for Facebook. I sold the book in a staggering model where each edition doubled in price from the previous one, making my old data more valuable over time while it was completely free at the time of creation. </p>
<p>In the following year, 2021, I started a new performance titled <em>more of the same</em> on my account using the dataset I downloaded as source material for a generative adversarial network. I trained a StyleGAN model, originally invented to create synthetic human faces, on the images I posted from 2015 to 2020. The model created abstract shapes, colors, and textures instead of replicating my images. As ~200 pictures are a very small dataset and the images are not very uniform ranging from screenshots to photographs depicting objects or people, the model could not converge on any meaningful representation. Many of the generated images are nearly symmetrical, reminiscent of Rorschach inkblots, which results from augmenting my image dataset by flipping the images. They also show circular blobs because of the model architecture and are seen as an error of the weighted network.<a href="#fn129" class="footnote-ref" id="fnref129" role="doc-noteref"><sup>129</sup></a></p>
<p>Ultimately, I wrote a small program that uploaded and posted a newly generated image with the caption “more of the same” for 100 days. </p>
<figure>
<img src="more_of_the_same.png" alt="Screenshot of more of the same | 200" /><figcaption aria-hidden="true">Screenshot of more of the same | 200</figcaption>
</figure>
<p><img src="Recommended_Hashtags_0.jpg" alt="Book cover of Recommended Hashtags | 200" /> <img src="Recommended_Hashtags_1.jpg" alt="First page of Recommended Hashtags | 200" /></p>
<h2 id="reflection">Reflection</h2>
<p>The idea behind computer programs that simulate the human brain goes back to the 1950s and not much has changed in the basic architecture of such systems, a few other things had to come together to make weighted networks perform so well. One crucial aspect was scale, which allowed researchers to access vast amounts of digital data uploaded and indexed on platforms and search engines. Another factor was the ability to programmatically exploit workers for minimal tasks like labeling images. Lastly, the utilization of parallel processing on graphics cards played a significant role in processing large datasets for an extended amount of time. I shared my personal experience how I got in contact with operational images through the viewing of 11,000 hands and investigated how creative commons licenses are used to deflect responsibility around privacy and copyright from researchers. As an example I looked through images of Flickr faces that served as a dataset for new synthetic image models. I also examined two art forms that emerged from these technological changes: Dataset Art and Scrapism. Dataset Art involves the re-framing of scientific datasets exposing them to a wider public and Scrapism is the practice of downloading assets from the internet for artistic purposes. I placed some of my works that fit into these categories and gave examples by other artists, such as Adam Harvey, Kate Crawford &amp; Trevor Paglen or Sam Lavigne. These projects all demonstrate how the internet shifted from a place to a corpus.</p>
<p>How the models derived from online data are reflecting us and how we find meaning in their outputs will be the focus of the next chapter. I will specifically take a closer look into text models which compress ever larger amounts of internet writing into weighted networks.</p>
<hr />
<h1 id="stochastic-meaning">Stochastic Meaning</h1>
<blockquote>
<p>What I had not realized is that extremely short exposures to a relatively simple computer program could induce powerful delusional thinking in quite normal people.<a href="#fn130" class="footnote-ref" id="fnref130" role="doc-noteref"><sup>130</sup></a></p>
</blockquote>
<h2 id="symbolic-language">Symbolic Language</h2>
<p>Understanding language is an essential aspect for computers to be truly useful. However, language is challenging since words can have various meanings depending on the context. Furthermore, there are multiple languages and different ways of converting spoken words into written symbols.</p>
<p>The use of computers for natural language processing (NLP) has been a subject of research since the invention of computers itself. One of the early examples of symbolic NLP is ELIZA, a computer program developed by Joseph Weizenbaum in the 1960s that simulated a conversation by rephrasing the input. ELIZA used pattern matching and substitution to generate responses based on the user’s input, giving the impression of understanding what was being said. For example, if the user said, “I feel sad,” ELIZA might respond with, “Why do you feel sad?” or “Tell me more about your feelings.”</p>
<p>While ELIZA was extremely limited in its capabilities, it sparked the general public’s interest and imagination, which saw the toy program as a breakthrough in artificial intelligence. Many people were fascinated by the idea that a machine could understand and respond language, even if the responses were simple and scripted. Joseph Weizenbaum himself was shocked about the willingness of people to attribute consciousness to even the most simple programs, like the 200 lines of code he wrote for the DOCTOR program that simulated (or, in his words <em>parodied</em>) a Regorian psychotherapist. Weizenbaum recalls that his office assistant, who was quite familiar with the program, demanded he leaves the office while conversing with the computer. Other people were shocked by the prospect that Weizenbaum would record the sessions with the chatbot, enough for him to believe that these people would share intimate and emotional feelings with the program. Some psychologists even considered using such systems to be able to treat more people in a shorter time, which for Weizenbaum was a horrendous thought and finally made him doubt the future of humanity and computers. In his 1976 book “Computer Power and Human Reason,” Weizenbaum argues that the increasing reliance on computers to solve problems and make decisions is a dangerous trend that could lead to dehumanization and the loss of autonomy of human beings. Conflating the computer with the human mind ultimately reduces the human to a computational being. He warns that we should not confuse the power of machines with human intelligence, intuition, and feeling, which are essential for ethical and meaningful interactions.<a href="#fn131" class="footnote-ref" id="fnref131" role="doc-noteref"><sup>131</sup></a></p>
<p>Two centuries later, another computer and cognitive scientist, Douglas Hofstadter, coined the term <em>Eliza effect</em> to describe the tendency of people to attribute intelligence and consciousness to computer programs. Even when people know of the inert limitations, the behavior and language used towards computer systems are often anthropomorphic. We empathize with supposed meaning and intentionality of a somewhat complex program, especially in probabilistic strings of text.<a href="#fn132" class="footnote-ref" id="fnref132" role="doc-noteref"><sup>132</sup></a></p>
<p>ELIZA paved the way for further development in natural language interaction. In the 1970s, some programmers created text-based games like Colossal Cave Adventure (1976), allowing users to interact with the game world using natural language commands. The program acts as a narrator describing the scene and the consequences of the actions taken by the user. The player explores a deep cave in search of supposed treasure by interacting with simple two-word text commands like ‘go west’ or ‘take lamp.’ Everything is pre-scripted and based upon conditionals, so if a room contains a lamp and the player executes the take command, the lamp will be stored and can be used in a puzzle in another room.<br />
Like the ELIZA program, Colossal Cave Adventure was first distributed to the limited computers in research facilities and played via a terminal to the main frame. However, the memetic distribution over the ARPANET, a predecessor to the internet, was much faster than copying disks. Many students immersed themselves in the interactive fiction and spent countless hours exploring the cave, solving puzzles, and uncovering hidden treasures.</p>
<p>At the same time, the use of conceptual ontologies, which are structured representations of knowledge in a particular domain, became popular. These ontologies can be used to help computers understand natural language by mapping words and phrases to their corresponding concepts in the ontology. For example, if someone says, “I want to buy a red car,” an ontology could map “red” and “car” to their corresponding concepts and infer that the person is interested in purchasing a vehicle with a specific color. One of the biggest endeavors in mapping words to concepts was the creation of WordNet in the 1980s, a lexical database of English words and their semantic relationships.</p>
<p>Until the early 2000s, programmers developed larger and more complex rule-based chatbots, creating systems like A.L.I.C.E. (1995), which used complex rules to simulate conversation with users. However, these systems were limited by their reliance on pre-programmed responses and lacked the ability to learn and adapt to new situations. The web application Cleverbot (2008) and its predecessor Jabberwacky (started in 1988 and online since 1997) shifted from a static database to one that is constantly growing.<a href="#fn133" class="footnote-ref" id="fnref133" role="doc-noteref"><sup>133</sup></a> By capturing every conversation and creating links of question-answer pairs, the system could use a fuzzy search algorithm to present the user with a likely response that was typed by another human previously. This application, based on statistical correlation and a large dataset of conversations, was able to trick 59.3% of people at the Techniche festival into believing that it was a human, as determined by the Turing test.<a href="#fn134" class="footnote-ref" id="fnref134" role="doc-noteref"><sup>134</sup></a></p>
<p>The Turing test, first proposed by British mathematician and computer scientist Alan Turing in 1950, is a measure of a machine’s ability to exhibit ‘intelligent behavior’ equivalent to or indistinguishable from that of a human. The test involves a human evaluator engaging in short natural language conversations over a text terminal with a machine and another human. If the evaluator cannot reliably distinguish between the machine and the human more than 50% of the time, the machine is said to have passed the Turing test.<a href="#fn135" class="footnote-ref" id="fnref135" role="doc-noteref"><sup>135</sup></a></p>
<p>While Cleverbot’s success in tricking humans may seem impressive, passing the Turing test does not necessarily indicate intelligence. The test only measures the ability to make people believe that another human is on the other side. It does not measure any reasoning ability or emotional cognition. In fact, many programmers developing such chatbots dumb down the output of the machine to get higher scores by adding delays, grammatical errors, or imitating a child.</p>
<p>These cases, from ELIZA to Cleverbot and even games like the Colossal Cave Adventure, involve users interacting with a machine through natural language commands. For the computer, there is no difference between the interactive fiction of an adventure game and the retrieval of likely text strings from a growing relational database. In both cases, the user is willing to immerse themselves in a fictional world created by other people. The difference here is that the computer game, in contrast to the big claims of artificial consciousness in chatterbots, invites people openly to join a world of make-believe, while the other is going one step further by using the Eliza Effect to create an illusion that there is no fictional element to it. It is like a magician leading the viewer’s attention away from the trick, but even when we know about the trickery involved, we so desperately want to believe that magic is real.</p>
<h2 id="attention-is-all-you-need">Attention is all you need</h2>
<p>While symbolic NLP methods have been successful in some applications (such as simple chatbots and question-answering systems), they have obvious limitations when handling ambiguity and nuances in language. As a result, more recent approaches to NLP have focused on machine learning techniques that allow computers to learn from large amounts of data without relying on pre-defined rules or ontologies.</p>
<p>The latest invention of this is the transformer architecture, which powers language models such as GPT (Generative Pre-trained Transformer). The transformer architecture was initially developed to solve language translation tasks in 2017.<a href="#fn136" class="footnote-ref" id="fnref136" role="doc-noteref"><sup>136</sup></a> In short, it takes sequential data like a string of words as an input and, after a conditioning loop, is able to predict the next likely data point in the sequence. More specifically, the Transformer architecture described in the paper “Attention is all you need” consists of an encoder that only receives input from the source language and the decoder, which is conditioned on the translation language.</p>
<p>The dataset is first divided into sub-word tokens that are commonly used. These tokens are a collection of numbers representing various parts of the original text, such as common words like ‘hello,’ smaller fragments like ‘lo,’ and even punctuation marks and individual letters. This tokenization helps to break down the text into manageable units for further analysis and processing.</p>
<p>The input to the encoder consists of a sequence of tokens in the source language, which are transformed into a sequence of hidden representations. Each of these hidden representations captures information as vectors in a high-dimensional space about the corresponding input token and its relationship to the other tokens in the sequence. On the other hand, the decoder receives input from both the encoder and the previously generated tokens in the target language. The decoder’s output is a probability distribution over the target language vocabulary, which is used to generate the next token in the translation.</p>
<p>Together, these two components form an end-to-end translation system that can convert between any two languages given enough data. The conditioning process involves optimizing various parameters and hyperparameters using backpropagation and gradient descent algorithms to minimize the error between predicted translations and the ground truth from the dataset.</p>
<p>One of the major advantages of transformers is their ability to handle long sequences of text efficiently. Traditional sequence-to-sequence models often struggle with longer sentences and paragraphs. Transformers overcome this limitation by using self-attention to capture dependencies between words, compared to previous techniques like Recurrent Neural Networks (RNNs), Convolutional Neural Networks (CNNs), or Long Short-Term Memory (LSTM). Self-attention allows the Transformer to weigh in all the words in the input sequence simultaneously and weigh the importance of each token in the context of the previous tokens. This allows the model to capture long-range dependencies between words, making it much more effective at handling longer sequences of text.</p>
<p>Using only the self-attention mechanism also means the process is highly parallelizable, as each step can calculate all tokens and their context simultaneously rather than sequentially calculating each token. This means that the context window of what these systems can attend to is pre-defined during the conditioning phase and is generally limited by the amount of GPU memory available. For example the context window for GPT-2 where it can relate words to each other is 1024 tokens.</p>
<p>Conditioning the original system described in the paper on 36 million English and French sentence pairs took 3.5 days on 8 GPUs, which was considered fast in 2017 compared to alternative RNN and CNN-based models. So both, the ability to weigh in long contexts of sequential data and efficiently train on high-end graphics cards made the Transformer architecture interesting for many applications, from translation to text generation and even image generation.</p>
<h2 id="too-dangerous-to-release">Too Dangerous To Release</h2>
<p>What Transformers are capable of appears to scale with the amount of input data. The more data is used, the better the system generalizes on a given task and the better the output. Testing the limits of this theory, the non-profit research company OpenAI<a href="#fn137" class="footnote-ref" id="fnref137" role="doc-noteref"><sup>137</sup></a> created GPT in 2018, which only uses the decoder network of the Transformer architecture, which makes it possible to use ‘unsupervised’ (not labeled) text data as a source. The first dataset was an open book corpus of ~7000 unreleased fiction novels with a size of 4.5GB.<a href="#fn138" class="footnote-ref" id="fnref138" role="doc-noteref"><sup>138</sup></a> In February 2019, attention turned towards OpenAI for withholding the release of their newest iteration GPT-2, which is ten times larger than the first version.<a href="#fn139" class="footnote-ref" id="fnref139" role="doc-noteref"><sup>139</sup></a> This was unprecedented as the standard for the AI research community was to open source the training data, models, and source code so that other researchers could reproduce the results and find biases and fail cases of the program. The Guardian titled “New AI fake text generator may be too dangerous to release, say creators,”<a href="#fn140" class="footnote-ref" id="fnref140" role="doc-noteref"><sup>140</sup></a> which sparked the imagination of rogue AI robots of the general public, creating both fear and fascination and a heated debate about the societal impact of Large Language Models (LLMs) on society. Conveniently it also created a lot of free PR for OpenAI, who saw this experiment in releasing their models successively as a case study for even more powerful models in the future.</p>
<p>The capabilities of GPT-2 were indeed novel and impressive, as the system can generate text which is both syntactically correct and (mostly) coherent in the output it produces. Most notably, the largest model with 1.5 billion parameters is large enough that it can be used for many downstream tasks like question answering, reading comprehension, summarization, and translation without needing specific labeled training data. By simply typing a few examples of how you want the system to behave, it can complete the rest with likely results in both style and subject. As the system is conditioned on 40GB of text scraped from the internet by scraping outgoing links from Reddit, which received at least three upvotes from other users, it is good in creating news-like output. This ability also makes it into a tool specifically suited to easily create fake news and spam, where the main concerns of OpenAI lie. The potential for misuse of such a powerful language model is high and can have severe consequences for society if they do not know that such tools even exist.</p>
<p>Soon after the announcement, the open source community started to rebuild the system based on the previous paper and released OpenGPT-2 in August 2019, conditioned on a similar dataset.<a href="#fn141" class="footnote-ref" id="fnref141" role="doc-noteref"><sup>141</sup></a> However, building this system is incredibly cost-intensive, using a large amount of computing power for weeks and months. OpenAI released its own larger models with the remark that they did not see strong evidence of misuse. For their newer generations, GPT-3 and further, the company became even more secretive about their model architecture and training data, only giving access to their models through a developer API.<a href="#fn142" class="footnote-ref" id="fnref142" role="doc-noteref"><sup>142</sup></a> One reason they provided is the need for computing clusters, only available to larger institutions and corporations, just to run inference on these very large language models, which again saw a size increase by orders of magnitude in terms of parameters.<a href="#fn143" class="footnote-ref" id="fnref143" role="doc-noteref"><sup>143</sup></a> Another reason might be the heavy investments of Microsoft and the need to commercialize the models they are creating.</p>
<h2 id="stochastic-parrots">Stochastic Parrots</h2>
<p>In late 2020 the AI ethics researcher Timnit Gebru was fired from Google over concerns about a yet unreleased paper she co-authored with other researchers from the company and the external computational linguist Emily M. Bender.<a href="#fn144" class="footnote-ref" id="fnref144" role="doc-noteref"><sup>144</sup></a> The paper discussed the potential harms of large language models, including their environmental impact, the biases they can perpetuate, and the power dynamics created by their ownership and control. It does not address Google or other companies developing ever larger transformers directly, but it does raise concerns about where attention and money is directed to in the field of NLP and tries to give possible alternatives creating a more thoughtful approach to creating language models that consider marginalized people.<a href="#fn145" class="footnote-ref" id="fnref145" role="doc-noteref"><sup>145</sup></a></p>
<p>Large language models rely on a massive amount of data collected from internet resources, which comes with the caveat that texts available on the internet do not equally represent all parts of society. Languages from the global south are often underrepresented, and user-generated text in places like Reddit or Wikipedia are disproportionately written by young male authors, leading to inherent biases in the language models developed using this data. As the training data grows, automated classifiers can not simply filter these biases and even worse cases of bigotry, sexism, and racism. A larger training set, therefore, does not equal more diverse data. Additionally, the computing power necessary to condition large language models emits an enormous amount of carbon dioxide, further amplifying climate change, which affects already marginalized communities much harder. Current research on increasingly large Transformer models mainly benefits those in privileged positions, amplifying hegemonic worldviews and prejudices, while people outside of big tech culture have to pay the costs of these experiments and have no say in shaping these technologies.</p>
<p>What exactly Transformer models are capturing from the hundreds of gigabytes of text data is still unclear. The system ultimately functions as a black box with a text-in-text-out paradigm, where the connections between word tokens should somehow embed the meaning of the underlying training data. Leading language models by OpenAI get even more obscured by the company’s decision not even to disclose the training source, making the attempt of analysis only possible through the output of the model itself. Large quantities of research efforts go towards optimizing how well a language model scores on benchmarks created for natural language understanding or question-answering tasks. However, as Gebru et al. argue, no actual language understanding is taking place by creating models that generate the most likely word based on a probability distribution: “languages are systems of signs, i.e. pairings of form and meaning. But the training data for LMs is only form; they do not have access to meaning.”<a href="#fn146" class="footnote-ref" id="fnref146" role="doc-noteref"><sup>146</sup></a></p>
<p>They call these systems, which manipulate linguistic form in a way to be similar to the source material, “Stochastic Parrots.” Not because they can simulate the mind of a bird but because they are parroting back words without meaning. On the other hand, these seemingly coherent sequences of words are getting interpreted by people in the same way as if written by human beings. Communication between humans usually presupposes an intention conveyed in language, while both individuals model each others’ mental states to understand the context they are situated in. This is most clear in direct communication but also holds true for spoken or written language for actors that are far apart. When we read a text, even from unknown authors, we build a mental model of who they are and what common ground we find ourselves in. With text generated by language models, this is not the case. The system does not have an intention or a will to act; it does not have a model of the world, let alone the reader’s state of mind. But even in this one-sided communication, the reader engages with language in the only commonly known way by creating the illusion of meaning.</p>
<p>The risks for such systems that manipulate the human psyche by creating the illusion of a feeling, understanding being (or thousands of them) are quite obvious: bad actors can automate the creation of disinformation, recruit people for extremist causes or simply add noise to information systems. Another potential danger of using text generators is the emotional connection that users may develop with them, believing they are interacting with an intelligent system. This can lead to a misplaced trust in the generated content, as seen in the case of recent trials in search engines where LLMs produce incorrect information presented as facts and supported by sources that do not exist.<a href="#fn147" class="footnote-ref" id="fnref147" role="doc-noteref"><sup>147</sup></a> In chat applications, uncontrolled and <em>seemingly</em> realistic conversations can have such devastating effects as reinforcing a suicidal person.<a href="#fn148" class="footnote-ref" id="fnref148" role="doc-noteref"><sup>148</sup></a></p>
<p>But who is accountable for harmful synthetic text? The use of automated algorithmic systems conveniently obfuscates and complicates the human decisions involved in it, shifting responsibility away from the company that deploys it to the automated system itself. If we were to believe that LLMs gained the status of personhood, the sole responsibility for their output is blamed on them, but no conventional penalty can apply. If we see them as simple tools, like knives, the person who uses it needs to be accountable for their action, but no blade can convince someone to murder, whereas a language model can be designed to do exactly that. So the people who develop and deploy large language models must be aware of the risks and harms and should do everything in their power to mitigate them.</p>
<h2 id="the-chiromancer">The Chiromancer</h2>
<figure>
<img src="the-chiromancer_HQ_2.jpg" alt="The Chiromancer" /><figcaption aria-hidden="true">The Chiromancer</figcaption>
</figure>
<p>In early 2020, I discussed with my colleague Giacomo Piazzi the increasing capabilities of language models like GPT-2 to generate arbitrary text. We talked about a previous project of mine on a hand dataset and pondered the magical thinking behind LLMs as conscious beings. Eventually, we decided to combine these concepts by creating a palm reading machine that creates the illusion of being able to predict the future.</p>
<p>Inspired by Eighteenth-century coin-operated fortunetelling machines, we wanted to upgrade this concept to the current age of technology, which operates as a black box using state-of-the-art language models. By using the hand and its reference to common touch gestures between humans and machines, we aim to amplify the trust in our system further. Relying on human biometric data to make predictions was another subtle comment on the state of AI and a way to individualize each fortune.</p>
<figure>
<img src="the-chiromancer_HQ_5.jpg" alt="The Chiromancer scanning my hand" /><figcaption aria-hidden="true">The Chiromancer scanning my hand</figcaption>
</figure>
<p>We use computers and AI models to help us make predictions about the future all the time. Be it weather and climate models, algorithmic trading on the stock market, or more personal systems such as news feeds or search engines. We often have to take their output at face value and adjust our lives accordingly, which is not dissimilar from ancient astrology, tarot, or hand reading practices. When people search on the internet, for example, about their current health problem, they sit in front of their glowing divination machine, hoping that the energy flowing through transnational cables brings them the correct answer. The information they receive is (hopefully) more often correct compared to a clairvoyant who might conduct a crystal ball to see into the future. Of course, the internet is a great place to reach out to many different kinds of automated and real practitioners of esoteric knowledge, too.</p>
<p>We wanted to use GPT-2 as a base to generate convincing palm readings, which comes in different parameter sizes 117M, 345M, 774M, and 1.5B. Though, the biggest model was not yet available at the time of our first experiments. The smaller models could be run on cheap or free cloud infrastructure with sufficient GPU memory. Trying to steer the next word prediction to something we want involves writing a creative prompt from which the model appends the next likely word tokens. After several failed attempts and writing ever larger prompts, it became apparent that using the model with just a few examples of the output we expect will not be stable enough. The likelihood of getting random gibberish was too high, so we needed to recalibrate the model weights.</p>
<p>To finetune our pretrained transformer model, we needed to find a data source that was large enough to prevent the model from overfitting and regurgitating the training set. We started by searching on Reddit for palmistry-related subforums and found /r/PalmReading/, where users sometimes upload a picture of their hand in the pursuit that someone deciphers it. It seemed perfect, but unfortunately, almost none of them got a response and if they did it was very short. Similar Facebook groups existed but served more of a sales funnel to private readings. Searching on YouTube, a few highly active accounts read the hands of famous or private people as examples. Fortunately, the video platform generates automatic subtitles, which are easy to download, and after putting them all together, we had around 4MB of text. The data scientist Max Woolf open sourced a simple wrapper that made it incredibly simple to run the finetuning loop.<a href="#fn149" class="footnote-ref" id="fnref149" role="doc-noteref"><sup>149</sup></a> After one night of conditioning on our simple YouTube dataset, we got some output that was similar to our source material. But as there was no punctuation or other syntactic elements, it resembled a waterfall of random sentences, including the occasional “please subscribe to my channel.”</p>
<p>Sample of GPT-2 345M finetuned on YouTube subtitles:</p>
<pre><code>okay let&#39;s see the last one on that on that line are your achievement lines do your past achievements and your upcoming achievements oh you&#39;re going to be a busy boy in your 50s there was circled ones of the extra good achievements either they&#39;re extra long there&#39;s a line to come up and off the top of that life line you&#39;re either extra long or they got really good color in them ages thirteen fourteen fifteen circled at nineteen circled at twenty-one twenty-three twenty-six thirty thirty nine forty six circled up fifty circled up fifty three circled at fifty-six they&#39;re called up fifty-seven hopefully there&#39;s some really good lines there down there at the base of that stump and then aged 59 aged 60 61 circled at 62 65 67 circled at 68 70 circled at 72 circled at 76 78 and circled at 82</code></pre>
<p>We could not find enough high quality palm reading case studies on the web, so we needed a different approach. The point of the artwork was not to create accurate readings in the first place, so we started to look at the adjacent field of astrology. Most fortunetelling practices make use of the Barnum-Forer effect, in which vague attributes and future scenarios are described, which can apply to a wide range of people that the individual then perceives to be true to them specifically. Some palmistry guides also subdivide the hand into mounts which, conveniently for us, have the same names as the planets in our solar system. Scraping horoscope data from the internet is a much easier task. Even some commercial APIs exist where we could send in a birth date and get a ‘custom’ prediction (except after cleaning our scraped dataset, we realized it only sends us a few 100 different text strings).</p>
<p>After finetuning on the horoscope dataset, we get decent results:</p>
<pre><code>You’re not the sort to play safe and even if you have been a bit more cautious than usual in recent weeks you will more than make up for it over the next few days. Plan your new adventure today and start working on it tomorrow.</code></pre>
<p>After finishing our fortunetelling model, we needed a hardware body to complete the illusion. The Chiromancer, just a fancier way of saying ‘Palmist,’ has a stack of 3 layers: On the top is a flatbed scanner with which our viewers will mainly interact. In the middle, the processing unit uses conventional desktop computer parts.<a href="#fn150" class="footnote-ref" id="fnref150" role="doc-noteref"><sup>150</sup></a> And at the bottom is an old dot-matrix printer that serves both as the device’s output and voice. Giacomo designed a mysterious black box using anodized aluminum beams and sleek black acrylic sheets to encase its components. Despite the fact that all of its components are exposed, the machine is unintelligible, and the only indication that it is functioning is through the movement of the GPU fan and a few pulsing orange LED lights. The cubic structure is standing on a black and silver flight case, ordinarily used to transport equipment for stage shows, again a reference and a modernization to the touring fortunetelling automata from the last century.</p>
<figure>
<img src="DSC01314.jpg" alt="Giacomo and I posing behind The Chiromancer at the Digital Square exhibition." /><figcaption aria-hidden="true">Giacomo and I posing behind The Chiromancer at the Digital Square exhibition.</figcaption>
</figure>
<p>Since 2021 we have exhibited The Chiromancer in several exhibitions around Europe. Out of all places, it’s debut was in a shopping mall in Salzburg. The “Digital Square” exhibition, made possible by Manuela Naveau and Fabricio Lamoncha, showcased students’ works crossing the boundaries of traditional art spaces and engaging with a broader public.<a href="#fn151" class="footnote-ref" id="fnref151" role="doc-noteref"><sup>151</sup></a> In this trial, we were able to observe if our machine, as alien as it might seem, is able to attract and engage with people. People were fascinated by the concept of a fortunetelling machine that uses technology to read their palms, and the critical aspects of AI systems and our reliance on their predictions were a good talking point after the initial spectacle.</p>
<figure>
<img src="chiro-IMG_2848.jpg" alt="Sample Print of The Chiromancer | 300" /><figcaption aria-hidden="true">Sample Print of The Chiromancer | 300</figcaption>
</figure>
<h2 id="reflection">Reflection</h2>
<p>Developing computer programs that manipulate symbols to create the illusion of a conversation is not new. Joseph Weizenbaum was one of the first to develop a chatbot and after observing how people interacted with it became one of the most vocal critics against the computerization of human beings. We are willingly immersing ourselves into a world of make-belief where the symbols become reality, not dissimilar to other contexts like computer games. The more complex our language manipulation software becomes, the more willing we are to attribute our own human traits to these systems. And even the researchers working on such systems are not protected from their default behavior to feel empathy. For the companies developing these, the notion of magic and mystery helps them protect against taking responsibility and it conveniently generates attention. With the advent of the transformer architecture text generators based on hundreds of gigabytes of text—the entire Wikipedia is only ~21GB—became somewhat feasible. The ability to predict the next word-tokens from such a large corpus makes of interesting applications that range from translation, correction and conversation. But this method comes with a baggage of downsides. Firstly, training such models requires significant financial and computational resources, which can worsen the climate crisis and disproportionately impact marginalized communities. Moreover, unmoderated models perpetuate (western) societal prejudices and contribute to systemic discrimination and oppression. Additionally, the lack of transparency and interpretability of these models, makes it difficult to understand how they reach their decisions. Finally, it is uncertain how these models will affect society as they create seemingly credible snippets that lack any factual basis.</p>
<p>With The Chiromancer we have created an artwork that examines the parallels between mysticism and AI systems. The esoteric practices of palm-reading and astrology, similar to computer predictions, rely on human cognitive biases of seeing patterns, identifying with them, and loosely extrapolating from them. In a way, both practices are attempts to understand the unknown and predict the future, albeit through different means. At the same time, The Chiromancer also raises questions about the ethics of using AI systems to make predictions about people’s lives. While astrology and palm-reading are often seen as harmless fun, what happens when we give such powers to machines? Why would we see such an analysis as ‘more credible?’ And what does that say about our current belief systems?</p>
<hr />
<h1 id="endoftext">&lt;|endoftext|&gt;</h1>
<h2 id="termination">Termination</h2>
<p>I started this journey with new friends through a shared interest in computers, trying to understand what “artificial intelligence” means, where it came from, and what this ideology envisions for our future with computer systems. I was intrigued by the sci-fi idea of virtual assistants and artificial pets that communicate with us through a common voice interface. However, as I delved deeper into the subject, I discovered a vast difference in how human language evolved and how computers are designed to analyze piles of data. The internet enables this data scraping process and has its own politics, raising questions about consent and what these new AI systems are ‘learning.’ Ultimately, I circled back to analyze the architecture of generative text models and how they create meaningful output. Only to realize that ‘intelligent’ machines are merely an illusion that exploit our innate ability to experience empathy and create meaning, particularly when interpreting symbols from our written languages. It is important to note that computers are not magical devices but are simply designed to perform magic tricks.</p>
<p>At first, my goal was to reveal the human efforts involved in constructing AI systems. This included the invention of electrical computers, coding languages, and the individuals who create, label, and organize data for these systems. I also recognized the significance of people utilizing these tools and how they are embedded into social structures. I hoped to change how we perceive AI technologies by acknowledging their origins in human creativity; maybe even cherish them for the collaborative effort that make them possible. However, I came to believe that exactly this obfuscation of human labor is a deliberate and essential part of the development and promotion of AI systems.</p>
<p>The myth of the self-sufficient AI agent, capable of learning and improving independently, is a powerful marketing tool. It promises a future where machines will make our lives easier and more efficient without requiring human input or intervention. This vision of AI as a fully autonomous entity ignores the fact that humans are integral to its development and obscures the potential consequences of this technology. As AI systems continue to be integrated into our daily lives by large corporations with the prospect of ‘natural technological advancement,’ it is important to recognize that they are not neutral or objective tools. Machines can only represent what can be abstracted and simplified and only encode a small subset of what exists in the world. They are built by humans who make selective choices and bring their biases, values, and perspectives into the software. The computer is then used to cloak their involvement. As Joseph Weizenbaum writes:</p>
<blockquote>
<p>“The myth of technological and political and social inevitability is a powerful tranquilizer of the conscience. Its service is to remove responsibility from the shoulders of everyone who truly believes in it. But, in fact, there <em>are</em> actors!”<a href="#fn152" class="footnote-ref" id="fnref152" role="doc-noteref"><sup>152</sup></a></p>
</blockquote>
<p>Weizenbaum’s comment is not only a demand for greater responsibility to those in tech industries and to look for the humans involved in making the decisions. He also reminds us that <em>we</em> are actors. We all have choices and responsibilities to shape the future of how we use computers and what we use them for. We are not, and should not, see ourselves just as victims of computational progress.</p>
<p>Projects like the <em>Silicon Friend Camp</em> are a way for communal and playful interaction, not only with machines but mainly with one another. It is a way to create collectives that digest and discuss both the technological and social implications of computer systems; it fosters an environment where people can learn from each other, share ideas, and collaborate on projects that are not driven by profit or market demand. The projects developed there deal with different subjects, from the environment, other animals, and virtual avatars, to our legacy in digital data, reflecting much broader perspectives beyond technology. Together we created a collective voice, quite literally, and shared it with others who were not co-present. The camp made me realize again that technology is not a means of itself; it is a social instrument, but often enough, it gets in the way of communication and isolates us in front of our screens.</p>
<p>Both the technological and social aspects have informed my body of work, and I hope to continue my journey into <em>artificial social networks</em>, as I have come to see these strange artifacts compressing the human experience into data points. I would have liked to write more about some things, like the intricacies of coding languages and the humor of their esoteric counterparts or the vast exploitation of labor orchestrated by computers. But for now, it is time to terminate the program and spend some time with my friends.&lt;|endoftext|&gt;<a href="#fn153" class="footnote-ref" id="fnref153" role="doc-noteref"><sup>153</sup></a></p>
<h1 id="contributions">Contributions</h1>
<p>Thank you to my supervisors <em>Manuela Naveau</em>, <em>Christa Sommerer</em> and <em>Laurent Mignonneau</em>.</p>
<p>A special thanks to <em>Davide Bevilacqua</em> and the <em>Servus.at</em> association for making the <em>Silicon Friend Camp</em>, exhibition, and symposium possible.</p>
<p>Thank you to <em>Bani Budassin</em> who invited me to write for Temes des Disseny, which formed my thoughts around Dataset Art.</p>
<p>And, of course, to all my new and old friends, contemporaries, and collaborators. I am because you are.</p>
<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-afifi11KHandsGender2018" class="csl-entry" role="doc-biblioentry">
Afifi, Mahmoud. 2018. <span>“<span>11k Hands</span>: <span>Gender</span> Recognition and Biometric Identification Using a Large Dataset of Hand Images.”</span> September 16, 2018. <a href="https://doi.org/10.48550/arXiv.1711.04322">https://doi.org/10.48550/arXiv.1711.04322</a>.
</div>
<div id="ref-afifi11kHands" class="csl-entry" role="doc-biblioentry">
———. n.d. <span>“11k <span>Hands</span>.”</span> Accessed March 7, 2023. <a href="https://sites.google.com/view/11khands">https://sites.google.com/view/11khands</a>.
</div>
<div id="ref-ArsElectronicaArchiv" class="csl-entry" role="doc-biblioentry">
<span>“Ars <span>Electronica Archiv</span>.”</span> n.d. Accessed March 20, 2023. <a href="https://archive.aec.at/prix/showmode/88/">https://archive.aec.at/prix/showmode/88/</a>.
</div>
<div id="ref-Avatars" class="csl-entry" role="doc-biblioentry">
<span>“Avatars.”</span> n.d. <span>So Kanno</span>. Accessed April 28, 2022. <a href="https://www.kanno.so/project/avatars">https://www.kanno.so/project/avatars</a>.
</div>
<div id="ref-Babycastles" class="csl-entry" role="doc-biblioentry">
<span>“Babycastles.”</span> n.d. <span>Babycastles</span>. Accessed January 30, 2022. <a href="https://www.babycastles.com">https://www.babycastles.com</a>.
</div>
<div id="ref-baileyDeepDreamCreatorUnveils2018" class="csl-entry" role="doc-biblioentry">
Bailey, Jason. 2018. <span>“<span>DeepDream Creator Unveils Very First Images After Three Years</span> — <span>Artnome</span>.”</span> December 30, 2018. <a href="https://www.artnome.com/news/2018/12/30/deepdream-creator-unveils-very-first-images-after-three-years">https://www.artnome.com/news/2018/12/30/deepdream-creator-unveils-very-first-images-after-three-years</a>.
</div>
<div id="ref-bandyAddressingDocumentationDebt" class="csl-entry" role="doc-biblioentry">
Bandy, Jack, and Nicholas Vincent. n.d. <span>“Addressing "<span>Documentation Debt</span>" in <span>Machine Learning</span>: <span>A Retrospective Datasheet</span> for <span>BookCorpus</span>.”</span>
</div>
<div id="ref-BetterImagesAI" class="csl-entry" role="doc-biblioentry">
<span>“Better <span>Images</span> of <span>AI</span>.”</span> n.d. Accessed March 4, 2023. <a href="https://betterimagesofai.org">https://betterimagesofai.org</a>.
</div>
<div id="ref-bostromAreYouLiving2001" class="csl-entry" role="doc-biblioentry">
Bostrom, Nick. 2001. <span>“Are <span>You Living</span> in a <span>Computer Simulation</span>?”</span> 14. <a href="https://www.simulation-argument.com/simulation.pdf">https://www.simulation-argument.com/simulation.pdf</a>.
</div>
<div id="ref-britishmovietoneHumanVoiceMachine2015" class="csl-entry" role="doc-biblioentry">
British Movietone, dir. 2015. <em>Human <span>Voice Machine</span></em>. <a href="https://www.youtube.com/watch?v=RZK9mEx59ts">https://www.youtube.com/watch?v=RZK9mEx59ts</a>.
</div>
<div id="ref-chakelianJourneyManyFaces2021" class="csl-entry" role="doc-biblioentry">
Chakelian, Anoosh. 2021. <span>“The Journey and Many Faces of the Hash Symbol.”</span> <span>New Statesman</span>. June 9, 2021. <a href="https://www.newstatesman.com/science-tech/2014/06/history-journey-and-many-faces-hash-symbol">https://www.newstatesman.com/science-tech/2014/06/history-journey-and-many-faces-hash-symbol</a>.
</div>
<div id="ref-ciresanFlexibleHighPerformance" class="csl-entry" role="doc-biblioentry">
Ciresan, Dan C, Ueli Meier, Jonathan Masci, Luca M Gambardella, and Jurgen Schmidhuber. n.d. <span>“Flexible, <span>High Performance Convolutional Neural Networks</span> for <span>Image Classification</span>.”</span>
</div>
<div id="ref-Cleverbot" class="csl-entry" role="doc-biblioentry">
<span>“Cleverbot.”</span> n.d. <span>Cleverbot</span>. Accessed March 21, 2023. <a href="http://www.cleverbot.com/">http://www.cleverbot.com/</a>.
</div>
<div id="ref-coleThisHorrifyingApp2019" class="csl-entry" role="doc-biblioentry">
Cole, Samantha. 2019. <span>“This <span>Horrifying App Undresses</span> a <span>Photo</span> of <span>Any Woman With</span> a <span>Single Click</span>.”</span> <span>Vice</span>. June 26, 2019. <a href="https://www.vice.com/en/article/kzm59x/deepnude-app-creates-fake-nudes-of-any-woman">https://www.vice.com/en/article/kzm59x/deepnude-app-creates-fake-nudes-of-any-woman</a>.
</div>
<div id="ref-dempsey-jonesNeuroscientistsPutDubious2018" class="csl-entry" role="doc-biblioentry">
Dempsey-Jones, Harriet. 2018. <span>“Neuroscientists Put the Dubious Theory of ’Phrenology’ Through Rigorous Testing for the First Time.”</span> <span>The Conversation</span>. January 22, 2018. <a href="http://theconversation.com/neuroscientists-put-the-dubious-theory-of-phrenology-through-rigorous-testing-for-the-first-time-88291">http://theconversation.com/neuroscientists-put-the-dubious-theory-of-phrenology-through-rigorous-testing-for-the-first-time-88291</a>.
</div>
<div id="ref-dengImageNetLargeScaleHierarchical2009" class="csl-entry" role="doc-biblioentry">
Deng, J., W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. 2009. <span>“<span>ImageNet</span>: <span>A Large-Scale Hierarchical Image Database</span>.”</span> In <em><span>Cvpr09</span></em>.
</div>
<div id="ref-EverythingDLVIDOREILLY" class="csl-entry" role="doc-biblioentry">
<span>“Everything — <span>DΛVID OREILLY</span> • Computer Art &amp; Research.”</span> n.d. Accessed April 28, 2022. <a href="https://www.davidoreilly.com/everything/">https://www.davidoreilly.com/everything/</a>.
</div>
<div id="ref-ExploringPossibilitySpace" class="csl-entry" role="doc-biblioentry">
<span>“Exploring <span>Possibility Space</span>: <span>Microsoft</span> #<span>TAYFAIL Smoking Gun</span>: <span>ALICE Open Source AI Library</span> and <span>AIML</span>.”</span> n.d. Accessed March 21, 2023. <a href="https://exploringpossibilityspace.blogspot.com/2016/03/microsoft-tayfail-smoking-gun-alice.html">https://exploringpossibilityspace.blogspot.com/2016/03/microsoft-tayfail-smoking-gun-alice.html</a>.
</div>
<div id="ref-francishungerArtificialIntelligenceAutomated2021" class="csl-entry" role="doc-biblioentry">
Francis Hunger. 2021. <span>“1. ’<span>Artificial Intelligence</span>’ =<span><span class="math inline">&gt;</span></span> ’ <span>Automated Pattern Recognition</span>’ 2. ’<span>Machine Learning</span>’ =<span><span class="math inline">&gt;</span></span> ’<span>Machine Conditioning</span>’ <span>OR</span> <span>‘<span>Automated Classification</span>’</span> 3. ’<span>Neural Network</span>’ =<span><span class="math inline">&gt;</span></span> ’<span>Weighted Network</span>’ 4. <span>‘<span>Deep Learning</span>’</span> =<span><span class="math inline">&gt;</span></span> <span>‘<span>Deep Conditioning</span>’</span> 5. <span>‘<span>Neuron</span>’</span> =<span><span class="math inline">&gt;</span></span> <span>‘<span>Weight</span>’</span> or <span>‘<span>Node</span>’</span>.”</span> Tweet. <span>Twitter</span>. July 9, 2021. <a href="https://twitter.com/databaseculture/status/1413462059291975680">https://twitter.com/databaseculture/status/1413462059291975680</a>.
</div>
<div id="ref-FurbySourceCode" class="csl-entry" role="doc-biblioentry">
<span>“Furby <span>Source Code</span>.”</span> n.d. <a href="http://www.seanriddle.com/furbysource.pdf">http://www.seanriddle.com/furbysource.pdf</a>.
</div>
<div id="ref-gadeWhatUbuntuDifferent2012" class="csl-entry" role="doc-biblioentry">
Gade, Christian B. N. 2012. <span>“What Is <span><em>Ubuntu</em></span> ,? <span>Different Interpretations</span> Among <span>South Africans</span> of <span>African Descent</span>.”</span> <em>South African Journal of Philosophy</em> 31 (3): 484–503. <a href="https://doi.org/10.1080/02580136.2012.10751789">https://doi.org/10.1080/02580136.2012.10751789</a>.
</div>
<div id="ref-godfreyjohnj.Switchboard1Release1993" class="csl-entry" role="doc-biblioentry">
Godfrey, John J., and Edward Holliman. 1993. <span>“Switchboard-1 <span>Release</span> 2.”</span> <span>Linguistic Data Consortium</span>. <a href="https://doi.org/10.35111/SW3H-RW02">https://doi.org/10.35111/SW3H-RW02</a>.
</div>
<div id="ref-harveyExposingAi" class="csl-entry" role="doc-biblioentry">
Harvey, Adam. n.d. <span>“Exposing.ai.”</span> <span>Exposing.ai</span>. Accessed March 11, 2023. <a href="https://exposing.ai/">https://exposing.ai/</a>.
</div>
<div id="ref-hertzfeldFolkloreOrgIt1984" class="csl-entry" role="doc-biblioentry">
Hertzfeld, Andy. 1984. <span>“Folklore.org: <span>It Sure Is Great To Get Out Of That Bag</span>!”</span> January 1, 1984. <a href="https://www.folklore.org/StoryView.py?project=Macintosh&amp;story=Intro_Demo.txt">https://www.folklore.org/StoryView.py?project=Macintosh&amp;story=Intro_Demo.txt</a>.
</div>
<div id="ref-hofstadterFluidConceptsCreative1995" class="csl-entry" role="doc-biblioentry">
Hofstadter, Douglas R. 1995. <em>Fluid Concepts &amp; Creative Analogies: Computer Models of the Fundamental Mechanisms of Thought</em>. <span>New York, NY</span>: <span>Basic Books</span>.
</div>
<div id="ref-HumanguidedBurritoBots2019" class="csl-entry" role="doc-biblioentry">
<span>“Human-Guided Burrito Bots Raise Questions about the Future of Robo-Delivery.”</span> 2019. <span>The Hustle</span>. June 3, 2019. <a href="https://thehustle.co/kiwibots-autonomous-food-delivery/">https://thehustle.co/kiwibots-autonomous-food-delivery/</a>.
</div>
<div id="ref-hungerTalkUnhypeAI2021" class="csl-entry" role="doc-biblioentry">
Hunger, Francis. 2021. <span>“Talk: <span>Unhype AI</span> (2021).”</span> April 28, 2021. <a href="https://www.irmielin.org/unhype-ai/">https://www.irmielin.org/unhype-ai/</a>.
</div>
<div id="ref-Ilias" class="csl-entry" role="doc-biblioentry">
<span>“Ilias.”</span> n.d. Accessed April 10, 2023. <a href="https://www.projekt-gutenberg.org/homer/ilias/ilias183.html">https://www.projekt-gutenberg.org/homer/ilias/ilias183.html</a>.
</div>
<div id="ref-ImageNetRouletteTrevor" class="csl-entry" role="doc-biblioentry">
<span>“<span>ImageNet Roulette</span> – <span>Trevor Paglen</span>.”</span> n.d. Accessed March 11, 2023. <a href="https://paglen.studio/2020/04/29/imagenet-roulette/">https://paglen.studio/2020/04/29/imagenet-roulette/</a>.
</div>
<div id="ref-ImageNet" class="csl-entry" role="doc-biblioentry">
<span>“<span>ImageNet</span>.”</span> n.d. Accessed March 4, 2023. <a href="https://image-net.org/index.php">https://image-net.org/index.php</a>.
</div>
<div id="ref-InceptionismGoingDeeper2015" class="csl-entry" role="doc-biblioentry">
<span>“Inceptionism: <span>Going Deeper</span> into <span>Neural Networks</span>.”</span> 2015. June 17, 2015. <a href="https://ai.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html">https://ai.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html</a>.
</div>
<div id="ref-JabberwackyThoughtsArtificial2006" class="csl-entry" role="doc-biblioentry">
<span>“Jabberwacky - <span>About Thoughts</span> - <span>An Artificial Intelligence AI</span> Chatbot, Chatterbot or Chatterbox, Learning <span>AI</span>, Database, Dynamic - Models Way Humans Learn - Simulate Natural Human Chat - Interesting, Humorous, Entertaining.”</span> 2006. July 1, 2006. <a href="https://web.archive.org/web/20060701052309/http://www.jabberwacky.com/j2about">https://web.archive.org/web/20060701052309/http://www.jabberwacky.com/j2about</a>.
</div>
<div id="ref-jemineMasterThesisAutomatic" class="csl-entry" role="doc-biblioentry">
Jemine, C. n.d. <span>“Master Thesis : <span>Automatic Multispeaker Voice Cloning</span>,”</span> 38.
</div>
<div id="ref-jemineRealTimeVoiceCloning2022" class="csl-entry" role="doc-biblioentry">
Jemine, Corentin. (2019) 2022. <em>Real-<span>Time Voice Cloning</span></em>. <a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning">https://github.com/CorentinJ/Real-Time-Voice-Cloning</a>.
</div>
<div id="ref-karrasAnalyzingImprovingImage2020" class="csl-entry" role="doc-biblioentry">
Karras, Tero, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. 2020. <span>“Analyzing and <span>Improving</span> the <span>Image Quality</span> of <span>StyleGAN</span>.”</span> March 23, 2020. <a href="http://arxiv.org/abs/1912.04958">http://arxiv.org/abs/1912.04958</a>.
</div>
<div id="ref-KATECRAWFORDTREVOR" class="csl-entry" role="doc-biblioentry">
<span>“<span>KATE CRAWFORD</span> | <span>TREVOR PAGLEN</span>: <span>TRAINING HUMANS</span> – <span>Fondazione Prada</span>.”</span> n.d. Accessed March 10, 2023. <a href="https://www.fondazioneprada.org/project/training-humans/?lang=en">https://www.fondazioneprada.org/project/training-humans/?lang=en</a>.
</div>
<div id="ref-kotowskiLatentSpaceDivination2021" class="csl-entry" role="doc-biblioentry">
Kotowski, Błażej. 2021. <span>“Latent <span>Space Divination</span>.”</span> 2021. <a href="http://lsd.blazejkotowski.com/">http://lsd.blazejkotowski.com/</a>.
</div>
<div id="ref-lessioMakingSADHome2020" class="csl-entry" role="doc-biblioentry">
Lessio, Nadine. 2020. <span>“Making <span><em>SAD Home</em></span> : <span>An</span> Exploration into Developing an <span>Alexa</span> with Depression.”</span> <em>Virtual Creativity</em> 10 (1): 59–71. <a href="https://doi.org/10.1386/vcr_00018_1">https://doi.org/10.1386/vcr_00018_1</a>.
</div>
<div id="ref-liuMeasuringFairnessSpeech2021" class="csl-entry" role="doc-biblioentry">
Liu, Chunxi, Michael Picheny, Leda Sarı, Pooja Chitkara, Alex Xiao, Xiaohui Zhang, Mark Chou, Andres Alvarado, Caner Hazirbas, and Yatharth Saraf. 2021. <span>“Towards <span>Measuring Fairness</span> in <span>Speech Recognition</span>: <span>Casual Conversations Dataset Transcriptions</span>.”</span> <a href="http://arxiv.org/abs/2111.09983">http://arxiv.org/abs/2111.09983</a>.
</div>
<div id="ref-markertMichaelMarkertPortfolio" class="csl-entry" role="doc-biblioentry">
Markert, Michael. n.d. <span>“Michael <span>Markert</span> <span>Portfolio</span>.”</span> <span>audiocommander.de</span>. Accessed April 10, 2023. <a href="https://www.audiocommander.de/portfolio/">https://www.audiocommander.de/portfolio/</a>.
</div>
<div id="ref-matthiasschafer11kHands2018" class="csl-entry" role="doc-biblioentry">
Matthias Schäfer, dir. 2018. <em>11k Hands</em>. <a href="https://www.youtube.com/watch?v=snJsKFxPlJ8">https://www.youtube.com/watch?v=snJsKFxPlJ8</a>.
</div>
<div id="ref-millerArtistMachineWorld2019" class="csl-entry" role="doc-biblioentry">
Miller, Arthur I. 2019. <em>The Artist in the Machine: The World of <span>AI</span> Powered Creativity</em>. <span>Cambridge, Massachusetts</span>: <span>The MIT Press</span>.
</div>
<div id="ref-mitchellArtificialIntelligenceGuide2019" class="csl-entry" role="doc-biblioentry">
Mitchell, Melanie. 2019. <em>Artificial Intelligence: A Guide for Thinking Humans</em>. <span>New York</span>: <span>Farrar, Straus and Giroux</span>.
</div>
<div id="ref-molnar10LearnedFeatures" class="csl-entry" role="doc-biblioentry">
Molnar, Christoph. n.d. <em>10.1 <span>Learned Features</span> | <span>Interpretable Machine Learning</span></em>. Accessed March 4, 2023. <a href="https://christophm.github.io/interpretable-ml-book/cnn-features.html">https://christophm.github.io/interpretable-ml-book/cnn-features.html</a>.
</div>
<div id="ref-murphyWhyStanfordResearchers2017" class="csl-entry" role="doc-biblioentry">
Murphy, Heather. 2017. <span>“Why <span>Stanford Researchers Tried</span> to <span>Create</span> a <span>‘<span>Gaydar</span>’</span> <span>Machine</span>.”</span> <em>The New York Times: Science</em>, October 9, 2017. <a href="https://www.nytimes.com/2017/10/09/science/stanford-sexual-orientation-study.html">https://www.nytimes.com/2017/10/09/science/stanford-sexual-orientation-study.html</a>.
</div>
<div id="ref-NutzungsbedingungenInstagramHilfebereich" class="csl-entry" role="doc-biblioentry">
<span>“Nutzungsbedingungen | <span>Instagram-Hilfebereich</span>.”</span> n.d. Accessed March 17, 2023. <a href="https://help.instagram.com/581066165581870">https://help.instagram.com/581066165581870</a>.
</div>
<div id="ref-OpenFutureOpen" class="csl-entry" role="doc-biblioentry">
<span>“Open <span>Future</span> – <span>Open Future Foundation</span>.”</span> n.d. <span>Open Future</span>. Accessed March 13, 2023. <a href="https://openfuture.eu">https://openfuture.eu</a>.
</div>
<div id="ref-panayotovLibrispeechASRCorpus2015" class="csl-entry" role="doc-biblioentry">
Panayotov, Vassil, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. 2015. <span>“Librispeech: <span>An ASR</span> Corpus Based on Public Domain Audio Books.”</span> In <em>2015 <span>IEEE International Conference</span> on <span>Acoustics</span>, <span>Speech</span> and <span>Signal Processing</span> (<span>ICASSP</span>)</em>, 5206–10. <span>South Brisbane, Queensland, Australia</span>: <span>IEEE</span>. <a href="https://doi.org/10.1109/ICASSP.2015.7178964">https://doi.org/10.1109/ICASSP.2015.7178964</a>.
</div>
<div id="ref-parkinsonHeySiriMeet2015" class="csl-entry" role="doc-biblioentry">
Parkinson, Hannah Jane. 2015. <span>“Hey, <span>Siri</span>! <span>Meet</span> the Real People Behind <span>Apple</span>’s Voice-Activated Assistant | <span>Apple</span> | <span>The Guardian</span>.”</span> August 12, 2015. <a href="https://www.theguardian.com/technology/2015/aug/12/siri-real-voices-apple-ios-assistant-jon-briggs-susan-bennett-karen-jacobsen">https://www.theguardian.com/technology/2015/aug/12/siri-real-voices-apple-ios-assistant-jon-briggs-susan-bennett-karen-jacobsen</a>.
</div>
<div id="ref-RuDALLE" class="csl-entry" role="doc-biblioentry">
<span>“<span class="nocase">ruDALL-E</span>.”</span> n.d. <span>ruDALL-E</span>. Accessed April 28, 2022. <a href="http://rudalle.ru/en/">http://rudalle.ru/en/</a>.
</div>
<div id="ref-sampathkumarAmazonEchoCould2017" class="csl-entry" role="doc-biblioentry">
Sampathkumar, Mythili. 2017. <span>“Amazon <span>Echo</span> Could Become Key Witness in Murder Investigation After Data Turned over to Police | <span>The Independent</span> | <span>The Independent</span>.”</span> March 9, 2017. <a href="https://www.independent.co.uk/news/world/americas/amazon-echo-murder-investigation-data-police-a7621261.html">https://www.independent.co.uk/news/world/americas/amazon-echo-murder-investigation-data-police-a7621261.html</a>.
</div>
<div id="ref-schaferMissingPictures" class="csl-entry" role="doc-biblioentry">
Schäfer, Matthias. n.d. <span>“Missing.pictures.”</span> Accessed March 17, 2023. <a href="https://missing.pictures/">https://missing.pictures/</a>.
</div>
<div id="ref-shulgin386DXCyberpunkrock" class="csl-entry" role="doc-biblioentry">
Shulgin, Alexei. n.d. <span>“386 <span>DX</span> - <span class="nocase">Cyberpunk-rock</span> Band.”</span> Accessed April 10, 2023. <a href="http://www.easylife.org/386dx/">http://www.easylife.org/386dx/</a>.
</div>
<div id="ref-stuppFraudstersUsedAI2019" class="csl-entry" role="doc-biblioentry">
Stupp, Catherine. 2019. <span>“Fraudsters <span>Used AI</span> to <span>Mimic CEO</span>’s <span>Voice</span> in <span>Unusual Cybercrime Case</span>.”</span> <em>Wall Street Journal: WSJ Pro</em>, August 30, 2019. <a href="https://www.wsj.com/articles/fraudsters-use-ai-to-mimic-ceos-voice-in-unusual-cybercrime-case-11567157402">https://www.wsj.com/articles/fraudsters-use-ai-to-mimic-ceos-voice-in-unusual-cybercrime-case-11567157402</a>.
</div>
<div id="ref-TalkingHeadInvention" class="csl-entry" role="doc-biblioentry">
<span>“Talking <span>Head</span> | <span>Invention</span> &amp; <span>Technology Magazine</span>.”</span> n.d. Accessed April 10, 2023. <a href="https://www.inventionandtech.com/content/talking-head-1">https://www.inventionandtech.com/content/talking-head-1</a>.
</div>
<div id="ref-tatmanGenderDialectBias2017" class="csl-entry" role="doc-biblioentry">
Tatman, Rachael. 2017. <span>“Gender and <span>Dialect Bias</span> in <span>YouTube</span>’s <span>Automatic Captions</span>.”</span> In <em>Proceedings of the <span>First ACL Workshop</span> on <span>Ethics</span> in <span>Natural Language Processing</span></em>, 53–59. <span>Valencia, Spain</span>: <span>Association for Computational Linguistics</span>. <a href="https://doi.org/10.18653/v1/W17-1606">https://doi.org/10.18653/v1/W17-1606</a>.
</div>
<div id="ref-TechnicallyResponsibleKnowledge" class="csl-entry" role="doc-biblioentry">
<span>“Technically <span>Responsible Knowledge</span>.”</span> n.d. Accessed March 10, 2023. <a href="http://trk.network/essay">http://trk.network/essay</a>.
</div>
<div id="ref-CanonCatMac2014" class="csl-entry" role="doc-biblioentry">
<span>“The <span>Canon Cat The Mac</span>’s <span>Ancestor</span>.”</span> 2014. October 25, 2014. <a href="https://web.archive.org/web/20141025053109/http://www.jagshouse.com/swyft.html">https://web.archive.org/web/20141025053109/http://www.jagshouse.com/swyft.html</a>.
</div>
<div id="ref-InternetYamiIchi" class="csl-entry" role="doc-biblioentry">
<span>“The Internet Yami-Ichi.”</span> n.d. <span>In Kepler’s Gardens</span>. Accessed March 20, 2023. <a href="https://ars.electronica.art/keplersgardens/de/the-internet-yami-ichi/">https://ars.electronica.art/keplersgardens/de/the-internet-yami-ichi/</a>.
</div>
<div id="ref-ThisPersonDoes2019" class="csl-entry" role="doc-biblioentry">
<span>“This <span>Person Does Not Exist</span>.”</span> 2019. May 31, 2019. <a href="https://web.archive.org/web/20190531222303/https://thispersondoesnotexist.com/">https://web.archive.org/web/20190531222303/https://thispersondoesnotexist.com/</a>.
</div>
<div id="ref-TIMEMagazineCover" class="csl-entry" role="doc-biblioentry">
<span>“<span>TIME Magazine Cover</span>: <span>Tech Comes</span> to <span>Toyland</span> - <span>Dec</span>. 11, 2000 - <span>Science</span> &amp; <span>Technology</span> - <span>Toys</span>.”</span> n.d. Accessed April 10, 2023. <a href="https://content.time.com/time/covers/europe/0,16641,20001211,00.html">https://content.time.com/time/covers/europe/0,16641,20001211,00.html</a>.
</div>
<div id="ref-tykaDeepdreamInceptionismRecap" class="csl-entry" role="doc-biblioentry">
Tyka, Mike. n.d. <span>“Deepdream/<span>Inceptionism</span> - Recap.”</span> Accessed March 4, 2023. <a href="http://mtyka.github.io/code/2015/07/21/one-month-after-deepdream.html">http://mtyka.github.io/code/2015/07/21/one-month-after-deepdream.html</a>.
</div>
<div id="ref-vanhouckeSpeechRecognitionDeep2012" class="csl-entry" role="doc-biblioentry">
Vanhoucke, Vincent. 2012. <span>“Speech <span>Recognition</span> and <span>Deep Learning</span>.”</span> August 6, 2012. <a href="https://ai.googleblog.com/2012/08/speech-recognition-and-deep-learning.html">https://ai.googleblog.com/2012/08/speech-recognition-and-deep-learning.html</a>.
</div>
<div id="ref-wanGeneralizedEndtoEndLoss2020" class="csl-entry" role="doc-biblioentry">
Wan, Li, Quan Wang, Alan Papir, and Ignacio Lopez Moreno. 2020. <span>“Generalized <span class="nocase">End-to-End Loss</span> for <span>Speaker Verification</span>.”</span> <a href="http://arxiv.org/abs/1710.10467">http://arxiv.org/abs/1710.10467</a>.
</div>
<div id="ref-wangDefiningArtificialIntelligence2019" class="csl-entry" role="doc-biblioentry">
Wang, Pei. 2019. <span>“On <span>Defining Artificial Intelligence</span>.”</span> <em>Journal of Artificial General Intelligence</em> 10 (January): 1–37. <a href="https://doi.org/10.2478/jagi-2019-0002">https://doi.org/10.2478/jagi-2019-0002</a>.
</div>
<div id="ref-weibelAAA" class="csl-entry" role="doc-biblioentry">
Weibel, Peter. n.d. <span>“AAA.”</span> <span>www.kunstforum.de</span>. Accessed January 16, 2022. <a href="https://www.kunstforum.de/artikel/aaa/">https://www.kunstforum.de/artikel/aaa/</a>.
</div>
<div id="ref-weizenbaumComputerPowerHuman1976" class="csl-entry" role="doc-biblioentry">
Weizenbaum, Joseph. 1976. <em>Computer Power and Human Reason: From Judgment to Calculation</em>. <span>San Francisco</span>: <span>W. H. Freeman</span>.
</div>
<div id="ref-XTRAMarkHansen" class="csl-entry" role="doc-biblioentry">
<span>“X-<span>TRA</span> → <span>Mark Hansen</span> and <span>Ben Rubin</span>: <span>Listening Post</span>.”</span> n.d. Accessed March 20, 2023. <a href="https://www.x-traonline.org/article/mark-hansen-and-ben-rubin-listening-post">https://www.x-traonline.org/article/mark-hansen-and-ben-rubin-listening-post</a>.
</div>
<div id="ref-zarkadakesOurOwnImage2016" class="csl-entry" role="doc-biblioentry">
Zarkadakēs, Giōrgos. 2016. <em>In Our Own Image: Savior or Destroyer?: The History and Future of Artificial Intelligence</em>. First Pegasus books hardcover edition. <span>New York, NY</span>: <span>Pegasus Books LLC</span>.
</div>
</div>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>The C Programming Language Book defined many standards of programming languages today and how technical descriptions are written. While it focuses on C and the UNIX system, I find this advice from <em>Chapter 1.1 Getting Started</em> particularly interesting “On other systems, the rules will be different; check with a local expert.” as it describes the social necessity of learning computers specifically.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>See <span class="citation" data-cites="InceptionismGoingDeeper2015"><a href="#ref-InceptionismGoingDeeper2015" role="doc-biblioref"><span>“Inceptionism: <span>Going Deeper</span> into <span>Neural Networks</span>”</span></a> (<a href="#ref-InceptionismGoingDeeper2015" role="doc-biblioref">2015</a>)</span>.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p>The initial classifier was conditioned on ImageNet, which contains many images of dog breeds. Therefore DeepDream is biased to generate textures of dog faces.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4" role="doc-endnote"><p>See <span class="citation" data-cites="ImageNet"><a href="#ref-ImageNet" role="doc-biblioref"><span>“<span>ImageNet</span>”</span></a> (<a href="#ref-ImageNet" role="doc-biblioref">n.d.</a>)</span>.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5" role="doc-endnote"><p>See <span class="citation" data-cites="tykaDeepdreamInceptionismRecap"><a href="#ref-tykaDeepdreamInceptionismRecap" role="doc-biblioref">Tyka</a> (<a href="#ref-tykaDeepdreamInceptionismRecap" role="doc-biblioref">n.d.</a>)</span> for a list of projects released within a month after publishing the source code of DeepDream.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6" role="doc-endnote"><p>See <span class="citation" data-cites="molnar10LearnedFeatures"><a href="#ref-molnar10LearnedFeatures" role="doc-biblioref">Molnar</a> (<a href="#ref-molnar10LearnedFeatures" role="doc-biblioref">n.d.</a>)</span> for more examples and explanations in the interpretable machine learning book. He notes that “Feature visualizations give unique insight into the working of neural networks,” but the “visualizations can convey the illusion that we understand what the neural network is doing.”<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7" role="doc-endnote"><p>See <span class="citation" data-cites="baileyDeepDreamCreatorUnveils2018"><a href="#ref-baileyDeepDreamCreatorUnveils2018" role="doc-biblioref">Bailey</a> (<a href="#ref-baileyDeepDreamCreatorUnveils2018" role="doc-biblioref">2018</a>)</span>.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8" role="doc-endnote"><p><span class="citation" data-cites="millerArtistMachineWorld2019"><a href="#ref-millerArtistMachineWorld2019" role="doc-biblioref">Miller</a> (<a href="#ref-millerArtistMachineWorld2019" role="doc-biblioref">2019</a>)</span>, p.122.<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9" role="doc-endnote"><p>Mordvintsev was inspired by a previous paper exploring the generative potential of CNNs: <em>Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps</em> by Simonyan et al. (2014).<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10" role="doc-endnote"><p>See <span class="citation" data-cites="weibelAAA"><a href="#ref-weibelAAA" role="doc-biblioref">Weibel</a> (<a href="#ref-weibelAAA" role="doc-biblioref">n.d.</a>)</span>.<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11" role="doc-endnote"><p>See <span class="citation" data-cites="BetterImagesAI"><a href="#ref-BetterImagesAI" role="doc-biblioref"><span>“Better <span>Images</span> of <span>AI</span>”</span></a> (<a href="#ref-BetterImagesAI" role="doc-biblioref">n.d.</a>)</span> for alternative imagery about AI.<a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn12" role="doc-endnote"><p>See <span class="citation" data-cites="francishungerArtificialIntelligenceAutomated2021"><a href="#ref-francishungerArtificialIntelligenceAutomated2021" role="doc-biblioref">Francis Hunger</a> (<a href="#ref-francishungerArtificialIntelligenceAutomated2021" role="doc-biblioref">2021</a>)</span><a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn13" role="doc-endnote"><p>See <span class="citation" data-cites="hungerTalkUnhypeAI2021"><a href="#ref-hungerTalkUnhypeAI2021" role="doc-biblioref">Hunger</a> (<a href="#ref-hungerTalkUnhypeAI2021" role="doc-biblioref">2021</a>)</span><a href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn14" role="doc-endnote"><p>In behaviorist psychology, Ivan Pavlov’s experiments with dogs are known as ‘classical conditioning.’ Using lever machines, B.F. Skinners experiments on rats and pigeons are called ‘operant conditioning.’ <a href="#fnref14" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn15" role="doc-endnote"><p><span class="citation" data-cites="wangDefiningArtificialIntelligence2019"><a href="#ref-wangDefiningArtificialIntelligence2019" role="doc-biblioref">Wang</a> (<a href="#ref-wangDefiningArtificialIntelligence2019" role="doc-biblioref">2019</a>)</span><a href="#fnref15" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn16" role="doc-endnote"><p>See <span class="citation" data-cites="gadeWhatUbuntuDifferent2012"><a href="#ref-gadeWhatUbuntuDifferent2012" role="doc-biblioref">Gade</a> (<a href="#ref-gadeWhatUbuntuDifferent2012" role="doc-biblioref">2012</a>)</span>. The term ubuntu originates from southern Africa and describes a humanist philosophy in which persons are interconnected. It is often used in phrases such as the Zulu ‘Umuntu ngumuntu ngabantu,’ which translates to ‘A person is a person through other persons.’ The term for personhood (ubuntu) itself is not clearly defined and has different meanings in different areas.<a href="#fnref16" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn17" role="doc-endnote"><ol class="example" type="1">
<li></li>
</ol>
<a href="#fnref17" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn18" role="doc-endnote"><p>In the short story <em>On Having No Head</em> D.E. Harding describes the realization that one can never experience their own head because the most important sensual organs are embedded in it. This could be an explanation why we perceive consciousness or the ‘self’ to be in our heads.<a href="#fnref18" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn19" role="doc-endnote"><p><span class="citation" data-cites="zarkadakesOurOwnImage2016"><a href="#ref-zarkadakesOurOwnImage2016" role="doc-biblioref">Zarkadakēs</a> (<a href="#ref-zarkadakesOurOwnImage2016" role="doc-biblioref">2016</a>)</span>, p.189.<a href="#fnref19" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn20" role="doc-endnote"><p>See <span class="citation" data-cites="bostromAreYouLiving2001"><a href="#ref-bostromAreYouLiving2001" role="doc-biblioref">Bostrom</a> (<a href="#ref-bostromAreYouLiving2001" role="doc-biblioref">2001</a>)</span>. The simulation argument was first proposed by Nick Bostrom. He argues that if we were able to create realistic simulated minds and worlds in the future, it is likely that we already live in a simulation.<a href="#fnref20" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn21" role="doc-endnote"><ol start="2" class="example" type="1">
<li></li>
</ol>
<a href="#fnref21" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn22" role="doc-endnote"><ol start="3" class="example" type="1">
<li></li>
</ol>
<a href="#fnref22" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn23" role="doc-endnote"><p>Signal is an encrypted messaging app and Jitsi Meet is an open source video conferencing software.<a href="#fnref23" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn24" role="doc-endnote"><p>See cwc.radical-openness.org/siliconfriendcamp.<a href="#fnref24" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn25" role="doc-endnote"><p>See <span class="citation" data-cites="Babycastles"><a href="#ref-Babycastles" role="doc-biblioref"><span>“Babycastles”</span></a> (<a href="#ref-Babycastles" role="doc-biblioref">n.d.</a>)</span> which is a NYC based collective fostering and amplifying diverse voices in video game culture.<a href="#fnref25" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn26" role="doc-endnote"><p>The open source forum software <em>Discourse</em> was hosted at the local servus.at data center.<a href="#fnref26" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn27" role="doc-endnote"><p>Bogna Konior, <em>The Dark Forest Theory of the Internet</em>; Rodney Brooks, Intelligence without Representation_; Matteo Pasquinelli, <em>The Arborescent Mind: The Intelligence of an Inverted Tree</em>.<a href="#fnref27" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn28" role="doc-endnote"><p>See <span class="citation" data-cites="CanonCatMac2014"><a href="#ref-CanonCatMac2014" role="doc-biblioref"><span>“The <span>Canon Cat The Mac</span>’s <span>Ancestor</span>”</span></a> (<a href="#ref-CanonCatMac2014" role="doc-biblioref">2014</a>)</span>.<a href="#fnref28" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn29" role="doc-endnote"><p>See <span class="citation" data-cites="RuDALLE"><a href="#ref-RuDALLE" role="doc-biblioref"><span>“<span class="nocase">ruDALL-E</span>”</span></a> (<a href="#ref-RuDALLE" role="doc-biblioref">n.d.</a>)</span>.<a href="#fnref29" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn30" role="doc-endnote"><p>See <span class="citation" data-cites="Avatars"><a href="#ref-Avatars" role="doc-biblioref"><span>“Avatars”</span></a> (<a href="#ref-Avatars" role="doc-biblioref">n.d.</a>)</span>. Together with the artist yang02, So Kanno modified multiple objects that could be remotely controlled from the web.<a href="#fnref30" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn31" role="doc-endnote"><p>See <span class="citation" data-cites="HumanguidedBurritoBots2019"><a href="#ref-HumanguidedBurritoBots2019" role="doc-biblioref"><span>“Human-Guided Burrito Bots Raise Questions about the Future of Robo-Delivery”</span></a> (<a href="#ref-HumanguidedBurritoBots2019" role="doc-biblioref">2019</a>)</span>.<a href="#fnref31" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn32" role="doc-endnote"><p>See <span class="citation" data-cites="EverythingDLVIDOREILLY"><a href="#ref-EverythingDLVIDOREILLY" role="doc-biblioref"><span>“Everything — <span>DΛVID OREILLY</span> • Computer Art &amp; Research”</span></a> (<a href="#ref-EverythingDLVIDOREILLY" role="doc-biblioref">n.d.</a>)</span>.<a href="#fnref32" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn33" role="doc-endnote"><p>See <span class="citation" data-cites="kotowskiLatentSpaceDivination2021"><a href="#ref-kotowskiLatentSpaceDivination2021" role="doc-biblioref">Kotowski</a> (<a href="#ref-kotowskiLatentSpaceDivination2021" role="doc-biblioref">2021</a>)</span>.<a href="#fnref33" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn34" role="doc-endnote"><p>This nonsensical sentence was generated by GPT3, prompting it to write the Open Call for the summer camp.<a href="#fnref34" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn35" role="doc-endnote"><p>“Hephaestus then limped out of the door’; and maidens supported the ruler, golden ones, like living ones, with youthful, charming education: These have understanding in the breast, and speaking voice, Have strength, and also learned art work from the gods.” <span class="citation" data-cites="Ilias"><a href="#ref-Ilias" role="doc-biblioref"><span>“Ilias”</span></a> (<a href="#ref-Ilias" role="doc-biblioref">n.d.</a>)</span>, 18, 417-420.<a href="#fnref35" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn36" role="doc-endnote"><p>Faber’s machine was first presented with a female mask in the USA and later in London under an oriental motif wearing a turban.<a href="#fnref36" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn37" role="doc-endnote"><p>See <span class="citation" data-cites="TalkingHeadInvention"><a href="#ref-TalkingHeadInvention" role="doc-biblioref"><span>“Talking <span>Head</span> | <span>Invention</span> &amp; <span>Technology Magazine</span>”</span></a> (<a href="#ref-TalkingHeadInvention" role="doc-biblioref">n.d.</a>)</span>.<a href="#fnref37" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn38" role="doc-endnote"><p>See <span class="citation" data-cites="markertMichaelMarkertPortfolio"><a href="#ref-markertMichaelMarkertPortfolio" role="doc-biblioref">Markert</a> (<a href="#ref-markertMichaelMarkertPortfolio" role="doc-biblioref">n.d.</a>)</span>.<a href="#fnref38" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn39" role="doc-endnote"><p>See <span class="citation" data-cites="britishmovietoneHumanVoiceMachine2015"><a href="#ref-britishmovietoneHumanVoiceMachine2015" role="doc-biblioref">British Movietone</a> (<a href="#ref-britishmovietoneHumanVoiceMachine2015" role="doc-biblioref">2015</a>)</span> for a demonstration of the Voder.<a href="#fnref39" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn40" role="doc-endnote"><p>A focus on black women who worked as computers for NACA (NASA’s predecessor) can be seen in the film Hidden Figures, 2016.<a href="#fnref40" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn41" role="doc-endnote"><p>The story of the ENIAC programmers is told in the documentary The Computers, 2016.<a href="#fnref41" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn42" role="doc-endnote"><p><span class="citation" data-cites="zarkadakesOurOwnImage2016"><a href="#ref-zarkadakesOurOwnImage2016" role="doc-biblioref">Zarkadakēs</a> (<a href="#ref-zarkadakesOurOwnImage2016" role="doc-biblioref">2016</a>)</span><a href="#fnref42" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn43" role="doc-endnote"><p>See <span class="citation" data-cites="TIMEMagazineCover"><a href="#ref-TIMEMagazineCover" role="doc-biblioref"><span>“<span>TIME Magazine Cover</span>: <span>Tech Comes</span> to <span>Toyland</span> - <span>Dec</span>. 11, 2000 - <span>Science</span> &amp; <span>Technology</span> - <span>Toys</span>”</span></a> (<a href="#ref-TIMEMagazineCover" role="doc-biblioref">n.d.</a>)</span> where the robot puppy is featured on the TIME magazine cover with the title “Tech comes to Toyland.”<a href="#fnref43" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn44" role="doc-endnote"><ol class="example" type="1">
<li></li>
</ol>
<a href="#fnref44" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn45" role="doc-endnote"><p>See <span class="citation" data-cites="FurbySourceCode"><a href="#ref-FurbySourceCode" role="doc-biblioref"><span>“Furby <span>Source Code</span>”</span></a> (<a href="#ref-FurbySourceCode" role="doc-biblioref">n.d.</a>)</span>.<a href="#fnref45" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn46" role="doc-endnote"><ol start="2" class="example" type="1">
<li></li>
</ol>
<a href="#fnref46" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn47" role="doc-endnote"><p>To make the demo work, engineers had to use a prototype Mac that was more powerful than the retail version. It said: “Hello, I am Macintosh. It sure is great to get out of that bag! Unaccustomed as I am to public speaking, I’d like to share with you a maxim I thought of the first time I met an IBM mainframe: Never trust a computer that you can’t lift! Obviously, I can talk, but right now I’d like to sit back and listen. So it is with considerable pride that I introduce a man who has been like a father to me… Steve Jobs!” <span class="citation" data-cites="hertzfeldFolkloreOrgIt1984"><a href="#ref-hertzfeldFolkloreOrgIt1984" role="doc-biblioref">Hertzfeld</a> (<a href="#ref-hertzfeldFolkloreOrgIt1984" role="doc-biblioref">1984</a>)</span><a href="#fnref47" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn48" role="doc-endnote"><p>See <span class="citation" data-cites="shulgin386DXCyberpunkrock"><a href="#ref-shulgin386DXCyberpunkrock" role="doc-biblioref">Shulgin</a> (<a href="#ref-shulgin386DXCyberpunkrock" role="doc-biblioref">n.d.</a>)</span>.<a href="#fnref48" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn49" role="doc-endnote"><p>See <span class="citation" data-cites="parkinsonHeySiriMeet2015"><a href="#ref-parkinsonHeySiriMeet2015" role="doc-biblioref">Parkinson</a> (<a href="#ref-parkinsonHeySiriMeet2015" role="doc-biblioref">2015</a>)</span>.<a href="#fnref49" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn50" role="doc-endnote"><ol start="3" class="example" type="1">
<li></li>
</ol>
<a href="#fnref50" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn51" role="doc-endnote"><p>Lyrebird was acquired by Descript in 2019.<a href="#fnref51" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn52" role="doc-endnote"><ol start="4" class="example" type="1">
<li></li>
</ol>
<a href="#fnref52" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn53" role="doc-endnote"><p>See <span class="citation" data-cites="stuppFraudstersUsedAI2019"><a href="#ref-stuppFraudstersUsedAI2019" role="doc-biblioref">Stupp</a> (<a href="#ref-stuppFraudstersUsedAI2019" role="doc-biblioref">2019</a>)</span>. Fraudsters used a synthetic voice to instruct a bank transfer of 220.000€.<a href="#fnref53" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn54" role="doc-endnote"><p>See <span class="citation" data-cites="coleThisHorrifyingApp2019"><a href="#ref-coleThisHorrifyingApp2019" role="doc-biblioref">Cole</a> (<a href="#ref-coleThisHorrifyingApp2019" role="doc-biblioref">2019</a>)</span>. In 2019, an app named DeepNude got public attention and was taken down by the author shortly after.<a href="#fnref54" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn55" role="doc-endnote"><p>See https://www.youtube.com/channel/UCID5qusrF32kSj-oSGq3rJg where examples of deep fake political comedy can be found on the YouTube Channel Speaking of AI.<a href="#fnref55" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn56" role="doc-endnote"><ol start="5" class="example" type="1">
<li></li>
</ol>
<a href="#fnref56" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn57" role="doc-endnote"><ol start="6" class="example" type="1">
<li></li>
</ol>
<a href="#fnref57" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn58" role="doc-endnote"><ol start="7" class="example" type="1">
<li></li>
</ol>
<a href="#fnref58" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn59" role="doc-endnote"><p>See https://hydra.ojack.xyz.<a href="#fnref59" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn60" role="doc-endnote"><p>As described in the introduction, I use “weighted” instead of “neural.”<a href="#fnref60" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn61" role="doc-endnote"><ol start="8" class="example" type="1">
<li></li>
</ol>
<a href="#fnref61" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn62" role="doc-endnote"><p>See <span class="citation" data-cites="vanhouckeSpeechRecognitionDeep2012"><a href="#ref-vanhouckeSpeechRecognitionDeep2012" role="doc-biblioref">Vanhoucke</a> (<a href="#ref-vanhouckeSpeechRecognitionDeep2012" role="doc-biblioref">2012</a>)</span>.<a href="#fnref62" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn63" role="doc-endnote"><p>See <span class="citation" data-cites="tatmanGenderDialectBias2017"><a href="#ref-tatmanGenderDialectBias2017" role="doc-biblioref">Tatman</a> (<a href="#ref-tatmanGenderDialectBias2017" role="doc-biblioref">2017</a>)</span>]: a study on Gender and Dialect Bias in YouTube’s Automatic Captions that shows a higher error rate for women and non-American speakers. And <span class="citation" data-cites="liuMeasuringFairnessSpeech2021"><a href="#ref-liuMeasuringFairnessSpeech2021" role="doc-biblioref">Liu et al.</a> (<a href="#ref-liuMeasuringFairnessSpeech2021" role="doc-biblioref">2021</a>)</span>: a dataset of people with different skin tones, gender, and age to evaluate speech recognition models for biases. The Facebook AI research team found significant differences in speech recognition between lighter and darker skin tones.<a href="#fnref63" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn64" role="doc-endnote"><ol start="9" class="example" type="1">
<li></li>
</ol>
<a href="#fnref64" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn65" role="doc-endnote"><p>See <span class="citation" data-cites="sampathkumarAmazonEchoCould2017"><a href="#ref-sampathkumarAmazonEchoCould2017" role="doc-biblioref">Sampathkumar</a> (<a href="#ref-sampathkumarAmazonEchoCould2017" role="doc-biblioref">2017</a>)</span> for the first Alexa recording in a court case was used after the defendant agreed to hand over his data.<a href="#fnref65" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn66" role="doc-endnote"><p>See <a href="https://commonvoice.mozilla.org/de">commonvoice.mozilla.org</a>.<a href="#fnref66" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn67" role="doc-endnote"><p>See <a href="https://librivox.org/">librivox.org/</a> and <span class="citation" data-cites="panayotovLibrispeechASRCorpus2015"><a href="#ref-panayotovLibrispeechASRCorpus2015" role="doc-biblioref">Panayotov et al.</a> (<a href="#ref-panayotovLibrispeechASRCorpus2015" role="doc-biblioref">2015</a>)</span>.<a href="#fnref67" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn68" role="doc-endnote"><p>See <span class="citation" data-cites="godfreyjohnj.Switchboard1Release1993"><a href="#ref-godfreyjohnj.Switchboard1Release1993" role="doc-biblioref">Godfrey and Holliman</a> (<a href="#ref-godfreyjohnj.Switchboard1Release1993" role="doc-biblioref">1993</a>)</span>.<a href="#fnref68" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn69" role="doc-endnote"><p>The title “I’d blush if I could” is also the response Siri gives to the insult “You’re a bitch.”<a href="#fnref69" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn70" role="doc-endnote"><p>See <span class="citation" data-cites="lessioMakingSADHome2020"><a href="#ref-lessioMakingSADHome2020" role="doc-biblioref">Lessio</a> (<a href="#ref-lessioMakingSADHome2020" role="doc-biblioref">2020</a>)</span>; and on the portfolio website <a href="https://nadinelessio.com/sadhome/">nadinelessio.com</a>.<a href="#fnref70" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn71" role="doc-endnote"><ol start="10" class="example" type="1">
<li></li>
</ol>
<a href="#fnref71" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn72" role="doc-endnote"><p>10, p.117.<a href="#fnref72" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn73" role="doc-endnote"><p>See <span class="citation" data-cites="ExploringPossibilitySpace"><a href="#ref-ExploringPossibilitySpace" role="doc-biblioref"><span>“Exploring <span>Possibility Space</span>: <span>Microsoft</span> #<span>TAYFAIL Smoking Gun</span>: <span>ALICE Open Source AI Library</span> and <span>AIML</span>”</span></a> (<a href="#ref-ExploringPossibilitySpace" role="doc-biblioref">n.d.</a>)</span>. Tay was the name of a chatbot Microsoft intended to have a teenage personality and could be interacted with over multiple channels. Twitter trolls co-opted the bot with sexist, racist, and antisemitic questions that the bot replied to with generic answers. They also used a repeat-after-me phrase to make it look like the bot was spewing hateful comments. After only 16 hours, Microsoft deleted all accounts, and the PR disaster was immense, with headlines happily personifying the AI that has learned to be racist. But no continuous learning algorithm was involved, as a blog post by Russel Cameron Thomas in which he explains the technology stack behind Tay.<a href="#fnref73" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn74" role="doc-endnote"><ol start="11" class="example" type="1">
<li></li>
</ol>
<a href="#fnref74" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn75" role="doc-endnote"><p>The Oracle was a terminal computer connected to GPT-3 with a generic Q&amp;A prompt.<a href="#fnref75" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn76" role="doc-endnote"><p>See <span class="citation" data-cites="jemineMasterThesisAutomatic"><a href="#ref-jemineMasterThesisAutomatic" role="doc-biblioref">C. Jemine</a> (<a href="#ref-jemineMasterThesisAutomatic" role="doc-biblioref">n.d.</a>)</span>. And the corresponding GitHub repository <span class="citation" data-cites="jemineRealTimeVoiceCloning2022"><a href="#ref-jemineRealTimeVoiceCloning2022" role="doc-biblioref">Corentin Jemine</a> (<a href="#ref-jemineRealTimeVoiceCloning2022" role="doc-biblioref">[2019] 2022</a>)</span>.<a href="#fnref76" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn77" role="doc-endnote"><ol start="12" class="example" type="1">
<li></li>
</ol>
<a href="#fnref77" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn78" role="doc-endnote"><p>The SV2TTS pipeline is based on these previous papers: <span class="citation" data-cites="wanGeneralizedEndtoEndLoss2020"><a href="#ref-wanGeneralizedEndtoEndLoss2020" role="doc-biblioref">Wan et al.</a> (<a href="#ref-wanGeneralizedEndtoEndLoss2020" role="doc-biblioref">2020</a>)</span>, 7, and 3.<a href="#fnref78" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn79" role="doc-endnote"><ol start="13" class="example" type="1">
<li></li>
</ol>
<a href="#fnref79" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn80" role="doc-endnote"><p>See https://hydra.ojack.xyz.<a href="#fnref80" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn81" role="doc-endnote"><ol class="example" type="1">
<li></li>
</ol>
<a href="#fnref81" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn82" role="doc-endnote"><p>The Mark I was an electromechanical machine that used motor-driven potentiometers to adjust the variable weights.<a href="#fnref82" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn83" role="doc-endnote"><p>Frank Rosenblatt died in a boating accident in 1971. A couple of years prior, Marvin Minsky heavily criticized the mathematics behind perceptrons and advocated for a symbolic approach. These events might have led to a lack of funding in the ‘connectionist’ AI research field and ultimately to a general disinterest when the symbolic approach could not keep its exaggerated promises.<a href="#fnref83" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn84" role="doc-endnote"><p><span class="citation" data-cites="mitchellArtificialIntelligenceGuide2019"><a href="#ref-mitchellArtificialIntelligenceGuide2019" role="doc-biblioref">Mitchell</a> (<a href="#ref-mitchellArtificialIntelligenceGuide2019" role="doc-biblioref">2019</a>)</span>, p. 114.<a href="#fnref84" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn85" role="doc-endnote"><ol start="2" class="example" type="1">
<li></li>
</ol>
<a href="#fnref85" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn86" role="doc-endnote"><ol start="3" class="example" type="1">
<li></li>
</ol>
<a href="#fnref86" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn87" role="doc-endnote"><ol start="4" class="example" type="1">
<li></li>
</ol>
<a href="#fnref87" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn88" role="doc-endnote"><ol start="5" class="example" type="1">
<li></li>
</ol>
<a href="#fnref88" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn89" role="doc-endnote"><p>“One epoch takes 35 GPU minutes but more than 35 CPU hours.” <span class="citation" data-cites="ciresanFlexibleHighPerformance"><a href="#ref-ciresanFlexibleHighPerformance" role="doc-biblioref">Ciresan et al.</a> (<a href="#ref-ciresanFlexibleHighPerformance" role="doc-biblioref">n.d.</a>)</span>.<a href="#fnref89" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn90" role="doc-endnote"><p><span class="citation" data-cites="afifi11KHandsGender2018"><a href="#ref-afifi11KHandsGender2018" role="doc-biblioref">Afifi</a> (<a href="#ref-afifi11KHandsGender2018" role="doc-biblioref">2018</a>)</span>; data and source code are available at <span class="citation" data-cites="afifi11kHands"><a href="#ref-afifi11kHands" role="doc-biblioref">Afifi</a> (<a href="#ref-afifi11kHands" role="doc-biblioref">n.d.</a>)</span>.<a href="#fnref90" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn91" role="doc-endnote"><p>See <span class="citation" data-cites="dempsey-jonesNeuroscientistsPutDubious2018"><a href="#ref-dempsey-jonesNeuroscientistsPutDubious2018" role="doc-biblioref">Dempsey-Jones</a> (<a href="#ref-dempsey-jonesNeuroscientistsPutDubious2018" role="doc-biblioref">2018</a>)</span>. In 2018, Parker et al. jokingly tested the Galls theory using 21st-century scientific methods and MRI data.<a href="#fnref91" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn92" role="doc-endnote"><p>I didn’t test the system as I don’t know how to run MatLab code, but I can imagine that the slightly better results on the dorsal hand result from nail polish only being applied on female hands. <a href="#fnref92" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn93" role="doc-endnote"><ol start="6" class="example" type="1">
<li></li>
</ol>
<a href="#fnref93" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn94" role="doc-endnote"><p>See <span class="citation" data-cites="matthiasschafer11kHands2018"><a href="#ref-matthiasschafer11kHands2018" role="doc-biblioref">Matthias Schäfer</a> (<a href="#ref-matthiasschafer11kHands2018" role="doc-biblioref">2018</a>)</span>.<a href="#fnref94" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn95" role="doc-endnote"><ol start="7" class="example" type="1">
<li></li>
</ol>
<a href="#fnref95" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn96" role="doc-endnote"><p>See <span class="citation" data-cites="murphyWhyStanfordResearchers2017"><a href="#ref-murphyWhyStanfordResearchers2017" role="doc-biblioref">Murphy</a> (<a href="#ref-murphyWhyStanfordResearchers2017" role="doc-biblioref">2017</a>)</span>. A particularly famous example by Michael Kosinski and Yiluna Wang. Their flawed study tried to predict if a person is gay by scraping dating sites and training a classifier on these images.<a href="#fnref96" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn97" role="doc-endnote"><p>See <span class="citation" data-cites="dengImageNetLargeScaleHierarchical2009"><a href="#ref-dengImageNetLargeScaleHierarchical2009" role="doc-biblioref">Deng et al.</a> (<a href="#ref-dengImageNetLargeScaleHierarchical2009" role="doc-biblioref">2009</a>)</span>. ImageNet started with 3,2 million images and aimed to collect 50 million by the end of 2011.<a href="#fnref97" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn98" role="doc-endnote"><ol start="8" class="example" type="1">
<li></li>
</ol>
<a href="#fnref98" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn99" role="doc-endnote"><ol start="9" class="example" type="1">
<li></li>
</ol>
<a href="#fnref99" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn100" role="doc-endnote"><p>See <span class="citation" data-cites="TechnicallyResponsibleKnowledge"><a href="#ref-TechnicallyResponsibleKnowledge" role="doc-biblioref"><span>“Technically <span>Responsible Knowledge</span>”</span></a> (<a href="#ref-TechnicallyResponsibleKnowledge" role="doc-biblioref">n.d.</a>)</span>.<a href="#fnref100" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn101" role="doc-endnote"><p>See <span class="citation" data-cites="KATECRAWFORDTREVOR"><a href="#ref-KATECRAWFORDTREVOR" role="doc-biblioref"><span>“<span>KATE CRAWFORD</span> | <span>TREVOR PAGLEN</span>: <span>TRAINING HUMANS</span> – <span>Fondazione Prada</span>”</span></a> (<a href="#ref-KATECRAWFORDTREVOR" role="doc-biblioref">n.d.</a>)</span>.<a href="#fnref101" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn102" role="doc-endnote"><ol start="10" class="example" type="1">
<li></li>
</ol>
<a href="#fnref102" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn103" role="doc-endnote"><p>See <span class="citation" data-cites="ImageNetRouletteTrevor"><a href="#ref-ImageNetRouletteTrevor" role="doc-biblioref"><span>“<span>ImageNet Roulette</span> – <span>Trevor Paglen</span>”</span></a> (<a href="#ref-ImageNetRouletteTrevor" role="doc-biblioref">n.d.</a>)</span>.<a href="#fnref103" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn104" role="doc-endnote"><ol start="11" class="example" type="1">
<li></li>
</ol>
<a href="#fnref104" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn105" role="doc-endnote"><p>See <span class="citation" data-cites="harveyExposingAi"><a href="#ref-harveyExposingAi" role="doc-biblioref">Harvey</a> (<a href="#ref-harveyExposingAi" role="doc-biblioref">n.d.</a>)</span>.<a href="#fnref105" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn106" role="doc-endnote"><ol start="12" class="example" type="1">
<li></li>
</ol>
<a href="#fnref106" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn107" role="doc-endnote"><p>See 13 for a more extensive analysis on the exploitation of CC media.<a href="#fnref107" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn108" role="doc-endnote"><ol start="13" class="example" type="1">
<li></li>
</ol>
<a href="#fnref108" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn109" role="doc-endnote"><p>YFCC100M only contains links and metadata to images and videos under CC license on Flickr.<a href="#fnref109" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn110" role="doc-endnote"><ol start="14" class="example" type="1">
<li></li>
</ol>
<a href="#fnref110" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn111" role="doc-endnote"><ol start="15" class="example" type="1">
<li></li>
</ol>
<a href="#fnref111" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn112" role="doc-endnote"><p>The Open Future Foundation is a think tank that actively tries to influence European digital policy debates. See <span class="citation" data-cites="OpenFutureOpen"><a href="#ref-OpenFutureOpen" role="doc-biblioref"><span>“Open <span>Future</span> – <span>Open Future Foundation</span>”</span></a> (<a href="#ref-OpenFutureOpen" role="doc-biblioref">n.d.</a>)</span>.<a href="#fnref112" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn113" role="doc-endnote"><ol start="16" class="example" type="1">
<li></li>
</ol>
<a href="#fnref113" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn114" role="doc-endnote"><p>See <span class="citation" data-cites="ThisPersonDoes2019"><a href="#ref-ThisPersonDoes2019" role="doc-biblioref"><span>“This <span>Person Does Not Exist</span>”</span></a> (<a href="#ref-ThisPersonDoes2019" role="doc-biblioref">2019</a>)</span>. At the time of this writing, the URL redirects to stability.ai, but there is an archived version.<a href="#fnref114" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn115" role="doc-endnote"><p>See 19.<a href="#fnref115" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn116" role="doc-endnote"><ol start="17" class="example" type="1">
<li></li>
</ol>
<a href="#fnref116" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn117" role="doc-endnote"><p>See ‘FFHQ Dataset’ at 13.<a href="#fnref117" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn118" role="doc-endnote"><ol start="18" class="example" type="1">
<li></li>
</ol>
<a href="#fnref118" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn119" role="doc-endnote"><ol start="19" class="example" type="1">
<li></li>
</ol>
<a href="#fnref119" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn120" role="doc-endnote"><ol start="20" class="example" type="1">
<li></li>
</ol>
<a href="#fnref120" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn121" role="doc-endnote"><p>See <span class="citation" data-cites="XTRAMarkHansen"><a href="#ref-XTRAMarkHansen" role="doc-biblioref"><span>“X-<span>TRA</span> → <span>Mark Hansen</span> and <span>Ben Rubin</span>: <span>Listening Post</span>”</span></a> (<a href="#ref-XTRAMarkHansen" role="doc-biblioref">n.d.</a>)</span> and <span class="citation" data-cites="ArsElectronicaArchiv"><a href="#ref-ArsElectronicaArchiv" role="doc-biblioref"><span>“Ars <span>Electronica Archiv</span>”</span></a> (<a href="#ref-ArsElectronicaArchiv" role="doc-biblioref">n.d.</a>)</span>.<a href="#fnref121" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn122" role="doc-endnote"><ol start="21" class="example" type="1">
<li></li>
</ol>
<a href="#fnref122" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn123" role="doc-endnote"><ol start="22" class="example" type="1">
<li></li>
</ol>
<a href="#fnref123" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn124" role="doc-endnote"><p>See <span class="citation" data-cites="schaferMissingPictures"><a href="#ref-schaferMissingPictures" role="doc-biblioref">Schäfer</a> (<a href="#ref-schaferMissingPictures" role="doc-biblioref">n.d.</a>)</span>.<a href="#fnref124" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn125" role="doc-endnote"><p>See <span class="citation" data-cites="NutzungsbedingungenInstagramHilfebereich"><a href="#ref-NutzungsbedingungenInstagramHilfebereich" role="doc-biblioref"><span>“Nutzungsbedingungen | <span>Instagram-Hilfebereich</span>”</span></a> (<a href="#ref-NutzungsbedingungenInstagramHilfebereich" role="doc-biblioref">n.d.</a>)</span>.<a href="#fnref125" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn126" role="doc-endnote"><p>Complete list of artists can be found here <a href="https://doggg.art/artistlist.html">doggg.art/artistlist.html</a>.<a href="#fnref126" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn127" role="doc-endnote"><p>See <span class="citation" data-cites="chakelianJourneyManyFaces2021"><a href="#ref-chakelianJourneyManyFaces2021" role="doc-biblioref">Chakelian</a> (<a href="#ref-chakelianJourneyManyFaces2021" role="doc-biblioref">2021</a>)</span>.<a href="#fnref127" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn128" role="doc-endnote"><p>See <span class="citation" data-cites="InternetYamiIchi"><a href="#ref-InternetYamiIchi" role="doc-biblioref"><span>“The Internet Yami-Ichi”</span></a> (<a href="#ref-InternetYamiIchi" role="doc-biblioref">n.d.</a>)</span>.<a href="#fnref128" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn129" role="doc-endnote"><p>See <span class="citation" data-cites="karrasAnalyzingImprovingImage2020"><a href="#ref-karrasAnalyzingImprovingImage2020" role="doc-biblioref">Karras et al.</a> (<a href="#ref-karrasAnalyzingImprovingImage2020" role="doc-biblioref">2020</a>)</span>.<a href="#fnref129" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn130" role="doc-endnote"><ol class="example" type="1">
<li></li>
</ol>
<a href="#fnref130" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn131" role="doc-endnote"><ol start="2" class="example" type="1">
<li></li>
</ol>
<a href="#fnref131" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn132" role="doc-endnote"><p><span class="citation" data-cites="hofstadterFluidConceptsCreative1995"><a href="#ref-hofstadterFluidConceptsCreative1995" role="doc-biblioref">Hofstadter</a> (<a href="#ref-hofstadterFluidConceptsCreative1995" role="doc-biblioref">1995</a>)</span>, p. 157.<a href="#fnref132" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn133" role="doc-endnote"><p>See <span class="citation" data-cites="JabberwackyThoughtsArtificial2006"><a href="#ref-JabberwackyThoughtsArtificial2006" role="doc-biblioref"><span>“Jabberwacky - <span>About Thoughts</span> - <span>An Artificial Intelligence AI</span> Chatbot, Chatterbot or Chatterbox, Learning <span>AI</span>, Database, Dynamic - Models Way Humans Learn - Simulate Natural Human Chat - Interesting, Humorous, Entertaining”</span></a> (<a href="#ref-JabberwackyThoughtsArtificial2006" role="doc-biblioref">2006</a>)</span> &amp; <span class="citation" data-cites="Cleverbot"><a href="#ref-Cleverbot" role="doc-biblioref"><span>“Cleverbot”</span></a> (<a href="#ref-Cleverbot" role="doc-biblioref">n.d.</a>)</span>.<a href="#fnref133" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn134" role="doc-endnote"><ol start="3" class="example" type="1">
<li></li>
</ol>
<a href="#fnref134" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn135" role="doc-endnote"><p>In the original paper “Computing Machinery and Intelligence” from 1950 Alan Turing was more specific and actually proposed for the computer to imitate the gendered role of a woman.<a href="#fnref135" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn136" role="doc-endnote"><ol start="4" class="example" type="1">
<li></li>
</ol>
<a href="#fnref136" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn137" role="doc-endnote"><p>OpenAI has since created a capped profit subsidiary based on investments by large corporations such as Microsoft.<a href="#fnref137" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn138" role="doc-endnote"><p>See <span class="citation" data-cites="bandyAddressingDocumentationDebt"><a href="#ref-bandyAddressingDocumentationDebt" role="doc-biblioref">Bandy and Vincent</a> (<a href="#ref-bandyAddressingDocumentationDebt" role="doc-biblioref">n.d.</a>)</span>.<a href="#fnref138" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn139" role="doc-endnote"><ol start="5" class="example" type="1">
<li></li>
</ol>
<a href="#fnref139" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn140" role="doc-endnote"><ol start="6" class="example" type="1">
<li></li>
</ol>
<a href="#fnref140" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn141" role="doc-endnote"><ol start="7" class="example" type="1">
<li></li>
</ol>
<a href="#fnref141" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn142" role="doc-endnote"><ol start="8" class="example" type="1">
<li></li>
</ol>
<a href="#fnref142" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn143" role="doc-endnote"><ol start="9" class="example" type="1">
<li></li>
</ol>
<a href="#fnref143" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn144" role="doc-endnote"><ol start="10" class="example" type="1">
<li></li>
</ol>
<a href="#fnref144" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn145" role="doc-endnote"><ol start="11" class="example" type="1">
<li></li>
</ol>
<a href="#fnref145" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn146" role="doc-endnote"><p>11 p. 615.<a href="#fnref146" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn147" role="doc-endnote"><ol start="12" class="example" type="1">
<li></li>
</ol>
<a href="#fnref147" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn148" role="doc-endnote"><ol start="13" class="example" type="1">
<li></li>
</ol>
<a href="#fnref148" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn149" role="doc-endnote"><ol start="14" class="example" type="1">
<li></li>
</ol>
<a href="#fnref149" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn150" role="doc-endnote"><p>The computer uses lower-end gaming hardware with an Nvidia GTX 1660 Super GPU that has 6GB VRAM. Enough to run the medium sized models of GPT-2.<a href="#fnref150" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn151" role="doc-endnote"><ol start="15" class="example" type="1">
<li></li>
</ol>
<a href="#fnref151" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn152" role="doc-endnote"><p><span class="citation" data-cites="weizenbaumComputerPowerHuman1976"><a href="#ref-weizenbaumComputerPowerHuman1976" role="doc-biblioref">Weizenbaum</a> (<a href="#ref-weizenbaumComputerPowerHuman1976" role="doc-biblioref">1976</a>)</span>, p. 241.<a href="#fnref152" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn153" role="doc-endnote"><p>GPT uses this token to give the model a reference when a piece of text is finished and a new one begins. In this thesis, I selectively used the models GPT-3 and GPT-3.5 (ChatGPT) to rephrase parts of my text, find synonyms, get inspiration when I’m stuck or correct my writing. It often created very uninspiring and generic output, but sometimes I could find precisely the right phrase or word. It also helped me validate some of the technical language I struggled with. As I am also using spellcheckers and search engines, I do not consider this unethical or plagiarism, but I recognize that I might have accidentally quoted someone without attribution by using this technology.<a href="#fnref153" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
