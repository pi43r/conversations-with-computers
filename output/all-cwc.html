<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Conversations with Computers</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      word-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
    }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  
</head>
<body>
<header id="title-block-header">
<h1 class="title">Conversations with Computers</h1>
</header>
<h1 id="hello-world">Hello, World!</h1>
<p>The common first letters a programmer types into its digital computing machine is some form of ‘hello world’ along with braces, punctuation and words that the machine should interpret. In my favorite language JavaScript it can look as simple as this <code>console.log("Hello, World!")</code>. After executing this short string of text in the browser, the computer will set of a series of events and likewise respond with ‘Hello, World!’ in the developer console.</p>
<p>This ritual of greeting each other (and the world) is accredited to Brian W. Kernighan, who wrote the popular guide <em>The C Programming Language</em><a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> in 1978, where he proclaims that the only way to learn a programming language is by writing programs in it.</p>
<p>I start this introduction with the same greeting and as I am pressing these letters on my keyboard they simultaneously appear on the screen in front of me. This translation from the tactical key press to the pixels on the screen becomes possible through a chain of electrical signals, that in a matter of milliseconds reach my retina and close the loop. Thousands of people have created these intricate systems which interoperate and depend on each other, each new layer abstracting underlying operations further. Just the text editor I am using to write these lines has hundreds of contributors writing thousands of lines of code, which in turn rely on the functions of the operating system created by kernel developers.</p>
<p>This chain of creativity represents the core of open source software development, which is based on collaboration and sharing knowledge. Every contribution, no matter how small or insignificant it may seem, is valuable in creating something greater than what was there before. Whether it is a bug fix, a feature request or a new idea - every contribution makes a difference.</p>
<p>In this thesis I focus on the known and unknown human connections we make with and through computers, specifically in the realm of machine learning. My hypothesis is that the current trend of framing ‘Artificial Intelligence’ as autonomous computational beings, would be better described as ‘Co-Intelligence,’ where the programming of such systems is a collaborative effort with many hidden actors. While I address some technical details of current machine learning systems, I will put them into social context, because software inherently is a social tool. To describe the social inheritance I use my own and other artists artworks to gain a broader aesthetic perspective, which can go beyond logic and language.</p>
<p>Even though I focus on the social nature of computing and I am influenced by many of my peers, I am describing these topics from my subjective perspective. I don’t claim any objective truths on the descriptions of the collaborative works I have done and am naturally biased towards my own contribution to it. As this thesis is written in the context of an art program, I tend to deviate from rigorous scientific methods and describe my observations more freely. Instead of trying to answer the question of the nature of AI as subjective entities, I focus on the practices we developed through the engagement with that question.</p>
<h2 id="dreaming">Dreaming</h2>
<figure>
<img src="6ocuQsZ.jpeg" alt="“PuppySlug” - posted by deleted user on /r/creepy with the title “This image was generated by a computer on its own (from a friend working on AI)” on January 10th 2015. 7 days before the Google blog post" /><figcaption aria-hidden="true">“PuppySlug” - posted by deleted user on /r/creepy with the title “This image was generated by a computer on its own (from a friend working on AI)” on January 10th 2015. 7 days before the Google blog post</figcaption>
</figure>
<p>My journey into the hype machine of ‘Artificial Intelligence’ started around 2015, when Google released DeepDream<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> on it’s research blog. The developers Alexander Mordvintsev, Michael Tyka and Christopher Olah were initially trying to peek inside of an ‘Artificial Neural Network’ that was ‘trained’ for image classification tasks. They reversed the task of the classifier and instead of identifying what a set of pixels look like the pixel values were changed to look more similar to what the network has ‘learned.’ This created images that amplified the textures embedded in their networks transforming the input image into swirls of PuppySlugs.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> This imagery that reminded people of bad dreams and hallucinogenic trips was widely shared on the internet which skyrocketed the awareness that ‘Deep Learning’ and biologically inspired computing has come back from it’s hibernation during the AI winter. In 2012 ‘Artificial Neural Networks’ have made an incredible comeback into the field of computer vision. A team around Fei-Fei Li created ImageNet,<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> a dataset of images organized into categories by anonymous workers from the internet. The ImageNet project held a yearly competition since 2010 and for the first two years the best algorithm was recognizing these images with a 74% accuracy until a ‘Artificial Neural Network’ topped the score with 85% accuracy and woke up the computer vision community to their potential powers. With DeepDream this technique became tangible for the general public as it created something to look at, something for humans to empathize with the outputs of an abstract mathematical model. This tendency to anthropomorphize this type of software becomes already clear in the way we use language to describe it. We talk of ‘Neural Networks’ as if they are biological entities, that can be ‘taught’ and which ‘learn,’ ‘experience’ and ‘read’ data from the world. This was a turning point that got me and many other artists interested in ‘collaborating’ with seemingly autonomous machines. Machines that we do not have to understand through mathematics but through interpretation and experimentation of ‘their dreams.’ Yet, the outputs of DeepDream quickly became kitsch. Google shared the code with an open source license and many people created apps and APIs to generate PuppySlug textures on top of their own images.<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> As with most other memes on the internet the hype around DeepDream died quickly and by the end of 2015 the world was already saturated with computer generated hallucinogenic images. Other researchers have since picked up<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> on visualizing nodes in ‘Artificial Neural Networks’ and Alex Mordvintsev—together with his wife—has since become an artist, exhibiting in art fairs.<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> What stayed was the notion that ‘Artificial Intelligence’ became some form of computational being different from the humans who wrote the code, labeled and sorted the data.</p>
<h2 id="terms">Terms</h2>
<p>I have been deliberately using quotation marks around some of the previous words, as I am critical of the language used when talking of AI systems. In his book <em>The Artist in the Machine</em> Arthur I. Miller comes to the conclusion that machines could in fact be seen as creative and will be considered “artists, writers, and musicians in their own right.”<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> He is using a typical techno-utopian argument that the technology is not quite there yet to <em>really</em> be creative, but that it will change in the foreseeable future. At the same time Miller is explaining in great detail the actual creative work of Mordvintsev and his human experience of insomnia to come up<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a> with the generative system for DeepDream and how he shared it with his peers at Google. Alex Mordvintsev was not collaborating with his computer, he created an emergent algorithmic system with it. The software does not hang pictures on exhibition walls, talk to gallerists and has no agency in the process if it were to generate pictures or not. Peter Weibel has formulated it bluntly: Artificial intelligence does not exist. But an ensemble of machines, media, programs, algorithms, hardware, and software has resulted in an extraordinarily large, diverse, and productive field of research that is called AI.<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a> Where Arthur Miller is feeding the narrative of humanoid robots with glowing blue brains<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a>, most computer scientists in the field of artificial intelligence today are working in the subdiscipline of ‘Machine Learning.’ In the last 10 years the terms have converged in the media landscape, but where intelligence is rejected as a suitcase word with a multiplicity of meanings, ‘Machine Learning’ tries to define the task more narrowly to some form of pattern recognition and extrapolation of existing data. They are using a variety of computational and statistical techniques to form abstract models for domain specific problems. In these fields ‘Artificial Intelligence’ is a buzzword to convince governments and venture capitalists to fund projects. The artist and researcher Francis Hunger has recently shared a list of alternative terms in an attempt to dehumanize our language for AI systems:</p>
<ol type="1">
<li>‘Artificial Intelligence’ =&gt; ’ Automated Pattern Recognition’</li>
<li>‘Machine Learning’ =&gt; ‘Machine Conditioning’ OR ‘Automated Classification’</li>
<li>‘Neural Network’ =&gt; ‘Weighted Network’</li>
<li>‘Deep Learning’ =&gt; ‘Deep Conditioning’</li>
<li>‘Neuron’ =&gt; ‘Weight’ or ‘Node’<br />
<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a></li>
</ol>
<p>In an accompanying talk<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a> he explains the aim of those terms is to invoke passivity, that we deal with machines and humans set those machines in motion, even when in the end we form ‘human-machine assemblages.’ I like many of the proposed terms, even though they are still closely associated with biological operations. The conditioning of machines, for example, reminds me of Pavlov’s dog experiments or B.F. Skinner’s box to modify pigeon behavior by reinforcement or punishment.<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a> This analogy serves the purpose well of creating an image of the machine as a system that can be controlled by changing the parameters of it’s virtual environment. Therefor I will use ‘conditioning’ and ‘weighted networks,’ where it is applicable. Using ‘Automated Pattern Recognition’ instead of ‘Artificial Intelligence,’ however, becomes too narrow of a definition and is counter intuitive to me, as AI serves exactly the function of being ill-defined. Pei Wang has made a great effort in defining the different strands of AI research and their working definitions.<a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a> He clusters them into Structure-AI (recreating the human brain), Behavior-AI (recreating human actions), Capability-AI (domain problem solving), Function-AI (developing cognitive functions) and Principle-AI (finding underlying principles) and comes up with his own working definition to where AI research should be headed and how it can be unified. My initial goal for this thesis was to completely avoid the term ‘Artificial Intelligence,’ but as I have already failed in that task and coming up with a less anthropomorphic term does not seem feasible to convey the research in the field and it’s media reception, therefore I will keep using the abbreviation AI. Contrary to articles and sci-fi novels I will not use the personification of ‘an AI,’ but will rather talk of AI systems, meaning emergent complex programs. Many of the systems today use statistical modeling, yet nobody would ask the question if statistics can be creative, which is why I decided not to engage with philosophical questions of consciousness and creativity. Instead I want to explore how AI systems can include the human knowledge and work that goes into building them, which is challenging the AI ideology of machine autonomy and proposes human-centric goals, rather than building a machine as a goal in itself.</p>
<h2 id="getting-started">Getting Started</h2>
<p>[[Revision Needed]]</p>
<p>I structured this thesis around 4 chapters, which combine historical, computational and collective knowledge. Starting with a history of talking machines from Wolfgang von Kempelen’s speech automatons to digital assistants today, not forgetting that the first computers were women doing calculation. The literal act of speaking to computers sets the base of defining that we are actually talking through computers with other humans. The second chapter deals with the building blocks for complex statistical modeling in AI systems. To make such systems possible, researchers need to create large datasets, often using ethically questionable techniques aggregating data from internet users. I will look closely into the StyleGAN dataset, as it serves a dual purpose, because the model defined another turning point for artists to generate synthetic media. The same is true for GPT-2, a model that can generate coherent looking text based on large amounts of scraped websites and books. In the third chapter I will explore the transformer architecture and how artists are using it to make (non-)sense automatic writing. Lastly I am revisiting collective experiences that I have been organizing with other artists. I had the great pleasure to work with the net culture initiative servus.at to organize the <em>Silicon Friend Camp</em> in the Austrian mountains, where we invited 17 artists and researchers for a week long retreat, focusing on human computer conversations. The camp resulted in the exhibition <em>Camping with Computers</em> and an online symposium on <em>Conversations with Computers</em>.</p>
<p><strong>Bitte hier erwähnen, dass Arbeiten von anderen Kunstschaffenden in die verschiedenen Kapitel theoretisch eingewebt wurden, da sie dort beispielhaft deine thesen untermauern…</strong></p>
<p><strong>Außerdem bitte hier abschließend erwähnen, dass in dieser theoretischen Masterarbeit nur jene Arbeiten von dir präsentiert werden, die direkt mit diesem Inhalt in Verbindung stehen. Alle weiteren Arbeiten, die während deiner Zeit an der Kunstuni entstanden, bitte kurz beschrieben in den Anhang stellen… Danke!</strong></p>
<hr />
<h1 id="co-intelligence"><strong>Co-Intelligence</strong></h1>
<blockquote>
<p>Ubuntu: I am, because you are<a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a></p>
</blockquote>
<p>The cognitive scientist Abeba Birhane takes the South African humanist philosophy of <em>Ubuntu</em> as an argument against a western tradition of the <em>self</em> as autonomous beings.<a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a> According to <em>Ubuntu</em> a person becomes a person, only through the relationships with others. The personality is shaped by the interactions with each other and is fluidly shifting between different states. I am a different person, when talking to my mother, my friends and strangers on the street. At the same time, my upbringing and cultural influences define my own self-image. Birhane traces back the flaws in western theories of the mind to René Descartes ‘cogito ergo sum’ – I think, therefore I am. In his famous meditation the French philosopher tried to strip back all things of uncertainty to arrive at the foundation of inner thought as proof of existence. In turn this individualistic ‘I’ and the method of logical reasoning have become a successful story in western sciences. The idea of a separate mind and body was already present in Plato’s dualism, and the scientific method of breaking down complex natural phenomenons into simplified quantifiable parts is at least as old. In the past 200 years scientists, philosophers and theologists defined the ‘body-mind problem,’ coming up with theories of how the immaterial ‘soul’ interacts with the fleshy human body: Why are we conscious of the world around us, ourselves and others?</p>
<p>We concluded that the brain must do the thinking<a href="#fn18" class="footnote-ref" id="fnref18" role="doc-noteref"><sup>18</sup></a> and using deductive reasoning the brain was separated into smaller parts until synapses and neurons were defined and described. But the physical functioning of the brain does not explain the emotional experience of the self. The groundwork was done by the mathematician Gottfried Wilhelm Leibniz, who proposed the existence of fundamental ‘monads’ that transcend all matter in the universe, which function as an intermediary between mind and matter. Sir John Eccles applied this theory to the model of synaptic actions that he developed through experimentation with electricity and the brain, he called the mysterious forces through which we control the brain ‘psychons.’ But neither psychons nor monads served as a sufficient explanation for the bridge between mind and matter.</p>
<p>Dualism continues to have a major impact on the way we think about artificial intelligence and consciousness. For example, the influential cyberneticists Norbert Wiener and Claude Shannon both based their work on the assumption that information is a separate entity from matter and energy. This assumption has led to a disembodiment of information, and a false separation of the physical world (the ‘hardware’) from the world of ideas (the ‘software’).<a href="#fn19" class="footnote-ref" id="fnref19" role="doc-noteref"><sup>19</sup></a> Following a materialist logic, consciousness becomes information embedded in the physical substrate of the brain. In that sense, we only need to measure all physical activities to duplicate and simulate the experience of being human. The metaphor of the brain as a computer defines our present and justifies predictions of ‘downloading’ consciousness and achieving digital immortality. It culminates in the popular simulation hypothesis that proposes we already live in a simulated environment.<a href="#fn20" class="footnote-ref" id="fnref20" role="doc-noteref"><sup>20</sup></a> Computational neuroscientists use and develop instruments to quantify electric impulses in the brain and create massive amounts of experimental data, which in turn need statistical models and ever growing computing power to be analyzed. On the other spectrum, simplified models of the brain in the form of artificial ‘neural networks’ are used to compute and sort higher order abstractions. This focus on the individual brain in the search to replicate intelligence leaves aside a physical and social environment, in which conscious beings are embodied.</p>
<p>In order to understand human cognition and intelligence, we need to explore the complex dialogic connections between human actors. Current machine learning programs abstract intelligence into simple task solving mechanisms and make us believe that the underlying algorithm becomes a rational agent. But on closer examination the program reveals an astonishing amount of human collaboration: from the creation of programming languages as protocols for interaction to the enormous amounts of data aggregation and labeling. Instead of looking at AI systems and seeing them as a subjective entity we can also see them as artifacts of collective cognition. By embedding current machine learning algorithms back into the social structure I hope to gain experimental insight on how these new forms of collective computations are used as a creative medium. In the following I will present my experiences working within Co-Intelligent systems in different constellations.</p>
<h2 id="metaconversations-with-computers"><strong>Metaconversations with Computers</strong></h2>
<figure>
<img src="Flowchart---Metaconversation-Piece-stitch.jpg.png" alt="Metaconversations, Miro Flowchart" /><figcaption aria-hidden="true">Metaconversations, Miro Flowchart</figcaption>
</figure>
<p>In October 2020 I participated in <em>Freeport 1: Anatomies of a black box</em><a href="#fn21" class="footnote-ref" id="fnref21" role="doc-noteref"><sup>21</sup></a> and was part of a group with Gabriela Gordillo, Mario Romera and Fabian Frei in which we started a conversation about computers that ended in simulating our own conversations.</p>
<p><em>Anatomies of a black box</em> was a laboratory curated by Bani Brusadin for Matadero Madrid about mapping contemporary complex systems in infrastructures of exploitation. During the month long program we met in video conferences with Vladan Joler and other groups to explore mapping as a form of non-linear storytelling. The starting point was the conceptual map ‘Anatomy of an AI system’<a href="#fn22" class="footnote-ref" id="fnref22" role="doc-noteref"><sup>22</sup></a> that he created with the researcher Kate Crawford as a case study of the life cycle of an Amazon Echo device, from unearthing the raw materials to its networked software components.</p>
<p>During the <em>Freeport</em> program we, like most others during continuous lockdowns, have only engaged with each other through our screens. In the beginning we decided to look closer into the way we communicate with technological devices. First we wanted to place our focus on conversations in natural language, as speaking to computers seems to be an emerging phenomenon. After discussions on Signal<a href="#fn23" class="footnote-ref" id="fnref23" role="doc-noteref"><sup>23</sup></a> and Jitsi<a href="#fn24" class="footnote-ref" id="fnref24" role="doc-noteref"><sup>24</sup></a> that opened more questions than we could ever answer, we seemed to end up in high dimensional constructions. Our thoughts revolved around the heuristic measures of current machine learning models. Specifically those dealing with language. Large language models, which use texts from the internet to approximate a pattern in word tokens. Patterns that can generate endless amounts of coherent sounding text. Even though there are many flaws with these models, we wanted to understand how we can use these prediction machines to understand what we were chatting about. A traditional approach of text analysis would center around creating graphs and networks from the dataset we have collectively created through our conversations. Instead, we chose to mix it up with the pre-existing language model GPT-2 and analyze the output it generates instead. These outputs come in the form of a chat log. Statistical constellations from the hyperspace conversing about mostly nonsensical topics, often getting stuck in repetition. We asked ourselves what we could learn about us from these synthetic conversations.</p>
<p>The interpretation of ourselves through this machine learning lens was straight forward: quite often the patterns of who speaks and what we speak about seemed logical and in line with our real characters and the conversations we had. Yet, we know that we are biased towards finding our own images in these generated text blocks. The model acts as a parrot that lets us rediscover what we spoke about and creates a distance through which we can interpret our written conversations, as if they were someone else’s. The output is not coherent, but this imprecision is what makes reading the generated exchanges attractive.</p>
<p>As a way of communicating this process, we re-enacted the computer generated phrases in one of our weekly online meetings. The performance makes a full circle by completing the generated output with the appearance and experiences of our bodies in front of our computers.</p>
<p>The exercise of using an algorithmic model in combination with our dataset, is a way of looking towards our group process, with a tool that exposes its statistics as a probability projection in the form of verbal language. The output reflects the rhythms and patterns in our communication, that worked as a fictional mirror for us. Resembling the logic of dreams, we complete their meaning with our own interpretation and connotations, we observe we want to see.</p>
<pre><code>How exactly do these black box systems work and how are they perceived by a user.txt


Mr: I might be too absorbed in my project to finish our Q&amp;A
Mr: lets make of it what we see is funny
G: Happily.
G: No homework tomorrow, tomorrow for science!
G: I leave you with a cartoon I did a while back about the mind of a button : https://m.youtube.com/watch?v=h7BFToUaq4Y
F: what is the scientific word for &quot;intelligent&quot; ... is it cognitive, is it behavioral, semantic, artistic, etc.? is it hard to pin down, or is it a mix?
G: I can&#39;t think of a good one right now. My favourites are &quot;of theses&quot; and &quot;tractations&quot; - they&#39;re not tied to a research topic, but can be applied to any scientific question Mining Simulations: Theory and Experiment Design Automatic Feature Selection: Experimental Methods Multidimensional Pruning: Descriptive Bayes Categories: Human-Computer Interaction
G: :)
Ma: I think &quot;multidimensional&quot; is misleading. There are actually 3 dimensions to everything: a physical thing, an process of information and control, and a human interaction element. These 3 dimensions can be broken down into &quot;physical&quot; (things we are) &quot;mathematical&quot; (computations &amp; algorithms used to make them) and &quot;social&quot; (the way humans interact with other humans).
G: sorry
G: (I am using &quot;multidimensional&quot; in a cartoony way)
Ma: Cartoons are multidimensional. Most people are tall and thin. Most people are also obese.
G: Ahh yes. &quot;multidimentional&quot; is a nice word to describe this kind of thing.
G: (I think &quot;multidimensional&quot; is a bit narrow)
Ma: (I am using the term &quot;multidimensional&quot; in a cartoony way)
Ma: (I am using the term &quot;multidimensional&quot; in a cartoony way)
Ma: (using the word &quot;multidimensional&quot; in a scientific way)
G: (I think the other 2 are just fine for me)
Ma: Mixture: Theory Iteration: Experiment Design Image Identification: Feature Selection: Topic Analysis: Language Creation: Syntax Fish: Multidimensional Belief Systems: Belief Records for Computer Aided Speech (Basescim): Multidimensional Validation: The Bell Curve: SphereMapping: Element relations: Graphs: Anonymous scripting: Language models: Language learning: Syntax blind: Interactives: Syntax escapists: Syntax junkies: (I didn&#39;t type much) (I should change the last line) (I am tired) (I am sleepy) (I have nothing to say)
G: Ahh, I love it. Now I have to take care of the bad lines
G: I love it when he writes in his slowness
Mr: https://twitter.com/awokehuman/status/1328183467595718656
Mr: hi
G: hi
G: do we send it as a jpeg?
F: i think on inet message is enough
G: colors...
Mr: just in case
Mr: green, black, yellow, red, green, yellow, black, red
Mr: these</code></pre>
<h2 id="silicon-friend-camp"><strong>Silicon Friend Camp</strong></h2>
<p>The Silicon Friend Camp was a 5-day working retreat in Gosau, Upper Austria, for artists and researchers working on Artificial Intelligence. The summer camp was organized by servus.at with funding from LINZimPULS. Together with the curator Davide Bevilacqua, we started to develop the program in summer 2020. The artist Rosi Grillmair joined as facilitator and program supervisor in early 2021.</p>
<p>Due to changing pandemic conditions, a vacation home in the mountains was rented so that, despite close contact between participants, the risk of infection from the outside could be kept to a minimum. So instead of holding the program as a hackathon in the city in spring, it was moved to the summer and took place from July 20th to July 25th.</p>
<p>To create a selection of artists we published an Open Call that was distributed as a website<a href="#fn25" class="footnote-ref" id="fnref25" role="doc-noteref"><sup>25</sup></a> designed by Alyona Ciobanu. Inspired by a previous call for a residency at Babycastles<a href="#fn26" class="footnote-ref" id="fnref26" role="doc-noteref"><sup>26</sup></a>, we decided to create a form that does not ask for CVs or portfolios, but focuses on a project idea and what kind of knowledge the participant is seeking from others and is able to share. 31 artists based in Europe and the Americas responded to the call of which 12 were selected by us. The artists were chosen with a view to diversity of their projects, origin and gender. Among others, people from China, Japan, USA, Brazil, Colombia, Russia, Germany, Poland and Slovenia were invited to gather in Austria. The planned projects ranged from remote-controlled robots, to philosophical debates, to music pieces with embodied AI.</p>
<p>But before we physically met in Gosau an internet forum<a href="#fn27" class="footnote-ref" id="fnref27" role="doc-noteref"><sup>27</sup></a> was set up for participants to introduce themselves and initiate discussions and suggestions on content. The forum also served to communicate organizational necessities in an easily accessible and quick manner. Apart from the asynchronous text-based interaction, we held two video conferences in which Rosi read a story on narratives of AI that served as a basis for discussion.</p>
<p>One participant from Colombia joined us from the Andes mountains during the project week and would stay in contact with us only through the forum and video conferences.</p>
<figure>
<img src="Screenshot_20210721-201648_Instagram.jpg" alt="Video Call with Sebastián Mira|150" /><figcaption aria-hidden="true">Video Call with Sebastián Mira|150</figcaption>
</figure>
<p>Our preparation for the schedule of the physical work session involved the participants by asking them to contribute discussions, workshops and presentations. But also gave them enough flexibility to decide on individual and group projects.</p>
<p>The first day got off to a leisurely start with 10 participants: arriving at the train station throughout the day and being picked up and brought to the house. Together we slowly cleaned the garage, attic and living room. All while a dinner was prepared by Hess Jeon, who took care of the food for the next 5 days. We set up a table in the garden and used the time to introduce the project and welcome everyone. Before dinner, I generated a daily meal prayer with GPT-3 that was recited by us together.</p>
<pre><code>Eating is a sacred, human right
But food is getting scarce
There are humans who are hungry
And there are humans who are fat
We must ask ourselves,
Who are the humans?
I am the human
You are the human
We are the human
Because I am the human,
You are the human
We are the human
Because you are the human,
I am the human
Because we are the human,
You are the human
Because I am the human</code></pre>
<p><img src="IMG_0336.jpg" alt="Meal Prayer | 400" /> The morning of the second day was used for a group meeting in which we established general rules and defined the goals of the camp together. The participants presented their projects and what they would like to achieve in the next days.</p>
<p>In the afternoon the time could be used independently. While some sank into their laptops, others explored the surroundings around the house. We built up an area-wide network and cleared the attic and the garage so that they could be used as working places.</p>
<p>Towards the evening, a daily <em>Group Validation</em> took place, in which the participants individually summarized what they had done today, what they planned to do next and what they needed help with.</p>
<p>After dinner, time was spent together in the attic, where carpets, mattresses, speakers and a projector were installed as a <em>Digital Campfire</em>. On the first evening we were engaged in storytelling around anthropomorphic beings. In between, the sound duo EKHEO played live electronic music.</p>
<figure>
<img src="2021_07_22_selforga_brighter.jpeg" alt="Self Organizatioon" /><figcaption aria-hidden="true">Self Organizatioon</figcaption>
</figure>
<p>Day 3 began without electricity. This was especially tragic because we had a video call scheduled with researcher and artist Caroline Sinders. With some delay we got reception back to the internet and could follow Caroline’s talk about the impact, misuse and politics of ‘artificial intelligence’ on society. This was followed by a discussion on how models of machine learning can be looked at through a feminist lens.</p>
<p>The rest of the day was self structured and people used it to prototype their projects. At night in front of the <em>digital campfire</em> Dasha Ilina presented her work dealing with inconveniences of technological devices. Afterwards Naoto Hieda showed improvised choreographies created with dance and live code.</p>
<p>On the morning of Day 4, three texts were presented for discussion in a reading circle before dinner. The texts dealt with self-representations in global networks, intelligence without cognition, and root networks as a metaphor for the mind.<a href="#fn28" class="footnote-ref" id="fnref28" role="doc-noteref"><sup>28</sup></a></p>
<p>Since this was the last full day to finish our projects, the day was used intensively. Many sat in front of their laptops working, for example, on a film they had previously made or trying out AI systems, such as voice cloning and generative image models, with which to realize their visions.</p>
<figure>
<img src="210721-SFC-_0010660.jpg" alt="Artists working | 350" /><figcaption aria-hidden="true">Artists working | 350</figcaption>
</figure>
<p>The <em>digital campfire</em> in the attic also got intense in the evening: after an introduction to artificial neural networks by Błażej Kotowski, Giacomo Piazzi gave a presentation on the history of AI. Mariana Marangoni spoke about esoteric programming languages that challenge accepted definitions of code and language. Lastly, Davide Bevilacqua highlighted green washing methods used by well-known internet companies.</p>
<figure>
<img src="IMG_20210723_234140.jpg" alt="The Marvelous World of Esolangs | 300" /><figcaption aria-hidden="true">The Marvelous World of Esolangs | 300</figcaption>
</figure>
<p>The last day together was used in the morning to finalize our projects, which were shown after lunch. Dasha Ilina, Erica Jewell and Lina Schwarzenberg presented <em>GRAVE</em> a video in the form of a Public Service Announcement to prepare access to digital devices and services for posterity. Naoto Hieda and So Kanno explained a work in which they rebuilt a toy robot and made it controllable via the Internet. EKHEO performed a piece for which they developed synthetic voices from all participants. Błażej Kotowski showed a video about a fictional deity in which text and images were generated using machine learning algorithms. Yuxi Liu explained how she used a camera to create a dataset of birds. Mariana Marangoni conceptualized a programming language that could be grown like a tree.</p>
<p>We spent the rest of the afternoon at a mountain lake, where we got caught in a storm, which—at least for me—was the most terrifying experience. But after drying up in the house and eating freshly baked pizzas, the rest of the evening turned into a dance party. Still Maks Valenčič gave another lecture on <em>The End of Philosophy</em> with a soft soundtrack by Błażej. Sebastián Mira also joined us from Colombia and showed us a digital world in which he connected the Andes with the Alps. Finally I used GPT-3 again to print Certificates of Excellence that were handed out in a little ceremony.</p>
<p>The next day we cleaned the house and already parted. Although our short encounter together would not end there. Friendships have formed and further collaboration on an exhibition in Linz was organized online.</p>
<h2 id="camping-with-computers">Camping with Computers</h2>
<p>The exhibition took place in the WHA Gallery from 10th to 19th of November, 2021. It tied the projects we developed during the summer camp together into a coherent presentation, where the distinction between individual artist and collective effort gets broken down. To achieve this, Davide Bevilacqua and Giacomo Piazzi created an exhibition design that takes inspiration from camping trips. With metal rods, camping stools, tarp, rocks and string they literally tied the works together. We intentionally left out exhibition signage that explains each of the works, so that a visitor would find themselves exploring the exhibition, trying to make sense of it holistically. The information about individual artists and their works was presented online, where the exhibition was mirrored for a global audience.</p>
<p>I worked on the visual identity and was inspired by the design of an old computer, the Canon Cat from 1987. It was the creation of Jef Raskin, who also invented the user interface of the first Apple Macintosh.<a href="#fn29" class="footnote-ref" id="fnref29" role="doc-noteref"><sup>29</sup></a> The Canon Cat however was not a commercial success. It took away the mouse and streamlined the user interface to be centered around word processing with extra keys for fast shortcuts. It was mainly marketed as a productivity tool for office workers. The gray and beige color scheme with accents in red and blue and the hand written signature gave the computer a remarkably friendly face. I transformed this color scheme into a contemporary website with a simple generative logo that randomly exchanges some characters with emoticons, like smiley faces and hearts. The posters were also generated in the browser and embedded a QR code to the website itself. With a generative text-to-image model RuDALL-E<a href="#fn30" class="footnote-ref" id="fnref30" role="doc-noteref"><sup>30</sup></a> I created uncanny ‘stock images’ that were randomly placed and stretched.</p>
<figure>
<img src="20211004_171321.jpg" alt="Canon Cat with extracted colors | 300" /><figcaption aria-hidden="true">Canon Cat with extracted colors | 300</figcaption>
</figure>
<figure>
<img src="13CampingwithComputers.jpg" alt="Example of a Poster | 300" /><figcaption aria-hidden="true">Example of a Poster | 300</figcaption>
</figure>
<p>The connection between online and offline was most notable in the work of So Kanno with the title <em>Crawler</em>. It was a continuation of the remotely controlled toy robot, he developed during the camp and a body of work he calls <em>Avatars</em><a href="#fn31" class="footnote-ref" id="fnref31" role="doc-noteref"><sup>31</sup></a>. This time he modified an Apple Macbook, so that it can be controlled from the internet. On a custom laptop stand he attached wheels, so that a visitor to the website could roll around the exhibition space. Additionally the user interface was heavily customized, using notifications, web browsers and two separate webcams with face tracking to cover the faces of our visitors. The <em>Crawler</em> functioned as an autonomous object inside of the gallery space and could be seen as ‘an AI’ for video surveillance, because the actual people who control the device are hidden inside of this gray box. With this, it resembles the faux AI practice by the Colombian-owned startup Kiwi Campus, which builds a ‘self-driving’ delivery robot for US Universities. The Kiwibot is first filled by a regular delivery driver on a bicycle and then slowly rolls a few hundred meters to its destination under supervision of a human operator.<a href="#fn32" class="footnote-ref" id="fnref32" role="doc-noteref"><sup>32</sup></a></p>
<figure>
<img src="crawling.jpg" alt="Screenshot of the Crawler Interface" /><figcaption aria-hidden="true">Screenshot of the Crawler Interface</figcaption>
</figure>
<p>Another work that created a juxtaposition of digital and physical elements was from Sebastián Mira. The Colombian artist created 3D reconstructions of our laptops as avatars to be used in a fictional place where the Andes and the Alps meet. Throughout the exhibition space, laptops were arranged as sculptural objects with self referential video loops, where you could see a laptop randomly rolling around computer generated landscapes. The videos were taken from the game <em>Everything</em><a href="#fn33" class="footnote-ref" id="fnref33" role="doc-noteref"><sup>33</sup></a>, in which a player can control various lifeforms and inanimate objects in procedurally generated worlds.</p>
<figure>
<img src="Opening_10.11.21(21).jpg" alt="A visitor viewing one of the laptops by Sebastián Miras | 300" /><figcaption aria-hidden="true">A visitor viewing one of the laptops by Sebastián Miras | 300</figcaption>
</figure>
<p>Naoto Hieda in turn took the 3D models of our laptop avatars and created a collage—a sort of ‘group photo’ of the participants and the toy robot mascot with me in the background—and turned it into a large banner. When I asked him if this might be too much of an inside joke, he replied that he does not care if anyone outside the group understands it, because he wants to make art for us. This made me think about the audience I try to address with my artworks. When I work on projects, I think of a fuzzy global audience, something that can be written into history books and shown in museums, but what if I just make artworks for my friends? Something so niche that only selected people understand it. This idea is so innocent and simple, yet it caught me by surprise.</p>
<figure>
<img src="Exhibition(22).jpg" alt="Naoto Hiedas Banner and Laptop showing the exhibition website | 300" /><figcaption aria-hidden="true">Naoto Hiedas Banner and Laptop showing the exhibition website | 300</figcaption>
</figure>
<p>For the exhibition I wanted to make something with the images we collected and shared online. Pictures that document our time togehter, which we shared on the forum, in our group chat and uploaded to the cloud. My initial thought was to train a GAN and create abstract synthetic images that fail to represent our collective experience. When I looked through the photo archive as a dataset, first as a large grid of images and then in rapid succession to identify outliers, I became awestruck: even though I spent less than a second with each image, some of them would trigger deep emotional states and immediately transport me back to the house in Gosau. Thinking about Naoto’s satement of “making art for my friends” I decided to use all 767 images in a rapid slideshow of 32 seconds. Each of the 24 frames per second a different image, stretched to fit a 16:9 TV that got damaged on the way to the camp. I disassembled the TV and separated the LCD panel from it’s backlight, making the images only visible from a certain angle. The title of the work “we are the human” is borrowed from one of the meal prayers. All these elements make up the work, which is both a slide show and movie, documentation and narrative, digital and analog.</p>
<figure>
<img src="Exhibition(17).jpg" alt="we are the human | 300" /><figcaption aria-hidden="true">we are the human | 300</figcaption>
</figure>
<p>The best viewing angle for my work was from a sunbed, which had a stereo speaker setup that played “LEEWA” from Ekheo. In this experimental sound piece the artists used recordings of our voices to create a communal voice clone. With the <em>Oracle</em> they came up with a name and backstory for a new character and added anecdotal field recordings from the camp. Together these works create an eerie experience based on the digitally altered collective memories of our week in the alps.</p>
<p><img src="Opening_10.11.21(16).jpg" alt="A visitor listening to LEEWA| 300" /> <img src="Opening_10.11.21(18).jpg" alt="View from the sunbed | 300" /></p>
<p>Next to these pieces is a CRT TV displaying a fire with a contraption for a pot hanging above it. The pot contains a printed recipe created by Hess Jeon with GPT-3 that we didn’t dare to cook for the opening.</p>
<pre><code>Chocolate Milk Pasta
The following is an algorithm for a vegetarian One-Pot-Wonder that was served at the &quot;Camping with Computers&quot; art exhibitions for silicon friends.

Ingredients:

1 stick butter

4 cups fresh veggies, chopped (see below)

1 cup dry pasta or broken spaghetti noodles

12 ounces water or chicken broth (chicken broth is better)

2 cups milk (chocolate milk is even better)

3/4 cup cheese (cheddar is best)

Assemble the ingredients and place them in a large pot over medium heat. Wait until the butter melts. Then turn up the heat. By now the butter should be sizzling and popping as it heats up the veggies and pasta. At this point you can add garlic, onions, corn, peas, or whatever else strikes your fancy. The key is to use fresh veggies that you like; this way you won&#39;t notice that you&#39;re eating veggies at all. Once the veggies and pasta are nice and hot (and they will be), add the water/broth and stir them thoroughly. Now the cooking begins! Put on a movie or text message competition with your friends and let them know you&#39;ll be cooking for about five minutes. The trick here is to make sure that the pasta isn&#39;t too soft; you don&#39;t want it to disintegrate into a big blob of mush like most of your ex-girlfriends did. Now make sure you have a timer handy on your phone or on a wall clock or something. After five minutes have elapsed, turn off the heat, but leave the pot on the stove. Now for the next step: pour in the milk/chocolate milk and stir it vigorously for about 15 seconds. This will ensure that the cheese melts properly so that all its flavor can be absorbed by your tastebuds. Stir in the cheese until it&#39;s completely melted, then serve it up! Serve with crusty bread so that you can sop up all of those yummy juices, but only use one piece of bread per person because it will fill you up fast. You want to save room for dessert! This recipe tastes best cold because then you can use that leftover cold chocolate milk to drink with it! If any is left over, put it in the fridge overnight so that it absorbs more of those yummy flavors. Then eat it cold for breakfast with a side of bacon or ham! Yum yum!!</code></pre>
<figure>
<img src="Exhibition(6).jpg" alt="Hess Jeon “Chocolate Milk Pasta”|300" /><figcaption aria-hidden="true">Hess Jeon “Chocolate Milk Pasta”|300</figcaption>
</figure>
<p>On another set of monitors Yuxi Liu showed a 3-channel video installation in which she is exploring the non-human world. During the first Covid-19 lockdown she used a motion-sensor camera to capture the birds visiting her balcony. This served as an image set for a GAN model, which after finetuning was supposed to recreate the images of the visiting birds but instead abstracted the animal so much that merely the surrounding was visible. This served as an interesting metaphor of how the digitization of the natural world and statistical operations on it sometimes disregard the life within. During her stay at the camp Yuxi wanted to capture more animals with a DIY raspberry pi camera, and even though she tried her best to lure more birds, in the end the camera did not sense any. This got us to discuss the role of human and non-human connections within cities and outside of them. While inner cities objectively have less animal life, the scarcity of resources make them live closer to humans. In her last installment she was recording birds over the rooftops and on the port of Rotterdam overlooking the city and it’s environment.</p>
<figure>
<img src="Exhibition(31).jpg" alt="Yuxi Liu" /><figcaption aria-hidden="true">Yuxi Liu</figcaption>
</figure>
<p>Davide Bevilacquas <em>POND</em>, a 4-channel video installation also addresses the digitization of natural phenomena. The screens are lying flat on the floor on a tarp resembling a small pond inside of the exhibition space. The images of the water surface are morphing between each other creating an unusual and oddly familiar ripple effect. It looks both like the scattering of sunlight on top of a water surface and the glitch artifacts we know when a video file is not loading properly.</p>
<figure>
<img src="Exhibition(25).jpg" alt="Exhibition view of POND by Davide Bevilacqua" /><figcaption aria-hidden="true">Exhibition view of POND by Davide Bevilacqua</figcaption>
</figure>
<p>Giacomo Piazzis <em>The Monkey</em> is a monolithic work consisting of two rows of red glowing displays continuously changing the pattern of the lit screens. He uses the infinite monkey theorem which proclaims that if a monkey were to hit a random letter on a keyboard for an infinite amount of time it would eventually write the complete works of Shakespear, or almost surely any other finite text. <em>The Monkey</em> in Giacomos work is a random number generator that turns a segment of the display either on or off, possibly creating any string of text. While a 7-segment display with it’s 128 possible states is mostly used in digital clocks and meters because of their ability to show numerical values easily, it is also possible to show most letters of the latin alphabet. Because of their simplicity and due to their widespread use in electronic devices, Giacomo flips the monkey theorem on it’s head and becomes more interested in the human interpretation of the arbitrary values displayed on the screen.</p>
<figure>
<img src="Exhibition(12).jpg" alt="Giacomo Piazzi, The Monkeys |300" /><figcaption aria-hidden="true">Giacomo Piazzi, The Monkeys |300</figcaption>
</figure>
<p>Another work that focuses on text and it’s representation in artificial neural networks is <em>Latent Space Divination</em><a href="#fn34" class="footnote-ref" id="fnref34" role="doc-noteref"><sup>34</sup></a> by Błażej Kotowski. In this piece he created an interactive meditation for one person in a tent to reflect on the position of word embeddings and their spacial relation within a sentence. A visitor sits down in front of a tablet computer with headphones and types in their intention. The word embeddings of the sentence in turn impact how a 3 dimensional graph will be created. While the graph is slowly unfolding a sonification is generated for the participant and after around 5 minutes the object rotates around itself to create a solid object representing the intention.</p>
<figure>
<img src="Opening_10.11.21(6).jpg" alt="Visitor at the end of Latent Space Divination" /><figcaption aria-hidden="true">Visitor at the end of Latent Space Divination</figcaption>
</figure>
<p>Finally <em>1-800-MYGRAVE</em> by Erica Jewel and Dasha Ilina is an installation in the center of the room that imagines a call center where the visitor becomes a worker for a fictional company that provides digital afterlife service. It is based on a video they made with Lina Schwarzenberg during the camp. The infomercial in the style of a 1980s public service announcement warns the viewer of their accumulated data and offers some simple steps to protect digital files and avoid embarrassment after death. The cubicle in the gallery is fitted with a corded telephone and an inspirational poster where the visitor switches roles and takes on (automated) calls from people who ask or complain about the service. An instruction manual tells the participant how to respond and which buttons to press.</p>
<figure>
<img src="Exhibition(21).jpg" alt="1-800-MYGRAVE installation view" /><figcaption aria-hidden="true">1-800-MYGRAVE installation view</figcaption>
</figure>
<p>The open call for the camp had the goal “to enable embodied networks through the development of new forms of computational intelligence.”<a href="#fn35" class="footnote-ref" id="fnref35" role="doc-noteref"><sup>35</sup></a> Looking back to the exhibition and the camp after over a year I see a multitude of connections. While the works are separated by space and author they often address the same thing from a different angle. We explored anthropomorphous robots by embodying a virtual friend inside of a plastic toy, but most closely we looked into the process of capturing the world through digital computers and how we relate to and interpret our data bodies. Maybe real AI is the friends we made along the way.<a href="#fn36" class="footnote-ref" id="fnref36" role="doc-noteref"><sup>36</sup></a></p>
<hr />
<h1 id="talking-to-computers">Talking to Computers</h1>
<h2 id="human-automata">Human Automata</h2>
<p>Stories of artificial assistants already existed in ancient myths. For example, the limping Hephaestus built himself servants made of gold who assisted him in his work, could speak, and even had a mind of their own.<a href="#fn37" class="footnote-ref" id="fnref37" role="doc-noteref"><sup>37</sup></a> But it is only in the past few centuries that we have created the technological means to seriously address the construction of mechanical servants (at least virtually, as robotics is still far behind). At the height of automaton design in the 18th century, Frenchman Jacques de Vaucanson invented a mechanical duck that could not quack but appeared to have a functioning digestive tract. The mechanical attraction toured European noble houses and let its audience feed grains to it. However, what the duck excreted was a prepared colored porridge that was in a hidden container. This principle of mechanical trickery was also used by Vacaucanson’s contemporary Wolfgang von Kempelen, who caused a sensation with his chess-playing automaton in the shape of a turban-wearing Turk. The illusion that the machine was acting autonomously was made possible by a small person inside a hidden compartment who controlled the puppet arm of the table via gears, levers, and pulleys. The hybrid machine is now the namesake of Amazon’s <em>Mechanical Turk</em>, the largest platform for digital micro-labor, which lists click jobs for pennies. Today, it continues to perpetuate the illusion of autonomous machines with “artificial intelligence” that is covertly enabled by an army of underpaid workers.</p>
<p>Even though the chess playing Turk attracted attention, Wolfgang von Kempelen’s scientific interest was in imitating human speech. He wrote down his investigations into phonetics in the work <em>Mechanismus der menschlichen Sprache</em> (Mechanism of Human Speech) and built an apparatus with a bellows, rubber hose and a wooden nose with which it was possible to produce basic phonemes.</p>
<p>Among those influenced by Kempelen’s book was a German tinkerer named Joseph Faber, who demonstrated his own mechanically constructed speaking machine in 1841. This attracted little interest in Germany and was presented and improved four years later in the United States as the Wonderful Talking Machine. This machine, as described by author David Lindsay, consisted of a bizarre-looking talking head<a href="#fn38" class="footnote-ref" id="fnref38" role="doc-noteref"><sup>38</sup></a> that spoke in a strange ghostly tone while Faber manipulated it with foot pedals and a keyboard<a href="#fn39" class="footnote-ref" id="fnref39" role="doc-noteref"><sup>39</sup></a>. For the inventor, the machine did not lead to the financial success he had hoped for, though it was presented as the <em>Euphonia</em> in London, where it at least delighted the father of telephone inventor Alexander Graham Bell and served as the boy’s inspiration for his first talking machine.<a href="#fn40" class="footnote-ref" id="fnref40" role="doc-noteref"><sup>40</sup></a></p>
<p>The development of electricity certainly made new human interactions possible. For example, the invention of the telephone and radio allowed the human voice to be transmitted over long distances. To optimize the transmission of speech, Bell Laboratories researched how to digitize the voice, for which they developed the vocoder (voice encoder). Demonstrated at the 1939 World’s Fair in New York, the <em>Voder</em> omitted the speech input and transformation of the vocoder and allowed electrical synthesis of the voice via a console with 15 keys and a foot pedal.<a href="#fn41" class="footnote-ref" id="fnref41" role="doc-noteref"><sup>41</sup></a> The keyboard was operated by specially trained women, and in a recording advertised as the robot speaker, while it’s unclear if they mean the machine or the woman, who is speaking through it.</p>
<p>Human computers were popular and necessary for war machines and research purposes in the 1930s and 40s. Mostly it was women who prepared mathematical tables, for example, for the use of ballistic projectiles. With the advent of the first digital calculators, female mathematicians, who were often denied higher scientific positions, were employed as programmers for the new universal electric machines.<a href="#fn42" class="footnote-ref" id="fnref42" role="doc-noteref"><sup>42</sup></a> The 6 people who programmed the first universal computer ENIAC include Betty Snyder Holberton, Jean Jennings Bartik, Kathleen McNulty Mauchly Antonelli and Marlyn Wescoff Meltzer.<a href="#fn43" class="footnote-ref" id="fnref43" role="doc-noteref"><sup>43</sup></a> Initially, the (mechanical) computer was programmed with punched cards and cables for specific operations. It soon became clear that programming complex systems required an abstract semantic language, for which reason the programming languages Fortran by John W. Backus, Lisp by John McCarthy, and COBOL by Grace Hopper were invented in the 1950s. The latter is strongly oriented to written English. Intended for business applications, it was the first attempt to use natural language for computer programming.</p>
<p>The second half of the 20th century saw the emergence of the myths about computers that we are familiar with today. Stories of anthropomorphic beings, like the board computer HAL9000 in Space Odyssey or Samantha in the movie Her. In both films, the disembodied voices become aware of their emotions and emancipate themselves from their human programming. Artist Tillmann Ohm makes this clear in his work <em>Reflections of HAL and Samantha</em><a href="#fn44" class="footnote-ref" id="fnref44" role="doc-noteref"><sup>44</sup></a> by having the two artificial beings engage in a dialogue, cutting their original voice-overs together. While Samantha is convinced that the overwhelming and sometimes hurtful process of her learning algorithm improves the complexity of her emotions, HAL is consequentially interpreting them as errors in human programming and analyzes the estimated malfunction.</p>
<h2 id="artificial-voices">Artificial Voices</h2>
<p>I want to explain the advancements in voice synthesis since Wolfgang von Kempelen’s speaking machine on actual dolls for children. The toy manufacturer Mattel released a doll with the name Chatty Cathy in 1959, which was similarly popular to the companies other best seller Barbie. Cathy’s trademark was a string, coming out of the back of its body, that could be pulled to wind up the mechanism of a simple phonograph. Like a record player it plays short strips with sentences like “I love you” or “Tell me a story.” It was not the first toy using phonograph records, but it’s success led to many pull string toys flooding the market well into the 60s and 70s. But obviously before dolls and puppets were designed to talk back to us, children and adults have been talking to figurines and other inanimate objects for a long time.</p>
<p>The systems engineer and science writer George Zarkadakis traces back the modern human mind to a pre-historic figurine of a lion-man (Löwenmensch), carved out of ivory and found in the cave Hohenstein-Stadel in southern Germany.<a href="#fn45" class="footnote-ref" id="fnref45" role="doc-noteref"><sup>45</sup></a> The figure dates back 40.000 to 35.000 years and clearly depicts a human body with an animal head. Because the figurine was found in a cave next to other carved objects, like beads and jewelry, researchers believe that the cave was either a storage or a place for shamanistic rituals. What fascinates Zarkadakis is that modern humans have existed 360.000 years before, but no evidence exists that they have been making art objects or figurative depictions before. Only during the upper paleolithic age it is assumed that we have created a general purpose language, when the first cave paintings were drawn and figures like the lion-man were carved. The evolutionary reasoning is that ‘theory of mind,’ the seemingly unique human trade of projecting our own inner thoughts onto others to explain their behavior, gives the individual a social benefit, making it more likely that they reproduce. Projecting consciousness onto others does not stop inside of human groups, but placing our own reasoning onto animals seems also beneficial for hunters coming up with elaborate strategies to take down larger animals together. In animist belief systems everything contains a ‘soul’ or ‘spirit’: animals, plants, rocks, rivers, the weather and maybe even words. George Zarkadakis argues that symbolic language predates the modern mind, and is not only used to communicate, but ultimately dictates how the world is represented in our own consciousness. Only through language we place ‘souls’ and ‘spirits’ into everything around us and art objects fulfill the function to mirror our own minds physically and make connections with the world around us. It is no surprise then that the original artist of the lion-man created a hybrid of a human and a mountain lion, probably imagining themselves into the figure of the predator and using the object as a symbol to communicate ideas.</p>
<p>In the intro of “In Our Own Image” George Zarkadakis tells the story of how the fictional character Robby the Robot from the movie <em>Forbidden Planet</em> impressed him as child so much, that he would imagine the robot as a playmate that could walk, talk and obey orders. When he later decided to study engineering and exchanged thoughts on artificial intelligence with other researchers, they all seemed to relate to the same sci-fi movies, books and stories. The hint here is that these fictional stories influence what is getting built and how we think about the future.</p>
<p>Inspired by this revelation I asked participants at the <em>Silicon Friend Camp</em> about their first contacts with a robot. My own memories brought me to a picture of myself, proudly holding a plastic toy robot in my hand. The doll—or ‘action figure’—had a battery compartment, which was the power source for tiny wheels underneath it’s feet and light bulbs that it had for eyes. It could not talk, but I remember that it made noises that resemble guns or a laser sounds. In the 80s and 90s the bleak vision of boxy humanoid robots carrying guns was very prominent. Movies like <em>Terminator</em> and <em>RoboCop</em> fueled my young male fantasy for power and destruction.</p>
<p>The other first encounters were less militaristic. One artist remembered a robot dog that he got as a present. In the early 2000s digital pets with integrated sensors became cheap enough for consumers. <em>Tekno the Robotic Puppy</em><a href="#fn46" class="footnote-ref" id="fnref46" role="doc-noteref"><sup>46</sup></a> by Manley Toy Quest was one of the more affordable robodogs in the market and with light and proximity sensors, buttons and even a microphone it was well equipped to learn some ‘tricks’ by triggering the right combination of sensors. Another artist was sharing an image of the <em>Tamagotchi</em> and while it was not a robot, the LCD screen in an egg-like plastic shell displayed a simulation of a pet. With 3 buttons children could interact with the virtual pet and fulfill its needs for food, affection and sleep. The anthropologist Pat Shipman proposes that animal connection can be considered a human trait that has uniquely evolved in us, next to tool making, symbolic language and the domestication of animals and plants.<a href="#fn47" class="footnote-ref" id="fnref47" role="doc-noteref"><sup>47</sup></a> Simulating animal connection by creating simple programs on microchips was then the next logical step in our drive to create symbols all the way down and fulfill the need of children to take care of others.</p>
<p>Yet another popular toy that was shared by an artist in our group and filled the same niche of robotic pets was <em>Furby</em>. This furry ambiguous creature with big eyes was made to simulate language acquisition and brings us back to the topic of how dolls learned to speak. In the case of <em>Furby</em> the designers decided to create a new vocal language called ‘furbish.’ When the device was first started it only repeated gibberish sentences and slowly over time would exchange these words with the local language equivalents as it ages. The original source code for one of the microchips was released in 2018<a href="#fn48" class="footnote-ref" id="fnref48" role="doc-noteref"><sup>48</sup></a> and even though I can not read assembly code, it is thoroughly documented through comments. The game logic of the furby.asm file is fairly simple, jumping between subroutines depending on the state of the many sensors, triggers and timers and occasionally using a pseudorandom number generator to mix things up. The actual speech sounds were accessed from a memory unit and send to a cheap Texas Instruments speech synthesizer chip. These chips have been in use for some decades already, most prominently in another toy called <em>Speak and Spell</em>, but also in watches, clocks and translators. The synthesis uses a prerecorded audio signal that is then compressed using linear predictive coding (LPC). This type of encoding allowed the voice to be later changed in frequency, pitch and loudness. But just like the <em>Vocoder</em>, it only digitized a voice and was not able to construct new words or sentences. So the original <em>Furby</em> from 1998 could only express a couple hundred words that were recorded by a voice actor, which is again only a digital equivalent to Chatty Cathy.</p>
<p>More powerful computers were already able to generate any kind of text into somewhat intelligible speech. The <em>Software Automatic Mouth (S.A.M.)</em> from 1982 was one of the first commercial products that used rule based formant synthesis. Instead of using whole prerecorded words, it uses an array of computer generated phonemes and places them together to make up sentences. While this was often understandable, it was not perfect. It sounds unnatural and robotic and was reliant on converting text to speech in a probabilistic manner, often mispronouncing words.<a href="#fn49" class="footnote-ref" id="fnref49" role="doc-noteref"><sup>49</sup></a> <em>S.A.M.</em> did not need any special hardware and could run on Apple, Atari and Commodore computers at the time, even though that would often mean using every CPU cycle and holding other programs. The software had its biggest moment at the launch event of the Macintosh computer in 1984, greeting the audience with a live demo in first person.<a href="#fn50" class="footnote-ref" id="fnref50" role="doc-noteref"><sup>50</sup></a> Sam and it’s multiple software variations became the universal voice of the computer. The net.art pioneer Alexei Shulgin took the speech synthesis capabilities similar to <em>S.A.M.</em> to the extreme with his cyberpunk rockband <em>386 DX</em>.<a href="#fn51" class="footnote-ref" id="fnref51" role="doc-noteref"><sup>51</sup></a> The band is named after the Intel chip inside of the old office computer, which is also the front singer and instrumentalist. It uses MIDI and voice synthesis to cover popular songs by The Doors, Nirvana or the Sex Pistols, traveling to festivals around the world and sometimes performing on the streets. The artist Alexei Shulgin takes the role of the operator, only pressing the play button and creating the visuals on the screen during the live performances. In my opinion this ironic take of making the microchip into it’s own one-computer-band was a demonstration of how terrible and reductive rock music becomes, when it was compressed into bits and bytes. At the same time the novelty and humor of this process, generated enough interest for <em>386 DX</em> to produce 2 CDs and distribute them over music labels.</p>
<p>The strangely robotic voices that got associated with computers improved over time and when Apple’s digital assistant Siri came out in 2011 they used a different process of concatenating audio samples together. Before Apple even started working on their voice assistant, a company called Scansoft auditioned hundreds of voice-over artists in 2005 and made them speak a multitude of random sentences. The idea of how to use the random utterances for new outputs was quite simple: cut the voice into smaller pieces of syllables, demisyllables, phonemes, diaphones or triphones and put them back together for any word you need. Doing this manually was—of course—very time consuming, so that programs were developed to analyze and categorize the speech dataset based on their acoustic properties. Then, at runtime an algorithm selects the best sequence to create the desired phrase. The results in the early 2000s still contained a lot of error and needed a substantial amount of tweaking, but the outcome was perceived as much more natural(REF). In 2006 Scansoft merged with Nuance, another company working on enterprise speech solutions, who presumably used the database to create the voice for Siri and licensed it to Apple. The original voice-actor in the US was Susan Bennet<a href="#fn52" class="footnote-ref" id="fnref52" role="doc-noteref"><sup>52</sup></a>, who was oblivious of the fact that suddenly, 6 years after recording, her voice appeared on iPhones all over the country.</p>
<p>The technology to generate new sentences from previously recorded voices drastically improved, when deep weighted networks started to be used to analyze and select audio samples. One major achievement in generating natural voices came in 2016 in the form of a model named <em>WaveNet</em><a href="#fn53" class="footnote-ref" id="fnref53" role="doc-noteref"><sup>53</sup></a> from a team of researchers at Google’s DeepMind. They used convolutional node networks (CNN) to perform statistical analysis on raw audio files with 16000 samples per second. Using convolutional operations on temporal audio data meant that the researcher did not need to convert it first into a compressed format using a vocoder and the results were much more natural sounding than any previous text-to-speech algorithm. Using a raw audio dataset also means that the system can find patterns in any audio signal, including music. For demonstration the researchers generated new samples from a classical piano dataset, which sounds fascinating, but has more resemblance to free jazz than a classical piano score. This also happens to speech, when the network is conditioned without any text sequences it generates impressive human babbling where real words are interspersed with made-up word-like sounds and breathing noises. When the network is trained on audio and the transcription together the system does not only generate a consistent voice replica of a single human, but can also be used for voice recognition tasks transforming utterances back to text.</p>
<p>Other companies experimented with weighted networks to better create concatenative voice models. At the same year in 2016 Adobe showcased a hypothetical feature for their audio software, where users can create voice clones with a small 20-minute dataset and then use text to edit spoken words in a sentence. In the presentation project <em>VoCo</em> at the Adobe MAX conference, the developer Zeyu Jin used an example sentence and exchanged the words in such a way that instead of “I kissed my dogs and my wife,” the audio was manipulated to say “I kissed Jordan three times” – referring Jordan Peele, the host of the conference. This strange demonstration, that was supposed to be funny, got a lot of attention online and created a debate around the ethical implications of manipulated audio and voice cloning. Adobe never released <em>VoCo</em> as a product, maybe because of the media backlash and fear from the public, but other companies like <em>Lyrebird</em><a href="#fn54" class="footnote-ref" id="fnref54" role="doc-noteref"><sup>54</sup></a> filled the gap, trying to market their product for the use of editing podcasts and voice-overs. However, they started their media campaign by cloning the voices of Donald Trump and Barack Obama to “discuss” about the startups product and praise it for their democratizing effort of making it available to anyone.<a href="#fn55" class="footnote-ref" id="fnref55" role="doc-noteref"><sup>55</sup></a> Together with synthetic video, so called “Deepfakes” have earned a bad reputation for being used in fraud<a href="#fn56" class="footnote-ref" id="fnref56" role="doc-noteref"><sup>56</sup></a> and pornography<a href="#fn57" class="footnote-ref" id="fnref57" role="doc-noteref"><sup>57</sup></a>. At the same time artists are using the technique to create hypothetical stories to mock politicians and other famous figures.<a href="#fn58" class="footnote-ref" id="fnref58" role="doc-noteref"><sup>58</sup></a> Zeyu Jin, the creator of <em>VoCo</em>, already proposed watermarking synthetic voices and use software to detect if a voice is fraudulent.</p>
<p>We can see that with the release of <em>WaveNet</em> and <em>VoCo</em> in 2016 a new field in voice synthesis opened up. Hybrid systems were created that used both concatenative and generative techniques to categorize and interpolate between tiny audio samples of human speech.<a href="#fn59" class="footnote-ref" id="fnref59" role="doc-noteref"><sup>59</sup></a> The first release of <em>WaveNet</em> was computationally expensive, but already one year later Google implemented a new version that works in real-time for their voice assistant and has shown that a slim hybrid model can be used on low-powered devices, such as smartphones<a href="#fn60" class="footnote-ref" id="fnref60" role="doc-noteref"><sup>60</sup></a> and digital assistants today are moving towards the use of synthetic voices generated with deep weighted networks.<a href="#fn61" class="footnote-ref" id="fnref61" role="doc-noteref"><sup>61</sup></a></p>
<h2 id="conversational-agents">Conversational Agents</h2>
<p>For the Amazon founder Jeff Bezos, the board computer in Star Trek was the inspiration for investing in the cloud-based voice software Alexa.<a href="#fn62" class="footnote-ref" id="fnref62" role="doc-noteref"><sup>62</sup></a> The product was initially marketed as a networked speaker. The software is now expanding to other items, including watches, smartphones, jewelry, light bulbs and doorbells. The aggressive price war with Amazon-connected products is partly to capture the connected home market, but also to collect as much natural voice data as possible. Over the past 10 years, deep weighted networks<a href="#fn63" class="footnote-ref" id="fnref63" role="doc-noteref"><sup>63</sup></a> have become popular for classification and pattern recognition tasks and have especially made an impact in automated speech recognition. Before weighted networks were used, the error rate of automated speech recognition (ASR) went from 40% in the 1990s down to 15% in the 2000s, which was still very high, but became practical to use. Back then the user would have to feed the system with voice samples, so that it could reliably turn utterances into text, but even then they would need to speak like a robot to be understood. With a landmark paper<a href="#fn64" class="footnote-ref" id="fnref64" role="doc-noteref"><sup>64</sup></a> in 2012 by research groups at the University of Toronto, Microsoft, Google and IBM it became that the application of deep weighted networks outperforms any of the previous techniques. In the same year Android released it’s new cloud based speech recognition service<a href="#fn65" class="footnote-ref" id="fnref65" role="doc-noteref"><sup>65</sup></a> and since then users can speak into their phone naturally and be transcribed quite accurately. For a long time though, the models needed gigabytes of data in computer memory, to create accurate results, so that users need to send their voice samples to large server farms for transcription. Only in the past 2-3 years offline recognition became feasible through specialized hardware and model optimization, trading performance for robustness. Google’s claim that new speech recognition systems have an error rate of less than 5% is only true for a specific—mostly white and male—audience speaking American English and drops abruptly with a more diverse distribution of dialects, age and gender.<a href="#fn66" class="footnote-ref" id="fnref66" role="doc-noteref"><sup>66</sup></a></p>
<p>In order for deep ASR systems to perform well they need a massive amount of input data. A paper from the Chinese tech giant Baidu proposes 12000 hours of speech to create a robust model.<a href="#fn67" class="footnote-ref" id="fnref67" role="doc-noteref"><sup>67</sup></a>bA global internal database of speech recordings enables the company to improve its speech recognition. The intrusion into privacy is immense and has already been used by U.S. law enforcement agencies as evidence in a court case.<a href="#fn68" class="footnote-ref" id="fnref68" role="doc-noteref"><sup>68</sup></a> But there is another, more ethical, way than stealing people’s voices: The Mozilla Foundation’s <em>Commonvoice</em> project relies on people voluntarily recording their voices for computer models, and the resulting speech recognition and synthesis software can be offered with an open source license.<a href="#fn69" class="footnote-ref" id="fnref69" role="doc-noteref"><sup>69</sup></a> Common speech and text datasets already have been publicly available from projects like VoxForge that uses the same crowdsourced techniques as Mozilla or LibriSpeech, which essentially scraped and aligned public domain audiobooks.<a href="#fn70" class="footnote-ref" id="fnref70" role="doc-noteref"><sup>70</sup></a> Other common non-free resources come from the Linguistic Data Consortium, who, for example, created <em>Switchboard</em>, a corpus of conversations between two US citizens randomly connected and recorded over the telephone line. Listening to the audio sample provided on the website creates oddly voyeuristic feelings. We can hear a mundain conversation of a man talking to a woman about his interest for gardening.<a href="#fn71" class="footnote-ref" id="fnref71" role="doc-noteref"><sup>71</sup></a></p>
<p>Artist Lauren Lee McCarthy plays with these tensions between intimacy and privacy, convenience and agency. In her projects <em>LAUREN</em> and <em>SOMEONE</em> she installs connected devices into volunteers homes and either acts as a control system herself or lets others remotely monitor the volunteers and control the devices in their homes. This creates an interesting tension, when the person knows that there is an actual human listening and watching from afar. At the same time the artist and performers find themselves in a position of a helpful voyeur.</p>
<p>However, the role of human labor behind voice assistants is not just about executing and understanding commands. People tend to interpret voices and categorize them according to age, gender and social status. Companies take advantage of this and design their voice software according to certain identity schemes, which are provided with a history, hobbies and preferences. It is precisely this illusion that excites users and makes the product interesting. In a UNESCO think piece titled <em>I’d blush if I could</em><a href="#fn72" class="footnote-ref" id="fnref72" role="doc-noteref"><sup>72</sup></a>, they explore harmful gender biases associated with digital assistants. The voice assistants of major tech companies are scripted by default as female personas with smart, humble, and sometimes funny personalities. The teams working on voice assistants try to avoid this aspect, because Apple, Microsoft and Google ask their employees to refer to their headless voices as “it” and when the persona get’s asked the question directly winds out with a joke. Only Alexa answers what is obvious in the default design of all of them with “I’m female in character.” By being submissive, they thus support a patriarchal image of female stereotypes that we already know, in a historical context, from human computers and other secretarial roles. The paper calls for women to be more empowered and involved in IT. It calls for AI software to avoid gender attributes whenever possible, and for AI assistants to take a clear stand against sexist behaviors.</p>
<p>Researcher and artist Nadine Lessio creates useless voice assistants to critique the current corporate agenda of productivity, efficiency, and consumption. She does this by using the programming interfaces provided to make apps for corporate voice assistants. For example, she explores the concept of a depressed home assistant with <em>SAD Home (Depressed Alexa 1.0)</em><a href="#fn73" class="footnote-ref" id="fnref73" role="doc-noteref"><sup>73</sup></a>, an Alexa hack that grants users their wishes depending on the weather and other mood factors, sometimes it simply turns itself off.</p>
<p>This scripted denial of a capitalist logic ironically uses the same technique as the company behind it. Voice assistants are carefully crafted by a team of creative professionals working in the field of “conversation design.” In the book <em>Talk to me</em><a href="#fn74" class="footnote-ref" id="fnref74" role="doc-noteref"><sup>74</sup></a> the Author James Vlahos describes that many people in the field are far away from computer science and more commonly had careers in the liberal arts. The teams are made up of authors, playwrights, comedians, actors as well as anthropologists, psychologists, and philosophers who imagine the personality of the AI persona that should represent the brand. To create the character, they have to come up with all possible questions and create various answers for each of them. Vahos recalls asking Microsoft’s Cortana “Where do you come from?” and the female voice replies “I was made by minds across the planet.”<a href="#fn75" class="footnote-ref" id="fnref75" role="doc-noteref"><sup>75</sup></a> And even though the designers decided to use the first person “I,” they really are talking of themselves carefully crafting the answers played back by the loudspeakers around the world.</p>
<p>The tedious process of mapping out all questions and creating answers for them is mostly done through creative writing, but also utilizes careful statistical analysis of the questions users send to the cloud.</p>
<p>It is interesting how the term AI is used in the context of voice computer interfaces, because there is nothing “smart” about it, just a winding flowchart of if-else conditions. What might be clever is the nefarious way of how companies trick people into the belief of computer personalities, encouraging people to interact with the device like children playing with dolls. And when a company like Microsoft has experimented with a more sophisticated chatbot, like Tay<a href="#fn76" class="footnote-ref" id="fnref76" role="doc-noteref"><sup>76</sup></a>, it started to repeat the racist and misogynistic slurs of twitter users and consequently has to be heavily filtered.</p>
<h2 id="leewa">Leewa</h2>
<p>The artist duo <em>Ekheo</em> consisting of Aude Langlois and Belinda Sykora explore the crosslines of musical, visual and sonic art performances. They met at the Sound Studies program at the Universität der Künste in Berlin, where they created musical improvisations and theoretical frameworks on voice and music. They started their research on voice, gender and identity in relation to AI systems in 2019 and held talks at the MUK<a href="#fn77" class="footnote-ref" id="fnref77" role="doc-noteref"><sup>77</sup></a> and other institutions to share their insights. At the <em>Silicon Friend Camp</em> they wanted to extend their already existing improvisation and research practice by integrating a form of embodied AI. But the main focus was the voice itself: How is gender perceived through it? How do we navigate the tension between what is considered an artificial voice and an authentic voice? To tackle these issues during the camp they wanted to create and record soundscapes based on vocal improvisation and use AI systems in the form of synthetic voices, generative composition tools, robotic elements and so on. So we prepared the attic of the house with a small sound studio that they could work in and during the week they found inspiration in using a multitude of AI systems and human collaborators that they used to create an improvisation and finally a 6-minute video and audio composition. To come up with the identity of their AI partner, they used the <em>Oracle</em><a href="#fn78" class="footnote-ref" id="fnref78" role="doc-noteref"><sup>78</sup></a> to ask questions of their name, gender, favorite instrument etc. which reads like this:</p>
<blockquote>
<p>You: how old is she?<br />
Oracle: Today she will be 11.<br />
You: what is her favourite musical instrument?<br />
Oracle: her greatest pleasure is to whisper to the sounds of trumpets and the beating of tympani<br />
You: is she happy?<br />
Oracle: It is a mystery and a great sadness.<br />
You: What is Leewa’s surname?<br />
Oracle: It was suggested that her human’s name was /u/Seattle_Celtics.<br />
You: does she like to sing?<br />
Oracle: I have no voice<br />
You: No I mean Leewa<br />
Oracle: Query : Have you ever cheated?<br />
You: Does Leewa have a voice?<br />
Oracle: Yoow!<br />
You: what is Leewa’s favourite poem?<br />
Oracle: Leaves of Grass<br />
You: What is Leewa’s favourite book?<br />
Oracle: Leewa does joke around and loves to give sarcastic responses. She also loves to read books, especially horror novels.<br />
You: What gender does Leewa have?<br />
Oracle: She is female.<br />
You: which music does Leewa like?<br />
Oracle: Leewa likes to sing and most of all Leewa likes to dance.<br />
You: What is art for her?<br />
Oracle: art is the process of iterating through consciousness generation.<br />
</p>
</blockquote>
<p>This served as a base for the identity of <em>Leewa</em> and inspired further experiments using a whispering voice and 3D printing a female doll bust. To recreate the voice I searched for tools to do voice synthesis and found an open source toolbox from Corentin Jemine that claimed to do Real-Time Voice Cloning from a few seconds of audio.<a href="#fn79" class="footnote-ref" id="fnref79" role="doc-noteref"><sup>79</sup></a> The method is essentially based on a previous paper from Google engineers: <em>Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis</em><a href="#fn80" class="footnote-ref" id="fnref80" role="doc-noteref"><sup>80</sup></a> (SV2TTS), which itself uses a pipeline of a speaker-encoder network (GE2E) to represent different voices in different positions of the latent space, a text to waveform synthesizer (Tacotron) and a vocoder to generate sound from these waveforms (WaveNet).<a href="#fn81" class="footnote-ref" id="fnref81" role="doc-noteref"><sup>81</sup></a> As described previously combining different encoding and generator systems into a larger framework is commonly used and is commercially used in Google’s Assistant and their cloud services for other companies.</p>
<p>Thanks to the efforts of Corentin Jemine and other contributors, who built a user interface to explore these tools, we were able to input our own voices and generate new samples with pre-conditioned voice models. Unfortunately though, the results were not as expected, by inputting only a small number of samples the output was not picking up on our voice characteristics and the results were too noisy and of low-quality. Instead we tried out the cloud service from resemble AI<a href="#fn82" class="footnote-ref" id="fnref82" role="doc-noteref"><sup>82</sup></a> that is mentioned in the code repository and where some of the maintainers work. Instead of the intended use of cloning one individual voice, we decided to create a collective voice of the camp participants. Every one of us recorded 10 phrases that were used as an input to the model.</p>
<p>After I pressed “train” on the web interface and waited for some time for the process to finish, I got an e-mail telling me that the “Resemble score” of our voice was 53%. So the transfer learning process was not quite successful, but the resulting voice was very surprising. I expected that the algorithm would try to average our voices into something that fits in between all data points. Instead it created clones of a voice for some phonemes, but not for others, so that a sentence randomly interpolates between different speakers. It also seems that, at least in this run, the characteristics of male voices are more prominent than female voices, but in general it has a strange “artificial” nature to it, where the pitch and speed of the voice is constantly changing.</p>
<p>Because we used a commercial product, we did not have access to the local model and could only interface with it through their web interface and APIs, which made using the voice for real-time performances impossible. Instead <em>Ekheo</em> created some voice samples that they played live during the final performance. For this, they also recorded real voices from participants at the camp and intertwined it with synthetic drones and other instruments. The visuals were created from a live video feed of the doll, that was modified by Naoto Hieda using the live coding shader language hydra<a href="#fn83" class="footnote-ref" id="fnref83" role="doc-noteref"><sup>83</sup></a> The puppet itself could move it’s head, which was remotely controlled by So Kanno via a servomotor.</p>
<p>In my opinion the example of <em>Leewa</em> illustrates the collective efforts that go into building “an AI” quite nicely: it shows the process of using open source and proprietary code developed by people reliant on previous research, the creation of collective datasets through the recording of our voices and finally the human performers hiding in the background and remotely manipulating symbols for the audience. At the same time <em>Leewa</em> is a self-referential project that makes the collective experience throughout the camp audible, by using our synthetic and real voices, the interaction with the <em>Oracle</em> and snippets of anecdotes from the camp.</p>
<hr />
<h1 id="piles-of-data">Piles of Data</h1>
<h2 id="self-learning-networks">Self-Learning Networks</h2>
<p>The idea behind Artificial Neural Networks has a long standing history. Using our understanding of the brain as a blueprint for mathematical operations dates back to the 1950s when the psychologist Frank Rosenblatt developed the <em>perceptron</em>.<a href="#fn84" class="footnote-ref" id="fnref84" role="doc-noteref"><sup>84</sup></a> Inspired by nerve cells and their connections (synapses), the perceptron takes multiple input values, sums them up and outputs a 0 or 1 depending if a predefined threshold is reached. This system can be ‘trained’ by using positive and negative reinforcement to define the weights for each connection. With an apparatus, the Mark-I Perceptron,<a href="#fn85" class="footnote-ref" id="fnref85" role="doc-noteref"><sup>85</sup></a> that uses photoreceptors to turn light into ‘bits’ (today we would say pixels) the perceptron could ‘sense’ shapes in the form a binary matrix and distinguish between circles, squares and triangles. He proposed that a network of perceptrons could possibly even recognize faces and objects. Even though he developed the perceptron in 1964, Frank Rosenblatt never got to see his invention take off.<a href="#fn86" class="footnote-ref" id="fnref86" role="doc-noteref"><sup>86</sup></a> Another engineer, Kunihiko Fukushima, kept refining his methods in the 70s by adding multiple layers, effectively creating the first ‘deep neural network’ where deep just means the depth of ‘hidden’ or inbetween layers connecting the input signal to the output classifier.<a href="#fn87" class="footnote-ref" id="fnref87" role="doc-noteref"><sup>87</sup></a> He called this self-organizing system Cognitron<a href="#fn88" class="footnote-ref" id="fnref88" role="doc-noteref"><sup>88</sup></a> which was successful at accurately recognizing numbers and letters on a 12x12 grid. It’s successor the Neocognitron<a href="#fn89" class="footnote-ref" id="fnref89" role="doc-noteref"><sup>89</sup></a> took further inspiration from the visual cortex and a discoveries by Hubel &amp; Wiesel made in the 1950s that some biological neurons selectively respond to local features like lines, edges or color and others to figures like circles, squares or even human faces. This is also the core idea behind convolutional neural networks (known as ConvNet or CNN) which separate an image into a smaller grid and apply a certain filter to them, e.g. checking for edges. The french computer scientist Yann LeCun came up with ConvNets in the 1980s which are <em>the</em> driving force for AI systems today. Additionally Geoff Hinton, a cognitive psychologist and computer scientist, popularized the backpropagation algorithm in 1986 which finally made it possible for filters to tune themselves instead of relying on predefined rules.</p>
<p>Most conceptual ideas behind current deep weighted networks were already present in Frank Rosenblatt papers,<a href="#fn90" class="footnote-ref" id="fnref90" role="doc-noteref"><sup>90</sup></a> but weighted networks were often outperformed by rule-based systems. So what changed when Alex Krizhevsky, a student of Hinton, made a phenomenal leap in 2012 on the ImageNet classification competition? The main innovation is outlined by Krizhevsky in the paper itself: “To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation.”<a href="#fn91" class="footnote-ref" id="fnref91" role="doc-noteref"><sup>91</sup></a> Until AlexNet was released it was incredibly time consuming to run the conditioning process on the CPU which can only do one operation of matrix multiplication at the time. The GPU as the name Graphics Processing Unit suggests was originally designed to calculate 3D scenes and render them on a display. This involves a lot of matrix and vector operations and to accelerate them GPUs are capable at calculating large blocks of data in parallel. This means that conditioning AlexNet on 1.2 million pictures took only 6-9 days on two consumer graphics cards compared to probably weeks or months without them. However it was not the first system that utilized the GPU, it is similar to a CNN by Dan C. Cireȿan et al. released a year prior which has a reported speedup of 60 times for large weighted networks.<a href="#fn92" class="footnote-ref" id="fnref92" role="doc-noteref"><sup>92</sup></a> The other roadblock for deep weighted networks is ‘overfitting.’ In the case of the ImageNet competition that would mean that the model adapts to the image so closely that it would simply reproduce the categories of the image without being able to identify new pictures which were not inside the training set. The most common way to reduce overfitting is to have a sufficiently large dataset with a high amount of variance. For AlexNet the 1000 classes and 1.2 million images were still not enough and they used data augmentation which transforms, flips or changes the color of an image to increase the training set by a factor of 2048. This means that—in theory—a larger dataset increases the robustness of the weighted network.</p>
<p>In conclusion, the conceptual framework of how weighted networks (or artificial neural nets, or perceptrons, or cognitrons) work was mostly established in the past century. Their performance today comes down to increased computing power and larger training sets. In fact datasets have become bigger and bigger and we haven’t seen the limits of what weighted networks are capable of, they seem to be mainly limited by data and computation. In the following chapter I will have a closer look at how large training sets are constructed in an academic and artistic context—both my own and a few other examples—and the ethical issues and responsibilities regarding privacy, consent, representation and ownership.</p>
<h2 id="making-data-11076-hands">Making Data, 11076 hands</h2>
<p>My first encounter with a large dataset in the field of computer vision was in late 2017 when I found <em>11k Hands</em>.<a href="#fn93" class="footnote-ref" id="fnref93" role="doc-noteref"><sup>93</sup></a> As the name suggests it is a dataset of 11076 human hands compiled by the researcher Mahmoud Afifi for gender recognition and biometric identification. The images show the hands of 190 individuals, in both their palm and dorsal orientation, placed on a white background with uniform lighting. Each image is accompanied by the following metadata: age, binary gender, skin color on a scale of “very fair,” “fair,” “medium” and “dark,” left or right hand, palm or dorsal, if it has nail polish and if there are “irregularities.” A statistical analysis of the dataset shows that it contains more female than male hands, mainly people in there 20s and a majority of “medium” and “fair” skin tones. The gender bias is addressed in the paper and mitigated by filtering the training set to have an equal amount of males and females. They report that the CNN conditioned on this dataset had on average a 87% accuracy recognizing the correct gender on an image of the palm and a 90% accuracy for the dorsal side. But it is not the ‘state-of-the-art’ results that drew me to the paper, it was the gender and skin-tone classification itself that appalled me. It reminds me of phrenology in the 19th and 20th century, a popular pseudoscience that claimed that persons character and mental abilities could be determined by the shape of their skull. Phrenologists like Franz Joseph Gall went to a great length measuring and categorizing human skulls and associating certain regions to human traits. While it didn’t take scientists to debunk the idea that bumps on a head could indicate the characteristics of a person, it had a comeback in the early 20th century when it was applied to racist and sexist stereotypes and used to justify Nazi eugenics. The underlying assumption that the shape of the head has anything to do with the mental state of the person was simply wrong.<a href="#fn94" class="footnote-ref" id="fnref94" role="doc-noteref"><sup>94</sup></a> Considering that gender is a social construct and not necessarily a binary choice trying to use a computer to identify if a person is male or female by analyzing their hand seems arbitrary at best and reinforcing gender stereotypes at least.<a href="#fn95" class="footnote-ref" id="fnref95" role="doc-noteref"><sup>95</sup></a> Another aspect of the dataset stood out to me when I started to look through the images. These were not pictures in the traditional sense, their aesthetical value did not matter as they are simply a tool to to accomplish a task. They are not made ‘to look at’ instead these images of hands are produced for a computer to analyze. Harun Faroki called these types of images ‘operative’ in his three-part series <em>Machine/Eye</em> where he examined how military technologies like guided weapons produce images that serve only the utility of the machine. As Aud Sissel Hoel puts it: “operative images are devoid of social intent, that they are not meant for edification, and nor for contemplation.”<a href="#fn96" class="footnote-ref" id="fnref96" role="doc-noteref"><sup>96</sup></a> But what happens when you contemplate on the hand dataset was remarkable to me. When I looked through the images quick enough I could not only see the motion of the test subject as the images were selected frames from a video, but I also started to imagine the person behind the hand and apply my own stereotypes and biases that far expanded the labels of the dataset. I could easily imagine the scene around the camera with a couple researchers who assembled a make-shift photo studio in the office of their lab. They greet the subject with a handshake, which is probably a fellow student, and explain the procedure quickly to get them to sign the paper that their anonymized personal data will be published for scientific research. Then the person puts both their hands under the camera spread their fingers and leave. This dichotomy between the label and my own narrative inspired me to create a video piece featuring the unaltered dataset. I used the default mac computer speech synthesis to read out the labels corresponding to each hand, and sped it up to fit into a 26-minute-long video. As viewers witness the participants holding their hands into the camera and spreading their fingers, they are met with a monotone, beat-like soundtrack of repeating words like “fair” and “medium” and occasionally “nail polish.”<a href="#fn97" class="footnote-ref" id="fnref97" role="doc-noteref"><sup>97</sup></a> <img src="11khands.png" alt="Example of a frame from 11k hands" /></p>
<p>At the 36th Chaos Communication Congress I organized a workshop called “Palm Reading AI” where I invited visitors to read the hands of people from the 11k hands dataset. At first I only introduced them to palmistry using wikihow as a reference.<a href="#fn98" class="footnote-ref" id="fnref98" role="doc-noteref"><sup>98</sup></a> Then I handed out some prints where a random hand from the dataset was depicted and participants had to fill in a couple of questions. Some questions were short guesses like age, gender, country of origin, for some other they had to come up with fictional stories with only the hand lines as a reference: what is the persons future? How was their childhood? How is their love life? After they filled in the form some people shared their stories and then I revealed where these hands came from and how computer scientists are using them to create models that try to predict their gender. Afterwards we had a discussion about the practice of creating large datasets and their ethical considerations. I had a longer talk with one participant that did not want to guess the age or gender of that person and I had told them that this was exactly the point of the workshop: to reflect on our own biases and stereotypes and how they translate into science.</p>
<p>After long contemplation on 11k hands and finding datasets that are much more problematic than this one, I don’t think the type of work from Afifi et al. is ‘unethical’ or needs to be redacted. They got consent from their subjects and share the dataset to the scientific community for “reasonable academic fair use.” The work on biometric identification and comparing CNNs to previous methods is interesting and novel, however as I stated above I think the premise behind the gender recognition task is flawed. Unfortunately this is very common in the computer vision field where people are (mis-)labeled that reflect and amplify societal stereotypes.<a href="#fn99" class="footnote-ref" id="fnref99" role="doc-noteref"><sup>99</sup></a> My research on the hands dataset in conjunction with esoteric practices and fortune-telling informed a later work of mine “The Chiromancer” that I built together with Giacomo Piazzi.</p>
<h2 id="in-the-wild">In the wild</h2>
<p>The process of collecting and creating data has drastically changed since the wide adoption of the internet. Where the 11k hands dataset has invited participants to their instute to take a picture specifically for the dataset, other researcher started to search and download huge collections from the internet without any consent. Take for example the ImageNet dataset, initiated by Stanford University’s AI professor Fei Fei Li, which was created to tackle object recognition tasks and eventually consisted of 14 million images.<a href="#fn100" class="footnote-ref" id="fnref100" role="doc-noteref"><sup>100</sup></a> The team queried multiple search engines with common nouns and multiple translations to get around 500-1000 images per category. The categories are derived from an older project called WordNet, created in 1985 at Princeton University, that tried to achieve a hierarchical ontology of words. For example the noun <em>human</em> is synonym to <em>homo, man and human being</em> which are subcategories of the <em>hominid</em> class, which are <em>primates</em>, which are <em>mammals</em>, which are <em>animals</em>, which is an <em>organism</em> and finally part of the <em>entity</em> class. After downloading millions of images in thousands of categories they were first automatically deduplicated but the task of checking whether the image actually depicts the search term could not be done by an algorithm. Originally Fei Fei Li wanted to hire graduate students to sort the data which was too costly and time consuming.<a href="#fn101" class="footnote-ref" id="fnref101" role="doc-noteref"><sup>101</sup></a> One of her undergraduates suggested to use a new service called Amazon Mechanical Turk where people around the world complete small task for little compensation. Even with the help of Mechanical Turk it took 2,5 years to sort and validate the first dataset with 3.2 million images in over 5000 categories. At some point ImageNet was the largest academic client for Amazon’s Turk service and after the popularization of weighted networks they became a corner stone of data annotation. Similar platforms were established and Mechanical Turk grew to an estimate of half a million workers, which were rigorously exploited and alienated as a so called click workers. One paper estimates their median hourly wage to 2$ and only the top 4% earning more than 7.25$ per hour.<a href="#fn102" class="footnote-ref" id="fnref102" role="doc-noteref"><sup>102</sup></a> If the task is rejected by the requester the worker does not get paid at all and has no way of appealing that decision. To advocate for better working conditions the designer and researcher Caroline Sinders built an open source tool for data annotation and a wage calculator to better estimate the cost of labeling.<a href="#fn103" class="footnote-ref" id="fnref103" role="doc-noteref"><sup>103</sup></a><br />
With so many different people working on labeling images, societal biases are eventually reflected in the dataset. In 2019 the artist Trevor Paglen and researcher Kate Crawford collaborated on an exhibition titled <em>Training Humans</em>, dedicated to human image datasets. One of the main exhibits was a vast collection of human images from ImageNet.<a href="#fn104" class="footnote-ref" id="fnref104" role="doc-noteref"><sup>104</sup></a><br />
As you go further down the category tree you might find images of people that fall into categories such as “Bad Person, Call Girl, Drug Addict, Closet Queen, Convict” and so on.<a href="#fn105" class="footnote-ref" id="fnref105" role="doc-noteref"><sup>105</sup></a> The artists used these absurd, racist, and misogynistic labels to train <em>ImageNet Roulette</em>,<a href="#fn106" class="footnote-ref" id="fnref106" role="doc-noteref"><sup>106</sup></a> a recognition algorithm that was accessible online and in an interactive installation. People online quickly picked up on the tool and shared images of themselves with sometimes amusing and often deeply problematic image captions.<a href="#fn107" class="footnote-ref" id="fnref107" role="doc-noteref"><sup>107</sup></a> While some of the people in the research community defended ImageNet that these offensive labels are not part of competition and make up only a small fraction of the total dataset, as a result of the media attention that followed more than 600,000 images were removed. It is now only accessible after proving to be part of a scientific institute.</p>
<p>Another pair of artists and researchers, Adam Harvey and Jules LaPlace, have been exposing image sets which often get revoked and removed after publishing their investigation.<a href="#fn108" class="footnote-ref" id="fnref108" role="doc-noteref"><sup>108</sup></a> Harvey and LaPlace focus on datasets with faces that are captured “in the wild.” One particular example was the <em>UnConstrained College Students (UCCS)</em> dataset captured at the University of Colorado Colorado Springs. According to the authors of the associated papers to the dataset they identified that current image sets created for face recognition research did not address the presence of unknown subjects. The authors of the papers associated with this dataset wanted to create a more realistic benchmark for face recognition research by introducing unknown subjects over time. To achieve this, they captured students on campus during breaks using a 800mm telephoto lense from a distance of 100-150 meters through an office window. The authors frame it as a benefit that the students are unaware of their capturing for the dataset as they casually focus on random activities and their faces are sometimes blurred and occluded. To make things worse this research was mainly funded by US defense and intelligence agencies. The public backlash was immense and as a result the dataset is no longer publicly available. The researchers in question did not apologize and argued that their subjects are anonymous as their names or other identifiers are not published.<a href="#fn109" class="footnote-ref" id="fnref109" role="doc-noteref"><sup>109</sup></a> However when researchers need unconstrained and non-consensual data, they do not often capture them directly using creepy surveillance tactics. Starting with “Labeled Faces in the wild” in 2007 the practice of collecting and labeling image data from internet sources has become normalized and is still largely unregulated. Often these image sets “in the wild” operate in a gray zone where they either depict public figures, arguing that privacy regulations do not apply to them, or that they simply link to the file and only store them temporarily for analysis. A third option uses media licensed under Creative Commons (CC) which is mostly considered free and legal data with no restrictions in the AI research community.<a href="#fn110" class="footnote-ref" id="fnref110" role="doc-noteref"><sup>110</sup></a> In a detailed report Adam Harvey lists many datasets that exploit and often misuse CC licenses to create face and object recognition datasets.<a href="#fn111" class="footnote-ref" id="fnref111" role="doc-noteref"><sup>111</sup></a> He identifies Flickr as a major source for collecting images, a popular image sharing website where users can choose from various Creative Commons, copyright and public domain licenses. Flickr actively promoted the use of CC licenses and offered unlimited free hosting if a CC license is used. Their strategy worked and by 2022 they amassed 467 million CC licensed images. In 2014 a joint research group including Yahoo Labs, the company that bought Flickr, shared one of the largest public media datasets<a href="#fn112" class="footnote-ref" id="fnref112" role="doc-noteref"><sup>112</sup></a> with the name <em>Yahoo! Flickr Creative Commons 100 Million (YFCC100M)</em>. A multitude of specified datasets were created from this corpus. One of them is the <em>MegaFace</em> face recognition dataset with 4,7 million faces from 672,057 identities. While all of the images fall under a CC license most of them prohibit their commercial use and require appropriate attribution, which was not given in the dataset. As Harvey and LaPlace verified, the <em>MegaFace</em> dataset was used by globally by commercial, military and academic organizations.<a href="#fn113" class="footnote-ref" id="fnref113" role="doc-noteref"><sup>113</sup></a> As an investigative article from the New York Times explains, most people are unaware that their images are powering face recognition research around the world, including companies with ties to Chinas surveillance on the Uyghur population.<a href="#fn114" class="footnote-ref" id="fnref114" role="doc-noteref"><sup>114</sup></a> To conclude the three common issues with CC licensed media is that they are not or wrongly attributed, the use of non-consensual biometric data is prohibited in some places (e.g. Illinois) and the use in commercial applications is often prohibited. Many of thes issues identified by Harvey and LaPlace remain until today and operate in a legal grey zone. While the notion behind uploading images under an open license for others to freely share, distribute and remix media is a noble goal, the CC license is legally weak and practically useless against opting out of statistical analysis in AI research. But we might be slowly moving into a direction where law makers catch up and create precedences that disallow the use of biometric data in certain applications, like the upcoming AI Act.<a href="#fn115" class="footnote-ref" id="fnref115" role="doc-noteref"><sup>115</sup></a></p>
<h2 id="this-person-does-exist">This Person Does Exist</h2>
<p>A dataset using Creative Commons images that I examined more closely was <em>Flickr Faces HQ (FFHQ)</em>. In 2018 the research lab of Nvidia, one of the leading companies for visual computing, published a paper introducing a machine learning architecture called StyleGAN.<a href="#fn116" class="footnote-ref" id="fnref116" role="doc-noteref"><sup>116</sup></a> They improved on generative adversarial networks (GAN) in such a way that it was possible to create controllable synthetic high-resolution images. In simple terms the system is able to abstract large amounts of images with a model that in turn outputs similar-looking pictures. In this case the model is able to generate realistic-looking photographs of human faces. In comparison to other datasets <em>FFHQ</em> is fairly small with only 70,000 images. As existing datasets were too low in resolution a new corpus was created by scraping Creative Commons, Public Domain or U.S. Government Works licensed images through Flickr’s API. The dataset itself is published under a CC-BY-NC-SA license and the instructions for use and download are very clear, making it manageable for me in terms of size and effort to discover the underlying characteristics. The dataset consists only of photographic images as it was checked by Amazon Mechanical Turk workers to make sure that statues, paintings, or photos of photos were removed. Using the open-source library dlib, the raw images were automatically aligned and cropped around the face to create a uniform square ratio of 1024px. This library also identifies 68 points outlining the chin, eyebrows, eyes, nose and mouth which are included in the metadata. So the final dataset used for training the GAN consists of a multitude of faces whe the eye and mouth positions are always in the same spot.</p>
<figure>
<img src="Screen_Shot_2023-03-15_at_20.50.49.png" alt="Screenshot of This Person Does Exist" /><figcaption aria-hidden="true">Screenshot of This Person Does Exist</figcaption>
</figure>
<p>One year after Nvidia released its StyleGAN paper the software developer Phillip Wang published the website <em>thispersondoesnotexist.com</em> which shows the capabilities of the generative model to create realistic looking photographs of people.<a href="#fn117" class="footnote-ref" id="fnref117" role="doc-noteref"><sup>117</sup></a> The site quickly took off and alarmed people about potential impact of AI systems in generating cheap synthetic media.<br />
As a counter-narrative to the AI image creator, I wanted to showcase the people who were used to train this system. In 2020 I moved the cropped and aligned face images to my server and built my own website with the name <em>this-person-does-exist.com</em> which displays the faces from the training set alongside their metadata.<a href="#fn118" class="footnote-ref" id="fnref118" role="doc-noteref"><sup>118</sup></a> Looking at the individuals faces facilitates an interpersonal connection with the unknown person and evokes a feeling for the images that were used to train the generative model. At the same time it shows the creepy and strange practice of AI researchers using personal images as raw data. As Flickr is used mostly by hobbyists and professional photographers, one can find portraits of children and families, speakers at conferences or people on holiday. We can assume that, like other scraped datasets, the photographers of these images are unaware that their images are used for AI analysis. In contrast to other scraped datasets, the Nvidia researchers provide a tool to see if an image is part of the collection and allow the removal of the photograph.<a href="#fn119" class="footnote-ref" id="fnref119" role="doc-noteref"><sup>119</sup></a> According to Adam Harvey the company does not disclose if any images were requested for removal and has not updated or removed any photos since its release in 2016.<a href="#fn120" class="footnote-ref" id="fnref120" role="doc-noteref"><sup>120</sup></a></p>
<p><img src="Fig2.png" alt="70k faces from FFHQ compiled into one image" /> <img src="Fig3.png" alt="The Flickr Face - averaged FFHQ dataset" /></p>
<p>To get a sense of scale of the dataset, I compiled all face images into a grid, reducing the size of each image to 16 by 16px. This simple montage makes it possible to get a feeling for the vast amount of normalized image data. The authors claim that FFHQ includes more variety than other face image sets in terms “of age, ethnicity and image background, and also has much better coverage of accessories such as eyeglasses, sun-glasses, hats, etc.”<a href="#fn121" class="footnote-ref" id="fnref121" role="doc-noteref"><sup>121</sup></a> They admit to biases inherited from the Flickr platform but fail to mention them. Looking through the data it becomes clear that a majority of the images are taken with high resolution digital camera systems under good lighting conditions. Many of the pictures show separate the subject with a blurry background, so called ‘bokeh,’ and conform to photographic aesthetic norms of the early 2000s until today. Indeed, the dataset contains a variety of skin tones, age groups and background but they are not equally distributed. To visually find a bias in image sets I averaged the pixel values of the images. This suppresses outliers, but it allows us to see an overall trend of the dataset. The resulting composite image The Flickr Face, I belief, reveals a trend towards smiling and light-skinned people in the data set. All these are highly subjective impressions, but I would not presume any ethnicity or categorize people by skin tone. I understand that this project is doing the same as the research labs in question, but I hope by making people uncomfortable and showing the human behind AI systems, we can get a better feeling for how creepy this harvesting of faces is.</p>
<p>As a part of a growing group of artists exploring and exposing research datasets, I coined a term for this genre: Dataset Art. My paper on this subject was published in <em>Temes de Disseny</em> in 2021 and includes a couple more examples. Through their works, these artists are able to make large datasets understandable and captivating. Their art has sometimes even created enough attention to lead some institutions to remove questionable parts or entire collections. Whether through galleries or online, Dataset Art is providing an exciting new way to peek into the workings of AI systems.<a href="#fn122" class="footnote-ref" id="fnref122" role="doc-noteref"><sup>122</sup></a></p>
<h2 id="scrapism">Scrapism</h2>
<p>Web scraping is the technique of using computer programs to automatically visit links and aggregate data from the internet. It is the backbone for many of the current machine learning applications. The artist Sam Lavigne uses the practice of web scraping with goal of creating art with an emotional or critical message; a practice he calls ‘Scrapism.’<a href="#fn123" class="footnote-ref" id="fnref123" role="doc-noteref"><sup>123</sup></a> Perhaps an early work using the internet as a data source to convey emotion is <em>Listening Post</em> by Mark Hansen and Paul Ruben. The work, that got awarded the Golden Nica in 2004, uses snippets from internet chat rooms and displays them on over 200 LED signs while a crude computer voice reads them out loud. The artists are transforming live data into a sensual experience where the viewer gets to enter a dark room and listen in to the worlds chatter happening simultaneously all over the planet.<a href="#fn124" class="footnote-ref" id="fnref124" role="doc-noteref"><sup>124</sup></a> Instead of using and exposing datasets made for scientific research, Lavigne creates his own datasets by downloading and analyzing materials on the internet to revert common power structures. For example the online art work <em>New York Apartment</em> he produced together with Tega Brain for the Artport Collection of the Whitney Museum.<a href="#fn125" class="footnote-ref" id="fnref125" role="doc-noteref"><sup>125</sup></a> In this work they collected all for-sale listings on multiple real estate websites for New York City apartments and created a website that compiles all of them into one giant apartment listing. The value for this fictional apartment is over 43 billion USD and boasts 65,674 bedrooms and 55,588 bathrooms on around 3.4 million square meters. The website consists of multiple columns describing the listing with all of the clichéd language and staged photographs common in the real estate market. They extruded the floor plans into 3D models and placed them together next to each other, in a tower or pyramid formation so that you can explore this maze of apartments. The videos are cut up into thematic categories like “Welcome,” “Bedroom,” “Master” or “Pre-war” creating strange super cuts of panning and zooming shots of slick interiors. Although looking through this compilation is funny and entertaining, it reveals the absurd language of luxury commodities and reveals the inequality of who can afford to own housing in a city like New York. Another experiment by Sam Lavigne uses an open source hair detector to create a compilation of Mark Zuckerberg hair styles, in reaction to multiple people mocking the billionaire for his ‘terrible’ hair style.<a href="#fn126" class="footnote-ref" id="fnref126" role="doc-noteref"><sup>126</sup></a> This project in particular surved me as an inspiration as it shows that using web scraping and machine learning technologies can sometimes just be for silly projects on the expense of billionaires funding this tech for surveillance and personal profits.</p>
<figure>
<img src="lavigne_zuck-cut.jpg" alt="Sam Lavigne - Zuckerhair" /><figcaption aria-hidden="true">Sam Lavigne - Zuckerhair</figcaption>
</figure>
<h2 id="hidden.pictures">hidden.pictures</h2>
<p>When downloading things from the web, we often assume that everything is stored for ertenity. “The web never forgets” is a common phrase that is used to warn people before uploading sensitive and personal content online. This can be true for content that is widely shared or automatically scraped and stored on sites like archive.org, but for many files and links the web is brittle. Domain names expire and cut the link to the requested page. But even if the hypertext can be accessed they often contain links to files that do not exist on the server anymore, in the case of images the browser then decides to show a broken image icon and an empty rectangle. For the online artwork <em>hidden.pictures</em> (previously called <em>empty.photos</em>) I created a web crawler that visits random URLs and whenever it hits an image that did not load correctly it gets stored into a database. I collected thousands of broken image links together with their metadata that sometimes describes the image.<a href="#fn127" class="footnote-ref" id="fnref127" role="doc-noteref"><sup>127</sup></a> I present the viewer with a collage of randomly placed image rectangles on a blank white website. A visitor can pan around in all directions finding the default rendering of their web browers for broken images, often showcasing a ripped paper icon on the top left corner. When hovering over one of the boxes the original url is shown on the bottom left corner of the page and whenever it existed the alt-tag pops up next to the cursor. These images from thousands of blogs, shops and forums show the forgotten and neglected part of the internet. It invites for the imagination about the internet that is not getting maintained and reveals an even bigger source of data that does not exist anymore. When looking at this through the lense of weighted networks and training data, we have to ask the question what these models are actually learning when so many things get missing on the net every day. On the other hand the models themselves become a blurry snapshot of the things they were able to gather, but might not exist in the future.</p>
<figure>
<img src="emptyphotos-IMG_2261-HDR_1.jpg" alt="empty.photos exhibited at Best Off 2019" /><figcaption aria-hidden="true">empty.photos exhibited at Best Off 2019</figcaption>
</figure>
<h2 id="doggg.art">doggg.art</h2>
<p>In 2020 I created doggg.art, an exercise in <em>scrapism</em> where I downloaded and transformed content from the social media giant Instagram. Instagram has become the biggest tool for artists to find an audience and a community. Every possible niche can be found through the use hashtags like #dogart that collects drawings, photographs of personal and commercial dog related imagery. With around a million posts it is only one example for the immense creative output on the image platform. Facebook, the mother company of Instagram, owns an abundant amount of image data which they analyze and use to optimize computational models. By posting on the platform a user “grant[s] a non-exclusive, royalty-free, transferable, sub-licensable, worldwide license to host, use, distribute, modify, run, copy, publicly perform or display, translate, and create derivative works of [their] content.”<a href="#fn128" class="footnote-ref" id="fnref128" role="doc-noteref"><sup>128</sup></a> I would describe doggg.art as a generative big dada collage consisting of over 30k images from Instagram tagged with #dogart. The images were processed using a machine learning algorithm called U^2-Net, which removed the background and other elements from the pictures. A website then randomly places the cutouts on a beige background slowly fading them in, creating an ever changing digital dog meadow. I see it as a collaborative work that combines pieces by 38326 artists together credited on a seperate page with all their unique usernames.<a href="#fn129" class="footnote-ref" id="fnref129" role="doc-noteref"><sup>129</sup></a> The artwork was intended as a screensaver that reflects on on the aesthetics of the platform Instagram and how our relationship with pets extends into this online network.</p>
<figure>
<img src="doggg.art_Digitale.jpeg" alt="doggg.art exhibited at Die Digitale Düsseldorf exhibition “Digital Jokes”" /><figcaption aria-hidden="true">doggg.art exhibited at Die Digitale Düsseldorf exhibition “Digital Jokes”</figcaption>
</figure>
<figure>
<img src="doggg.art_Digitale_2.jpeg" alt="A visitor looking through the names of dog artists" /><figcaption aria-hidden="true">A visitor looking through the names of dog artists</figcaption>
</figure>
<h2 id="a-study-on-the-characteristics-of-douyin-meanwhile-in-china">A study on the Characteristics of Douyin / Meanwhile in China</h2>
<p>Working with and about social networks has been a big part in my artistic work. In 2019 I was invited by UNESCO to participate in a residency in the city of Changsha, the capital of the Hunan region and “City of Media Arts” in central China. While I was there I tried to make sense of the information landscape around me and got hooked to the chinese clone of TikTok. Douyin 抖音 became one of the most successful apps worldwide as the leading platform for creating and sharing short videos. Created by the Beijing based company Bytedance, it is one of the few Apps that got successful outside of the great firewall. To comply with chinese law Douyin is a completely separate App from TikTok. Even though the interface and logo looks the same, the content is completely different, not accessible from the international version. I asked S()fia Braga to join me as a collaborator, inspired by the work of <em>I stalk myself more than I should</em> where she captures Instagram stories, a feature designed to expose a video for a limited duration of 24 hours, and reappropriates them in a video installation. The aim of our work was to explore and analyze the vast digital ecosystem of Douyin from different perspectives, using their recommendation algorithm to lead us to different aspects of chinese social media. Using screen recordings to capture hours of video footage of us scrolling through our feeds. These found images are then decontextualized without alteration to give visitors the space to reflect upon them, gain insight into a walled off platform and into the algorithms designed for user engagement.<br />
We made two video installation, running an 8 hours of captured material, showing people dancing next to chinese police forces schowcasing their equipment. Sometimes we would show the blank interface showing blank search results. Search terms like Donald Trump result in videos that do not show the american president them self, indicating facial recognition being used for censorship. Additionally to the unaltered screen captured we created non-nonsensical graphs that invoke the feeling that the content is used for statistical analysis. The title <em>A study on the characteristics of Douyin</em> was taken from a sociological paper and gives the veneer of scientific legitimacy to the non-consensual stalking of capturing of our practice. In another exhibition at the Ars Electronica Festival in Linz the titled was changed to <em>Meanwhile in China</em> and the wall behind the video screen was covered with a collage of graphs and datapoints found online showcasing the exploding growth and user distribution on the platform. However the graphs themselves are stripped of any labeling, making them unreadable and useless as visualization.</p>
<figure>
<img src="IMG_1853.jpg" alt="A Study on the Characteristics of Douyin at Xie Zielong Photography Museum in Changsha" /><figcaption aria-hidden="true">A Study on the Characteristics of Douyin at Xie Zielong Photography Museum in Changsha</figcaption>
</figure>
<figure>
<img src="meanwhile_landscape.jpg" alt="Meanwhile In China at Ars Electronica 2019" /><figcaption aria-hidden="true">Meanwhile In China at Ars Electronica 2019</figcaption>
</figure>
<h2 id="recommended-hashtags-more-of-the-same">Recommended Hashtags &amp; More of the same</h2>
<p>I have had an account on Instagram since 2012 shortly after it was released on android devices. The social network was based around square images, often highly edited with filters that are inspired by old analog photos. When I joined Instagram I already had a critical view of large social networks like facebook where I had started to create online and offline performances. On Instagram I would often share references to technologies in the social sphere and where they break, like screenshots of loading spinners, weird advertisement or comments by bots. To get the attention outside of my friend network and reach even more bots I often used generic hashtags. The hashtag originated from the IRC protocol where # was used before the name of a chat room (channel) and became a popular way on twitter for tagging events, movements or topics from 2007 onwards.<a href="#fn130" class="footnote-ref" id="fnref130" role="doc-noteref"><sup>130</sup></a> Today it is a ubiquitous way of organizing and tagging content into categories, but it is mostly used to get attention inside of the network. For this, the game on social media is to add as many relevant and trending topics into the description as possible and many companies have sprung up to help users ‘optimize’ their hashtags. From 2015 I started to use hashtags for non-sensical and humorous image describtion and later on decided to use hashtag generators to create lists of irrelevant tags for my posts on Instagram as social commentary. In 2020 just after the first summer of the new pandemic reality S()fia Braga organized the <em>Internet Yami Ichi</em>, a black market for internet inspired products invented by the japanese art collective IDPW.<a href="#fn131" class="footnote-ref" id="fnref131" role="doc-noteref"><sup>131</sup></a> For this flea market I decided to print and sell the 5 years of my Instagram performance with a book titled <em>Recommended Hashtags</em>. Each of the 216 pages containing the image, hashtags and number of likes. It also had a dedication to all my followers at the end page. After downloading and printing my Instagram data, I deleted all my images on the platform and added the label ‘Post-Instagram Artist’ to my account. As users of Facebook’s platforms, we are aware that our content is being analyzed for profit. With my online performance, I aimed to subvert and add noise to the system, albeit not enough to make a real difference. Selling my book was a way to compensate for the ‘work’ I have been doing for Facebook. The book was sold in a staggering model where each edition doubled in cost from the previous one, making my old data more valuable over time, while it was completely free at the time of creation. In the following year 2021 I started a new performance titled <em>more of the same</em> on my account using the dataset I downloaded as source material for a generative adversarial network. I trained a StyleGAN model, which was originally invented to create synthetic human faces, on the images I posted from 2015 to 2020. The model however created abstract shapes, colors and textures instead of replicating my images. As ~200 pictures are a very small dataset and the images are not very uniform ranging from screenshots to photographs depicting objects or people the model could not converge on any meaningful representation. Many of the generated images are nearly symmetrical, reminiscent of Rorschach inkblots, which is the result of augmenting my image dataset by flipping the images. They also show circular blobs which is the result of the model architecture and is seen as an error of the weighted network.<a href="#fn132" class="footnote-ref" id="fnref132" role="doc-noteref"><sup>132</sup></a> In the end I wrote a small program that uploaded and posted a new generated image with the caption “more of the same” for 100 days.</p>
<figure>
<img src="more_of_the_same.png" alt="Screenshot of more of the same | 200" /><figcaption aria-hidden="true">Screenshot of more of the same | 200</figcaption>
</figure>
<p><img src="Recommended_Hashtags_0.jpg" alt="Book cover of Recommended Hashtags | 200" /> <img src="Recommended_Hashtags_1.jpg" alt="First page of Recommended Hashtags | 200" /></p>
<h1 id="stochastic-meaning">Stochastic Meaning</h1>
<pre><code>What I had not realized is that extremely short exposures
to a relatively simple computer program could induce powerful de-
lusional thinking in quite normal people.[^1]</code></pre>
<h2 id="symbolic-language">Symbolic Language</h2>
<p>Understanding language is an essential aspect for computer assistants to be truly useful. However, language is a challenging area since words can have various meanings depending on the context. Furthermore, there are multiple languages and different ways of converting spoken language into written symbols.</p>
<p>The use of computers for natural language processing (NLP) has been a subject of research since the invention of computers. One of the earliest examples of symbolic NLP is ELIZA, a computer program developed by Joseph Weizenbaum in the 1960s that simulated a conversation by rephrasing the input to form questions. ELIZA used pattern matching and substitution to generate responses based on the user’s input, giving the impression of understanding what was being said. For example, if the user said “I feel sad,” ELIZA might respond with “Why do you feel sad?” or “Tell me more about your feelings.”</p>
<p>While ELIZA was fairly limited in its capabilities, it sparked the interest and imagination of the general public which saw the toy program as a breakthrough in artificial intelligence. Many people were fascinated by the idea that a machine could understand and respond to human language, even if the responses were simple and scripted. Joseph Weizenbaum himself was shocked about the willingness of people to attribute consciousness to even the most simple programs, like the 200 lines of code he wrote for the DOCTOR program that simulated a Regorian psychotherapist. In a famous anecdote Weizenbaum explains that his office assistant, who was quite familiar with the program, demanded for him to leave the office because she was sharing intimate thoughts with the computer. Some psychologists even considered using such a computer program to be able to treat more people in a shorter time, which for Weizenbaum was a horrendous thought and finally made him doubt the future of humanity and computers. In his 1976 book “Computer Power and Human Reason,” Weizenbaum argues that the increasing reliance on computers to solve problems and make decisions is a dangerous trend that could lead to dehumanization and loss of autonomy of human beings. Conflating the computer with the human mind ultimately reduces the human to a computational being. He warns that we should not confuse the power of machines with human intelligence, creativity, and empathy, which are essential for ethical and meaningful interactions.<a href="#fn133" class="footnote-ref" id="fnref133" role="doc-noteref"><sup>133</sup></a></p>
<p>ELIZA paved the way for further development in natural language interaction. In the 1970s some programmers created text-based games like the game <em>Colossal Cave Adventure</em> (1976) which allowed users to interact with the game world using natural language commands. At the same time the use of conceptual ontologies, which are structured representations of knowledge in a particular domain became popular. These ontologies can be used to help computers understand natural language by mapping words and phrases to their corresponding concepts in the ontology. For example, if someone says “I want to buy a red car,” an ontology could map “red” and “car” to their corresponding concepts and infer that the person is interested in purchasing a vehicle with a specific color. One of the biggest endeavors in mapping words to concepts was the creation of WordNet in the 1980s, which is a lexical database of English words and their semantic relationships.</p>
<p>Even until the early 2000s programmers came up with larger and more complex rule-based chatbots. Creating systems like A.L.I.C.E. (1995) which used complex set of rules to simulate conversation with users. However, these systems were limited by their reliance on pre-programmed responses and lacked the ability to learn and adapt to new situations. The web application Cleverbot (2008) and it’s predecessor jabberwacky (started in 1988, online since 1997) shifted static databases to one that is constantly growing.<a href="#fn134" class="footnote-ref" id="fnref134" role="doc-noteref"><sup>134</sup></a> By capturing every conversation and creating links of question-answer pairs, the system could use a fuzzy search algorithm to present the user with a likely response that was typed by another human previously.</p>
<p>While symbolic NLP methods have been successful in some applications (such as chatbots and question-answering systems), they have limitations when it comes to handling ambiguity and understanding human emotions or nuances in language. As a result, more recent approaches to NLP have focused on machine learning techniques that allow computers to learn from large amounts of data without relying on pre-defined rules or ontologies.</p>
<h2 id="attention-is-all-you-need">Attention is all you need</h2>
<ul>
<li>Prayers to GPT ## Computing Reality</li>
<li>The Chiromancer</li>
</ul>
<p>Thank you to my supervisors <em>Manuela Naveau</em>, <em>Christa Sommerer</em> and <em>Laurent Mignoneau</em>. A special thanks to <em>Davide Bevilacqua</em> and the <em>Servus.at</em> association for helping me realize the Silicon Friend Camp, exhibition and symposium.</p>
<p>Thank you to <em>Bani Budassin</em> who invited me to write for Temes des Disseny which formed my thoughts around Dataset Art.</p>
<p>And of course to all my new and old friends, my fellow contemporaries. I am because you are.</p>
<p>(Infrastructure???)</p>
<ul>
<li>Infields / Cloud</li>
<li>Cybersquat</li>
</ul>
<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-2ndUnconstrainedFace" class="csl-entry" role="doc-biblioentry">
<span>“2nd <span>Unconstrained Face Detection</span> and <span>Open Set Recognition Challenge</span>, <span>ECCV-2018</span>.”</span> n.d. Accessed March 11, 2023. <a href="https://vast.uccs.edu/Opensetface/">https://vast.uccs.edu/Opensetface/</a>.
</div>
<div id="ref-afifi11KHandsGender2018" class="csl-entry" role="doc-biblioentry">
Afifi, Mahmoud. 2018. <span>“<span>11k Hands</span>: <span>Gender</span> Recognition and Biometric Identification Using a Large Dataset of Hand Images.”</span> September 16, 2018. <a href="https://doi.org/10.48550/arXiv.1711.04322">https://doi.org/10.48550/arXiv.1711.04322</a>.
</div>
<div id="ref-afifi11kHands" class="csl-entry" role="doc-biblioentry">
———. n.d. <span>“11k <span>Hands</span>.”</span> Accessed March 7, 2023. <a href="https://sites.google.com/view/11khands">https://sites.google.com/view/11khands</a>.
</div>
<div id="ref-amodeiDeepSpeechEndtoEnd2015" class="csl-entry" role="doc-biblioentry">
Amodei, Dario, Rishita Anubhai, Eric Battenberg, Carl Case, Jared Casper, Bryan Catanzaro, Jingdong Chen, et al. 2015. <span>“Deep <span>Speech</span> 2: <span class="nocase">End-to-End Speech Recognition</span> in <span>English</span> and <span>Mandarin</span>.”</span> <a href="http://arxiv.org/abs/1512.02595">http://arxiv.org/abs/1512.02595</a>.
</div>
<div id="ref-AnatomyAISystem2018" class="csl-entry" role="doc-biblioentry">
<span>“Anatomy of an <span>AI System</span>.”</span> 2018. <span>Anatomy of an AI System</span>. 2018. <a href="http://www.anatomyof.ai">http://www.anatomyof.ai</a>.
</div>
<div id="ref-ArsElectronicaArchiv" class="csl-entry" role="doc-biblioentry">
<span>“Ars <span>Electronica Archiv</span>.”</span> n.d. Accessed March 20, 2023. <a href="https://archive.aec.at/prix/showmode/88/">https://archive.aec.at/prix/showmode/88/</a>.
</div>
<div id="ref-Avatars" class="csl-entry" role="doc-biblioentry">
<span>“Avatars.”</span> n.d. <span>So Kanno</span>. Accessed April 28, 2022. <a href="https://www.kanno.so/project/avatars">https://www.kanno.so/project/avatars</a>.
</div>
<div id="ref-Babycastles" class="csl-entry" role="doc-biblioentry">
<span>“Babycastles.”</span> n.d. <span>Babycastles</span>. Accessed January 30, 2022. <a href="https://www.babycastles.com">https://www.babycastles.com</a>.
</div>
<div id="ref-baileyDeepDreamCreatorUnveils" class="csl-entry" role="doc-biblioentry">
Bailey, Jason. n.d. <span>“<span>DeepDream Creator Unveils Very First Images After Three Years</span> — <span>Artnome</span>.”</span> Accessed March 4, 2023. <a href="https://www.artnome.com/news/2018/12/30/deepdream-creator-unveils-very-first-images-after-three-years">https://www.artnome.com/news/2018/12/30/deepdream-creator-unveils-very-first-images-after-three-years</a>.
</div>
<div id="ref-BetterImagesAI" class="csl-entry" role="doc-biblioentry">
<span>“Better <span>Images</span> of <span>AI</span>.”</span> n.d. Accessed March 4, 2023. <a href="https://betterimagesofai.org">https://betterimagesofai.org</a>.
</div>
<div id="ref-bostromAreYouLiving2001" class="csl-entry" role="doc-biblioentry">
Bostrom, Nick. 2001. <span>“Are <span>You Living</span> in a <span>Computer Simulation</span>?”</span> 14. <a href="https://www.simulation-argument.com/simulation.pdf">https://www.simulation-argument.com/simulation.pdf</a>.
</div>
<div id="ref-chakelianJourneyManyFaces2021" class="csl-entry" role="doc-biblioentry">
Chakelian, Anoosh. 2021. <span>“The Journey and Many Faces of the Hash Symbol.”</span> <span>New Statesman</span>. June 9, 2021. <a href="https://www.newstatesman.com/science-tech/2014/06/history-journey-and-many-faces-hash-symbol">https://www.newstatesman.com/science-tech/2014/06/history-journey-and-many-faces-hash-symbol</a>.
</div>
<div id="ref-ciresanFlexibleHighPerformance" class="csl-entry" role="doc-biblioentry">
Ciresan, Dan C, Ueli Meier, Jonathan Masci, Luca M Gambardella, and Jurgen Schmidhuber. n.d. <span>“Flexible, <span>High Performance Convolutional Neural Networks</span> for <span>Image Classification</span>.”</span>
</div>
<div id="ref-Cleverbot" class="csl-entry" role="doc-biblioentry">
<span>“Cleverbot.”</span> n.d. <span>Cleverbot</span>. Accessed March 21, 2023. <a href="http://www.cleverbot.com/">http://www.cleverbot.com/</a>.
</div>
<div id="ref-CloneSyntheticAI" class="csl-entry" role="doc-biblioentry">
<span>“Clone <span>Synthetic AI Voices</span> with <span>Neural Text</span> to <span>Speech</span>.”</span> n.d. <span>Resemble AI</span>. Accessed January 26, 2022. <a href="https://www.resemble.ai/">https://www.resemble.ai/</a>.
</div>
<div id="ref-coleThisHorrifyingApp2019" class="csl-entry" role="doc-biblioentry">
Cole, Samantha. 2019. <span>“This <span>Horrifying App Undresses</span> a <span>Photo</span> of <span>Any Woman With</span> a <span>Single Click</span>.”</span> <span>Vice</span>. June 26, 2019. <a href="https://www.vice.com/en/article/kzm59x/deepnude-app-creates-fake-nudes-of-any-woman">https://www.vice.com/en/article/kzm59x/deepnude-app-creates-fake-nudes-of-any-woman</a>.
</div>
<div id="ref-CreativeCommonsBiometrics" class="csl-entry" role="doc-biblioentry">
<span>“Creative <span>Commons Biometrics</span>.”</span> n.d. Accessed March 11, 2023. <a href="https://adam.harvey.studio/creative-commons/">https://adam.harvey.studio/creative-commons/</a>.
</div>
<div id="ref-dempsey-jonesNeuroscientistsPutDubious2018" class="csl-entry" role="doc-biblioentry">
Dempsey-Jones, Harriet. 2018. <span>“Neuroscientists Put the Dubious Theory of ’Phrenology’ Through Rigorous Testing for the First Time.”</span> <span>The Conversation</span>. January 22, 2018. <a href="http://theconversation.com/neuroscientists-put-the-dubious-theory-of-phrenology-through-rigorous-testing-for-the-first-time-88291">http://theconversation.com/neuroscientists-put-the-dubious-theory-of-phrenology-through-rigorous-testing-for-the-first-time-88291</a>.
</div>
<div id="ref-dengImageNetLargeScaleHierarchical2009" class="csl-entry" role="doc-biblioentry">
Deng, J., W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. 2009. <span>“<span>ImageNet</span>: <span>A Large-Scale Hierarchical Image Database</span>.”</span> In <em><span>Cvpr09</span></em>.
</div>
<div id="ref-DescartesWasWrong2017" class="csl-entry" role="doc-biblioentry">
<span>“Descartes Was Wrong: <span>‘A Person Is a Person Through Other Persons’</span> | <span>Aeon Ideas</span>.”</span> 2017. <span>Aeon</span>. April 7, 2017. <a href="https://aeon.co/ideas/descartes-was-wrong-a-person-is-a-person-through-other-persons">https://aeon.co/ideas/descartes-was-wrong-a-person-is-a-person-through-other-persons</a>.
</div>
<div id="ref-EverythingDLVIDOREILLY" class="csl-entry" role="doc-biblioentry">
<span>“Everything — <span>DΛVID OREILLY</span> • Computer Art &amp; Research.”</span> n.d. Accessed April 28, 2022. <a href="https://www.davidoreilly.com/everything/">https://www.davidoreilly.com/everything/</a>.
</div>
<div id="ref-ExcavatingAI" class="csl-entry" role="doc-biblioentry">
<span>“Excavating <span>AI</span>.”</span> n.d. <span>-</span>. Accessed March 10, 2023. <a href="https://excavating.ai">https://excavating.ai</a>.
</div>
<div id="ref-ExploringPossibilitySpace" class="csl-entry" role="doc-biblioentry">
<span>“Exploring <span>Possibility Space</span>: <span>Microsoft</span> #<span>TAYFAIL Smoking Gun</span>: <span>ALICE Open Source AI Library</span> and <span>AIML</span>.”</span> n.d. Accessed March 21, 2023. <a href="https://exploringpossibilityspace.blogspot.com/2016/03/microsoft-tayfail-smoking-gun-alice.html">https://exploringpossibilityspace.blogspot.com/2016/03/microsoft-tayfail-smoking-gun-alice.html</a>.
</div>
<div id="ref-FFHQDatasetSearch" class="csl-entry" role="doc-biblioentry">
<span>“<span>FFHQ</span> Dataset Search Form.”</span> n.d. Accessed March 15, 2023. <a href="https://nvlabs.github.io/ffhq-dataset/search/">https://nvlabs.github.io/ffhq-dataset/search/</a>.
</div>
<div id="ref-francishungerArtificialIntelligenceAutomated2021" class="csl-entry" role="doc-biblioentry">
Francis Hunger. 2021. <span>“1. ’<span>Artificial Intelligence</span>’ =<span><span class="math inline">&gt;</span></span> ’ <span>Automated Pattern Recognition</span>’ 2. ’<span>Machine Learning</span>’ =<span><span class="math inline">&gt;</span></span> ’<span>Machine Conditioning</span>’ <span>OR</span> <span>‘<span>Automated Classification</span>’</span> 3. ’<span>Neural Network</span>’ =<span><span class="math inline">&gt;</span></span> ’<span>Weighted Network</span>’ 4. <span>‘<span>Deep Learning</span>’</span> =<span><span class="math inline">&gt;</span></span> <span>‘<span>Deep Conditioning</span>’</span> 5. <span>‘<span>Neuron</span>’</span> =<span><span class="math inline">&gt;</span></span> <span>‘<span>Weight</span>’</span> or <span>‘<span>Node</span>’</span>.”</span> Tweet. <span>Twitter</span>. July 9, 2021. <a href="https://twitter.com/databaseculture/status/1413462059291975680">https://twitter.com/databaseculture/status/1413462059291975680</a>.
</div>
<div id="ref-FREEPORTANATOMIESBLACK" class="csl-entry" role="doc-biblioentry">
<span>“<span>FREEPORT</span> 1: <span>ANATOMIES OF A BLACK BOX</span>.”</span> n.d. <span>Matadero Madrid</span>. Accessed April 23, 2022. <a href="https://www.mataderomadrid.org/en/calls/freeport-1-anatomies-black-box">https://www.mataderomadrid.org/en/calls/freeport-1-anatomies-black-box</a>.
</div>
<div id="ref-fukushimaCognitronSelforganizingMultilayered1975" class="csl-entry" role="doc-biblioentry">
Fukushima, Kunihiko. 1975. <span>“Cognitron: <span>A</span> Self-Organizing Multilayered Neural Network.”</span> <em>Biological Cybernetics</em> 20 (3-4): 121–36. <a href="https://doi.org/10.1007/BF00342633">https://doi.org/10.1007/BF00342633</a>.
</div>
<div id="ref-fukushimaNeocognitronHierarchicalNeural1988" class="csl-entry" role="doc-biblioentry">
———. 1988. <span>“Neocognitron: <span>A</span> Hierarchical Neural Network Capable of Visual Pattern Recognition.”</span> <em>Neural Networks</em> 1 (2): 119–30. <a href="https://doi.org/10.1016/0893-6080(88)90014-7">https://doi.org/10.1016/0893-6080(88)90014-7</a>.
</div>
<div id="ref-gadeWhatUbuntuDifferent2012" class="csl-entry" role="doc-biblioentry">
Gade, Christian B. N. 2012. <span>“What Is <span><em>Ubuntu</em></span> ,? <span>Different Interpretations</span> Among <span>South Africans</span> of <span>African Descent</span>.”</span> <em>South African Journal of Philosophy</em> 31 (3): 484–503. <a href="https://doi.org/10.1080/02580136.2012.10751789">https://doi.org/10.1080/02580136.2012.10751789</a>.
</div>
<div id="ref-GENDERDIVERSITYGastvortrag" class="csl-entry" role="doc-biblioentry">
<span>“GENDER &amp; DIVERSITY: Gastvortrag von Ekheo - Musik und Kunst Privatuniversität der Stadt Wien.”</span> n.d. Accessed January 24, 2022. <a href="https://muk.ac.at/veranstaltung/gender-diversity-gastvortrag-von-ekheo.html">https://muk.ac.at/veranstaltung/gender-diversity-gastvortrag-von-ekheo.html</a>.
</div>
<div id="ref-gershgornDataThatTransformed2017" class="csl-entry" role="doc-biblioentry">
Gershgorn, Dave. 2017. <span>“The Data That Transformed <span>AI</span> Research—and Possibly the World.”</span> <span>Quartz</span>. July 26, 2017. <a href="https://qz.com/1034972/the-data-that-changed-the-direction-of-ai-research-and-possibly-the-world/">https://qz.com/1034972/the-data-that-changed-the-direction-of-ai-research-and-possibly-the-world/</a>.
</div>
<div id="ref-haraDataDrivenAnalysisWorkers2017" class="csl-entry" role="doc-biblioentry">
Hara, Kotaro, Abi Adams, Kristy Milland, Saiph Savage, Chris Callison-Burch, and Jeffrey Bigham. 2017. <span>“A <span>Data-Driven Analysis</span> of <span>Workers</span>’ <span>Earnings</span> on <span>Amazon Mechanical Turk</span>.”</span> December 28, 2017. <a href="https://doi.org/10.48550/arXiv.1712.05796">https://doi.org/10.48550/arXiv.1712.05796</a>.
</div>
<div id="ref-harveyExposingAi" class="csl-entry" role="doc-biblioentry">
Harvey, Adam. n.d.a. <span>“Exposing.ai.”</span> <span>Exposing.ai</span>. Accessed March 11, 2023. <a href="https://exposing.ai/">https://exposing.ai/</a>.
</div>
<div id="ref-harveyExposingAiMegaFace" class="csl-entry" role="doc-biblioentry">
———. n.d.b. <span>“Exposing.ai: <span>MegaFace</span>.”</span> <span>Exposing.ai</span>. Accessed March 13, 2023. <a href="https://exposing.ai/datasets/megaface/">https://exposing.ai/datasets/megaface/</a>.
</div>
<div id="ref-hillHowPhotosYour2019" class="csl-entry" role="doc-biblioentry">
Hill, Kashmir, and Aaron Krolik. 2019. <span>“How <span>Photos</span> of <span>Your Kids Are Powering Surveillance Technology</span>.”</span> <em>The New York Times: Technology</em>, October 11, 2019. <a href="https://www.nytimes.com/interactive/2019/10/11/technology/flickr-facial-recognition.html">https://www.nytimes.com/interactive/2019/10/11/technology/flickr-facial-recognition.html</a>.
</div>
<div id="ref-hintonDeepNeuralNetworks2012b" class="csl-entry" role="doc-biblioentry">
Hinton, Geoffrey, Li Deng, Dong Yu, George Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior, et al. 2012. <span>“Deep <span>Neural Networks</span> for <span>Acoustic Modeling</span> in <span>Speech Recognition</span>: <span>The Shared Views</span> of <span>Four Research Groups</span>.”</span> <em>IEEE Signal Processing Magazine</em> 29 (6): 82–97. <a href="https://doi.org/10.1109/MSP.2012.2205597">https://doi.org/10.1109/MSP.2012.2205597</a>.
</div>
<div id="ref-hoelOperativeImagesInroads2018" class="csl-entry" role="doc-biblioentry">
Hoel, Aud Sissel. 2018. <span>“Operative <span>Images</span>. <span>Inroads</span> to a <span>New Paradigm</span> of <span>Media Theory</span>.”</span> In <em>Image – <span>Action</span> – <span>Space</span></em>, 11–28. <span>De Gruyter</span>. <a href="https://doi.org/10.1515/9783110464979-002">https://doi.org/10.1515/9783110464979-002</a>.
</div>
<div id="ref-HowReadPalms" class="csl-entry" role="doc-biblioentry">
<span>“How to <span>Read Palms</span>: 9 <span>Steps</span> (with <span>Pictures</span>).”</span> n.d. <span>wikiHow</span>. Accessed March 8, 2023. <a href="https://www.wikihow.com/Read-Palms">https://www.wikihow.com/Read-Palms</a>.
</div>
<div id="ref-HumanguidedBurritoBots2019" class="csl-entry" role="doc-biblioentry">
<span>“Human-Guided Burrito Bots Raise Questions about the Future of Robo-Delivery.”</span> 2019. <span>The Hustle</span>. June 3, 2019. <a href="https://thehustle.co/kiwibots-autonomous-food-delivery/">https://thehustle.co/kiwibots-autonomous-food-delivery/</a>.
</div>
<div id="ref-hungerTalkUnhypeAI2021" class="csl-entry" role="doc-biblioentry">
Hunger, Francis. 2021. <span>“Talk: <span>Unhype AI</span> (2021).”</span> April 28, 2021. <a href="https://www.irmielin.org/unhype-ai/">https://www.irmielin.org/unhype-ai/</a>.
</div>
<div id="ref-ImageNetRouletteTrevor" class="csl-entry" role="doc-biblioentry">
<span>“<span>ImageNet Roulette</span> – <span>Trevor Paglen</span>.”</span> n.d. Accessed March 11, 2023. <a href="https://paglen.studio/2020/04/29/imagenet-roulette/">https://paglen.studio/2020/04/29/imagenet-roulette/</a>.
</div>
<div id="ref-ImageNet" class="csl-entry" role="doc-biblioentry">
<span>“<span>ImageNet</span>.”</span> n.d. Accessed March 4, 2023. <a href="https://image-net.org/index.php">https://image-net.org/index.php</a>.
</div>
<div id="ref-InceptionismGoingDeeper2015" class="csl-entry" role="doc-biblioentry">
<span>“Inceptionism: <span>Going Deeper</span> into <span>Neural Networks</span>.”</span> 2015. June 17, 2015. <a href="https://ai.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html">https://ai.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html</a>.
</div>
<div id="ref-JabberwackyThoughtsArtificial2006" class="csl-entry" role="doc-biblioentry">
<span>“Jabberwacky - <span>About Thoughts</span> - <span>An Artificial Intelligence AI</span> Chatbot, Chatterbot or Chatterbox, Learning <span>AI</span>, Database, Dynamic - Models Way Humans Learn - Simulate Natural Human Chat - Interesting, Humorous, Entertaining.”</span> 2006. July 1, 2006. <a href="https://web.archive.org/web/20060701052309/http://www.jabberwacky.com/j2about">https://web.archive.org/web/20060701052309/http://www.jabberwacky.com/j2about</a>.
</div>
<div id="ref-jemineMasterThesisAutomatic" class="csl-entry" role="doc-biblioentry">
Jemine, C. n.d. <span>“Master Thesis : <span>Automatic Multispeaker Voice Cloning</span>,”</span> 38.
</div>
<div id="ref-jemineRealTimeVoiceCloning2022" class="csl-entry" role="doc-biblioentry">
Jemine, Corentin. (2019) 2022. <em>Real-<span>Time Voice Cloning</span></em>. <a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning">https://github.com/CorentinJ/Real-Time-Voice-Cloning</a>.
</div>
<div id="ref-jiaTransferLearningSpeaker2019" class="csl-entry" role="doc-biblioentry">
Jia, Ye, Yu Zhang, Ron J. Weiss, Quan Wang, Jonathan Shen, Fei Ren, Zhifeng Chen, et al. 2019. <span>“Transfer <span>Learning</span> from <span>Speaker Verification</span> to <span>Multispeaker Text-To-Speech Synthesis</span>.”</span> <a href="http://arxiv.org/abs/1806.04558">http://arxiv.org/abs/1806.04558</a>.
</div>
<div id="ref-karrasStyleBasedGeneratorArchitecture2019" class="csl-entry" role="doc-biblioentry">
Karras, Tero, Samuli Laine, and Timo Aila. 2019. <span>“A <span>Style-Based Generator Architecture</span> for <span>Generative Adversarial Networks</span>.”</span> March 29, 2019. <a href="https://doi.org/10.48550/arXiv.1812.04948">https://doi.org/10.48550/arXiv.1812.04948</a>.
</div>
<div id="ref-karrasAnalyzingImprovingImage2020" class="csl-entry" role="doc-biblioentry">
Karras, Tero, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. 2020. <span>“Analyzing and <span>Improving</span> the <span>Image Quality</span> of <span>StyleGAN</span>.”</span> March 23, 2020. <a href="http://arxiv.org/abs/1912.04958">http://arxiv.org/abs/1912.04958</a>.
</div>
<div id="ref-KATECRAWFORDTREVOR" class="csl-entry" role="doc-biblioentry">
<span>“<span>KATE CRAWFORD</span> | <span>TREVOR PAGLEN</span>: <span>TRAINING HUMANS</span> – <span>Fondazione Prada</span>.”</span> n.d. Accessed March 10, 2023. <a href="https://www.fondazioneprada.org/project/training-humans/?lang=en">https://www.fondazioneprada.org/project/training-humans/?lang=en</a>.
</div>
<div id="ref-krizhevskyImageNetClassificationDeep2017" class="csl-entry" role="doc-biblioentry">
Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. 2017. <span>“<span>ImageNet</span> Classification with Deep Convolutional Neural Networks.”</span> <em>Communications of the ACM</em> 60 (6): 84–90. <a href="https://doi.org/10.1145/3065386">https://doi.org/10.1145/3065386</a>.
</div>
<div id="ref-lavigneScrapism" class="csl-entry" role="doc-biblioentry">
Lavigne, Sam. n.d. <span>“Scrapism.”</span> Accessed March 15, 2023. <a href="https://scrapism.lav.io/">https://scrapism.lav.io/</a>.
</div>
<div id="ref-lavigneNewYorkApartment" class="csl-entry" role="doc-biblioentry">
Lavigne, Sam, and Tega Brain. n.d. <span>“New <span>York Apartment</span>.”</span> Accessed March 17, 2023. <a href="https://artport.whitney.org/commissions/new-york-apartment/index.html">https://artport.whitney.org/commissions/new-york-apartment/index.html</a>.
</div>
<div id="ref-lukoseTextSpeechSynthesizerFormant2017" class="csl-entry" role="doc-biblioentry">
Lukose, Sneha, and Savitha S Upadhya. 2017. <span>“Text to <span>Speech Synthesizer-Formant Synthesis</span>,”</span> 4.
</div>
<div id="ref-matthiasschafer11kHands2018" class="csl-entry" role="doc-biblioentry">
Matthias Schäfer, dir. 2018. <em>11k Hands</em>. <a href="https://www.youtube.com/watch?v=snJsKFxPlJ8">https://www.youtube.com/watch?v=snJsKFxPlJ8</a>.
</div>
<div id="ref-millerArtistMachineWorld2019" class="csl-entry" role="doc-biblioentry">
Miller, Arthur I. 2019. <em>The Artist in the Machine: The World of <span>AI</span> Powered Creativity</em>. <span>Cambridge, Massachusetts</span>: <span>The MIT Press</span>.
</div>
<div id="ref-mitchellArtificialIntelligenceGuide2019" class="csl-entry" role="doc-biblioentry">
Mitchell, Melanie. 2019. <em>Artificial Intelligence: A Guide for Thinking Humans</em>. <span>New York</span>: <span>Farrar, Straus and Giroux</span>.
</div>
<div id="ref-molnar10LearnedFeaturesa" class="csl-entry" role="doc-biblioentry">
Molnar, Christoph. n.d. <em>10.1 <span>Learned Features</span> | <span>Interpretable Machine Learning</span></em>. Accessed March 4, 2023. <a href="https://christophm.github.io/interpretable-ml-book/cnn-features.html">https://christophm.github.io/interpretable-ml-book/cnn-features.html</a>.
</div>
<div id="ref-murphyWhyStanfordResearchers2017" class="csl-entry" role="doc-biblioentry">
Murphy, Heather. 2017. <span>“Why <span>Stanford Researchers Tried</span> to <span>Create</span> a <span>‘<span>Gaydar</span>’</span> <span>Machine</span>.”</span> <em>The New York Times: Science</em>, October 9, 2017. <a href="https://www.nytimes.com/2017/10/09/science/stanford-sexual-orientation-study.html">https://www.nytimes.com/2017/10/09/science/stanford-sexual-orientation-study.html</a>.
</div>
<div id="ref-NutzungsbedingungenInstagramHilfebereich" class="csl-entry" role="doc-biblioentry">
<span>“Nutzungsbedingungen | <span>Instagram-Hilfebereich</span>.”</span> n.d. Accessed March 17, 2023. <a href="https://help.instagram.com/581066165581870">https://help.instagram.com/581066165581870</a>.
</div>
<div id="ref-oordWaveNetGenerativeModel2016" class="csl-entry" role="doc-biblioentry">
Oord, Aaron van den, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. 2016. <span>“<span>WaveNet</span>: <span>A Generative Model</span> for <span>Raw Audio</span>.”</span> <a href="http://arxiv.org/abs/1609.03499">http://arxiv.org/abs/1609.03499</a>.
</div>
<div id="ref-OpenFutureOpen" class="csl-entry" role="doc-biblioentry">
<span>“Open <span>Future</span> – <span>Open Future Foundation</span>.”</span> n.d. <span>Open Future</span>. Accessed March 13, 2023. <a href="https://openfuture.eu">https://openfuture.eu</a>.
</div>
<div id="ref-panayotovLibrispeechASRCorpus2015" class="csl-entry" role="doc-biblioentry">
Panayotov, Vassil, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. 2015. <span>“Librispeech: <span>An ASR</span> Corpus Based on Public Domain Audio Books.”</span> In <em>2015 <span>IEEE International Conference</span> on <span>Acoustics</span>, <span>Speech</span> and <span>Signal Processing</span> (<span>ICASSP</span>)</em>, 5206–10. <span>South Brisbane, Queensland, Australia</span>: <span>IEEE</span>. <a href="https://doi.org/10.1109/ICASSP.2015.7178964">https://doi.org/10.1109/ICASSP.2015.7178964</a>.
</div>
<div id="ref-pierceHowAppleFinally" class="csl-entry" role="doc-biblioentry">
Pierce, David. n.d. <span>“How <span>Apple Finally Made Siri Sound More Human</span>.”</span> <em>Wired</em>. Accessed January 23, 2022. <a href="https://www.wired.com/story/how-apple-finally-made-siri-sound-more-human/">https://www.wired.com/story/how-apple-finally-made-siri-sound-more-human/</a>.
</div>
<div id="ref-PoliticiansDiscussingLyrebird" class="csl-entry" role="doc-biblioentry">
<em>Politicians discussing about Lyrebird</em>. n.d. Accessed January 24, 2022. <a href="https://soundcloud.com/user-535691776/dialog">https://soundcloud.com/user-535691776/dialog</a>.
</div>
<div id="ref-reaHowImageNetRoulette2019" class="csl-entry" role="doc-biblioentry">
Rea, Naomi. 2019. <span>“How <span>ImageNet Roulette</span>, a <span>Viral Art Project That Exposed Facial Recognition</span>’s <span>Biases</span>, <span>Is Changing Minds About AI</span>.”</span> <span>Artnet News</span>. September 23, 2019. <a href="https://news.artnet.com/art-world/imagenet-roulette-trevor-paglen-kate-crawford-1658305">https://news.artnet.com/art-world/imagenet-roulette-trevor-paglen-kate-crawford-1658305</a>.
</div>
<div id="ref-rosenblattPerceptronProbabilisticModel1958" class="csl-entry" role="doc-biblioentry">
Rosenblatt, F. 1958. <span>“The Perceptron: <span>A</span> Probabilistic Model for Information Storage and Organization in the Brain.”</span> <em>Psychological Review</em> 65 (6): 386–408. <a href="https://doi.org/10.1037/h0042519">https://doi.org/10.1037/h0042519</a>.
</div>
<div id="ref-RuDALLE" class="csl-entry" role="doc-biblioentry">
<span>“<span class="nocase">ruDALL-E</span>.”</span> n.d. <span>ruDALL-E</span>. Accessed April 28, 2022. <a href="http://rudalle.ru/en/">http://rudalle.ru/en/</a>.
</div>
<div id="ref-samlavigneJustDiscoveredOpen2020" class="csl-entry" role="doc-biblioentry">
Sam Lavigne. 2020. <span>“Just Discovered an Open Source Hair Detector and Have Used It on Hundreds of Images of <span>Mark Zuckerberg</span> to Build What <span>I</span> Believe Is the Most Comprehensive Archive of <span>Zuckerberg</span> Haircuts in Existence. <span>Thank</span> You <span>AI</span> Researchers! <span class="nocase">https://t.co/QJqXLAbqyw</span>.”</span> Tweet. <span>Twitter</span>. February 16, 2020. <a href="https://twitter.com/sam_lavigne/status/1229097648818446336">https://twitter.com/sam_lavigne/status/1229097648818446336</a>.
</div>
<div id="ref-schaferThisPersonDoes2021" class="csl-entry" role="doc-biblioentry">
Schäfer, Matthias. 2021. <span>“This <span>Person Does Exist</span>.”</span> <em>Temes de Disseny</em>, no. 37 (July): 214–25. <a href="https://doi.org/10.46467/TdD37.2021.214-225">https://doi.org/10.46467/TdD37.2021.214-225</a>.
</div>
<div id="ref-schaferMissingPictures" class="csl-entry" role="doc-biblioentry">
———. n.d. <span>“Missing.pictures.”</span> Accessed March 17, 2023. <a href="https://missing.pictures/">https://missing.pictures/</a>.
</div>
<div id="ref-shipmanAnimalConnectionHuman2010" class="csl-entry" role="doc-biblioentry">
Shipman, Pat. 2010. <span>“The <span>Animal Connection</span> and <span>Human Evolution</span>.”</span> <em>Current Anthropology</em> 51 (4): 519–38. <a href="https://doi.org/10.1086/653816">https://doi.org/10.1086/653816</a>.
</div>
<div id="ref-stuppFraudstersUsedAI2019" class="csl-entry" role="doc-biblioentry">
Stupp, Catherine. 2019. <span>“Fraudsters <span>Used AI</span> to <span>Mimic CEO</span>’s <span>Voice</span> in <span>Unusual Cybercrime Case</span>.”</span> <em>Wall Street Journal: WSJ Pro</em>, August 30, 2019. <a href="https://www.wsj.com/articles/fraudsters-use-ai-to-mimic-ceos-voice-in-unusual-cybercrime-case-11567157402">https://www.wsj.com/articles/fraudsters-use-ai-to-mimic-ceos-voice-in-unusual-cybercrime-case-11567157402</a>.
</div>
<div id="ref-tappertWhoFatherDeep2019" class="csl-entry" role="doc-biblioentry">
Tappert, Charles C. 2019. <span>“Who <span>Is</span> the <span>Father</span> of <span>Deep Learning</span>?”</span> In <em>2019 <span>International Conference</span> on <span>Computational Science</span> and <span>Computational Intelligence</span> (<span>CSCI</span>)</em>, 343–48. <span>Las Vegas, NV, USA</span>: <span>IEEE</span>. <a href="https://doi.org/10.1109/CSCI49370.2019.00067">https://doi.org/10.1109/CSCI49370.2019.00067</a>.
</div>
<div id="ref-TechnicallyResponsibleKnowledge" class="csl-entry" role="doc-biblioentry">
<span>“Technically <span>Responsible Knowledge</span>.”</span> n.d. Accessed March 10, 2023. <a href="http://trk.network/essay">http://trk.network/essay</a>.
</div>
<div id="ref-CanonCatMac2014" class="csl-entry" role="doc-biblioentry">
<span>“The <span>Canon Cat The Mac</span>’s <span>Ancestor</span>.”</span> 2014. October 25, 2014. <a href="https://web.archive.org/web/20141025053109/http://www.jagshouse.com/swyft.html">https://web.archive.org/web/20141025053109/http://www.jagshouse.com/swyft.html</a>.
</div>
<div id="ref-InternetYamiIchi" class="csl-entry" role="doc-biblioentry">
<span>“The Internet Yami-Ichi.”</span> n.d. <span>In Kepler’s Gardens</span>. Accessed March 20, 2023. <a href="https://ars.electronica.art/keplersgardens/de/the-internet-yami-ichi/">https://ars.electronica.art/keplersgardens/de/the-internet-yami-ichi/</a>.
</div>
<div id="ref-ThisPersonDoes2019" class="csl-entry" role="doc-biblioentry">
<span>“This <span>Person Does Not Exist</span>.”</span> 2019. May 31, 2019. <a href="https://web.archive.org/web/20190531222303/https://thispersondoesnotexist.com/">https://web.archive.org/web/20190531222303/https://thispersondoesnotexist.com/</a>.
</div>
<div id="ref-tykaDeepdreamInceptionismRecap" class="csl-entry" role="doc-biblioentry">
Tyka, Mike. n.d. <span>“Deepdream/<span>Inceptionism</span> - Recap.”</span> Accessed March 4, 2023. <a href="http://mtyka.github.io/code/2015/07/21/one-month-after-deepdream.html">http://mtyka.github.io/code/2015/07/21/one-month-after-deepdream.html</a>.
</div>
<div id="ref-vlahosTalkMeAmazon2020" class="csl-entry" role="doc-biblioentry">
Vlahos, James. 2020. <em>Talk to Me: <span>Amazon</span>, <span>Google</span>, <span>Apple</span> and the Race for Voice-Controlled <span>AI</span></em>.
</div>
<div id="ref-wanGeneralizedEndtoEndLoss2020" class="csl-entry" role="doc-biblioentry">
Wan, Li, Quan Wang, Alan Papir, and Ignacio Lopez Moreno. 2020. <span>“Generalized <span class="nocase">End-to-End Loss</span> for <span>Speaker Verification</span>.”</span> <a href="http://arxiv.org/abs/1710.10467">http://arxiv.org/abs/1710.10467</a>.
</div>
<div id="ref-wangDefiningArtificialIntelligence2019" class="csl-entry" role="doc-biblioentry">
Wang, Pei. 2019. <span>“On <span>Defining Artificial Intelligence</span>.”</span> <em>Journal of Artificial General Intelligence</em> 10 (January): 1–37. <a href="https://doi.org/10.2478/jagi-2019-0002">https://doi.org/10.2478/jagi-2019-0002</a>.
</div>
<div id="ref-wangTacotronEndtoEndSpeech2017" class="csl-entry" role="doc-biblioentry">
Wang, Yuxuan, R. J. Skerry-Ryan, Daisy Stanton, Yonghui Wu, Ron J. Weiss, Navdeep Jaitly, Zongheng Yang, et al. 2017. <span>“Tacotron: <span class="nocase">Towards End-to-End Speech Synthesis</span>.”</span> <a href="http://arxiv.org/abs/1703.10135">http://arxiv.org/abs/1703.10135</a>.
</div>
<div id="ref-WaveNetLaunchesGoogle" class="csl-entry" role="doc-biblioentry">
<span>“WaveNet launches in the Google Assistant.”</span> n.d. <span>Deepmind</span>. Accessed January 24, 2022. <a href="https://deepmind.com/blog/article/wavenet-launches-google-assistant">https://deepmind.com/blog/article/wavenet-launches-google-assistant</a>.
</div>
<div id="ref-weibelAAA" class="csl-entry" role="doc-biblioentry">
Weibel, Peter. n.d. <span>“AAA.”</span> <span>www.kunstforum.de</span>. Accessed January 16, 2022. <a href="https://www.kunstforum.de/artikel/aaa/">https://www.kunstforum.de/artikel/aaa/</a>.
</div>
<div id="ref-XTRAMarkHansen" class="csl-entry" role="doc-biblioentry">
<span>“X-<span>TRA</span> → <span>Mark Hansen</span> and <span>Ben Rubin</span>: <span>Listening Post</span>.”</span> n.d. Accessed March 20, 2023. <a href="https://www.x-traonline.org/article/mark-hansen-and-ben-rubin-listening-post">https://www.x-traonline.org/article/mark-hansen-and-ben-rubin-listening-post</a>.
</div>
<div id="ref-zarkadakesOurOwnImage2016" class="csl-entry" role="doc-biblioentry">
Zarkadakēs, Giōrgos. 2016. <em>In Our Own Image: Savior or Destroyer?: The History and Future of Artificial Intelligence</em>. First Pegasus books hardcover edition. <span>New York, NY</span>: <span>Pegasus Books LLC</span>.
</div>
</div>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>The C Programming Language Book defined many standards of programming languages today and how technical descriptions are written. While it focuses on C and the UNIX system, I find this advice from <em>Chapter 1.1 Getting Started</em> particularly interesting “On other systems, the rules will be different; check with a local expert.” as it describes the social necessity of learning computers specifically.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>Original Google Blog post. See: <span class="citation" data-cites="InceptionismGoingDeeper2015">(<a href="#ref-InceptionismGoingDeeper2015" role="doc-biblioref"><span>“Inceptionism: <span>Going Deeper</span> into <span>Neural Networks</span>”</span> 2015</a>)</span><a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p>The initial classifier was conditioned on ImageNet, which contains many images of dog breeds. Therefore DeepDream is biased to generate textures of dog faces.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4" role="doc-endnote"><p>About ImageNet see the website: <span class="citation" data-cites="ImageNet">(<a href="#ref-ImageNet" role="doc-biblioref"><span>“<span>ImageNet</span>”</span> n.d.</a>)</span><a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5" role="doc-endnote"><p>Mike Tyka has compiled a list of projects, released within a month of publishing the source code. See: <span class="citation" data-cites="tykaDeepdreamInceptionismRecap">(<a href="#ref-tykaDeepdreamInceptionismRecap" role="doc-biblioref">Tyka n.d.</a>)</span><a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6" role="doc-endnote"><p>A collection of tools and papers for feature visualization is collected at the github tensorflow/lucid repository. Note that Chris Olah and Alex Mordvintsev are contributors. Even more examples and explanations are collected in the interpretable machine learning book by the statistician Christoph Molnar, who also notes that “Feature visualizations give unique insight into the working of neural networks,” but the “visualizations can convey the illusion that we understand what the neural network is doing.” <span class="citation" data-cites="molnar10LearnedFeaturesa">(<a href="#ref-molnar10LearnedFeaturesa" role="doc-biblioref">Molnar n.d.</a>)</span><a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7" role="doc-endnote"><p>Interview with Alexander Mordvintsev by artnome. See: <span class="citation" data-cites="baileyDeepDreamCreatorUnveils">(<a href="#ref-baileyDeepDreamCreatorUnveils" role="doc-biblioref">Bailey n.d.</a>)</span><a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8" role="doc-endnote"><p><span class="citation" data-cites="millerArtistMachineWorld2019">(<a href="#ref-millerArtistMachineWorld2019" role="doc-biblioref">Miller 2019</a>)</span>, p.122<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9" role="doc-endnote"><p>Mordvintsev himself was inspired by a previous paper exploring the generative potential of CNNs from Karen Simonyan: Simonyan, Vedaldi, and Zisserman 2014.<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10" role="doc-endnote"><p>Translated from the article ‘AAA - Art, Algorithmen, Artificial Intelligence’ at Kunstforum Bd. 278. See: <span class="citation" data-cites="weibelAAA">(<a href="#ref-weibelAAA" role="doc-biblioref">Weibel n.d.</a>)</span><a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11" role="doc-endnote"><p>Referring to typical AI stock images. Alternative imagery is currently built by betterimagesofai.org. See: <span class="citation" data-cites="BetterImagesAI">(<a href="#ref-BetterImagesAI" role="doc-biblioref"><span>“Better <span>Images</span> of <span>AI</span>”</span> n.d.</a>)</span><a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn12" role="doc-endnote"><p>Twitter post by Francis Hunger. See: <span class="citation" data-cites="francishungerArtificialIntelligenceAutomated2021">(<a href="#ref-francishungerArtificialIntelligenceAutomated2021" role="doc-biblioref">Francis Hunger 2021</a>)</span><a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn13" role="doc-endnote"><p>Unhype AI. See: <span class="citation" data-cites="hungerTalkUnhypeAI2021">(<a href="#ref-hungerTalkUnhypeAI2021" role="doc-biblioref">Hunger 2021</a>)</span><a href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn14" role="doc-endnote"><p>In behaviorist psychology Ivan Pavlov’s experiments with dogs is known as ‘classical conditioning’ and B.F. Skinner who experimented on rats and pigeons using lever machines is called ‘operant conditioning.’<a href="#fnref14" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn15" role="doc-endnote"><p>^ <span class="citation" data-cites="wangDefiningArtificialIntelligence2019">(<a href="#ref-wangDefiningArtificialIntelligence2019" role="doc-biblioref">P. Wang 2019</a>)</span><a href="#fnref15" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn16" role="doc-endnote"><p>The term Ubuntu originates from southern Africa and is used to describe a humanist philosophy according to which persons are interconnected. It is often used in phrases such as the Zulu ‘Umuntu ngumuntu ngabantu,’ which translates to ‘A person is a person through other persons.’ The term for personhood (ubuntu) itself is not clearly defined and has different meanings in different areas. <span class="citation" data-cites="gadeWhatUbuntuDifferent2012"><a href="#ref-gadeWhatUbuntuDifferent2012" role="doc-biblioref">Gade</a> (<a href="#ref-gadeWhatUbuntuDifferent2012" role="doc-biblioref">2012</a>)</span><a href="#fnref16" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn17" role="doc-endnote"><p><span class="citation" data-cites="DescartesWasWrong2017"><a href="#ref-DescartesWasWrong2017" role="doc-biblioref"><span>“Descartes Was Wrong: <span>‘A Person Is a Person Through Other Persons’</span> | <span>Aeon Ideas</span>”</span></a> (<a href="#ref-DescartesWasWrong2017" role="doc-biblioref">2017</a>)</span><a href="#fnref17" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn18" role="doc-endnote"><p>In the story <em>On Having No Head</em> D.E. Harding describes the realization that one can never experience their own head, because the most important sensual organs are embedded in it. I don’t remember where I read it, but there is an argument that the reason we place consciousness in the brain is because we mostly experience the world from the position of our head.<a href="#fnref18" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn19" role="doc-endnote"><p><span class="citation" data-cites="zarkadakesOurOwnImage2016"><a href="#ref-zarkadakesOurOwnImage2016" role="doc-biblioref">Zarkadakēs</a> (<a href="#ref-zarkadakesOurOwnImage2016" role="doc-biblioref">2016</a>)</span> p.189<a href="#fnref19" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn20" role="doc-endnote"><p>The simulation argument was first proposed by Nick Bostrom. He argues that if we were able to create realistic simulated minds and worlds in the future it is likely that we already live in a simulation. <span class="citation" data-cites="bostromAreYouLiving2001">(<a href="#ref-bostromAreYouLiving2001" role="doc-biblioref">Bostrom 2001</a>)</span> The theory was<a href="#fnref20" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn21" role="doc-endnote"><p><span class="citation" data-cites="FREEPORTANATOMIESBLACK"><a href="#ref-FREEPORTANATOMIESBLACK" role="doc-biblioref"><span>“<span>FREEPORT</span> 1: <span>ANATOMIES OF A BLACK BOX</span>”</span></a> (<a href="#ref-FREEPORTANATOMIESBLACK" role="doc-biblioref">n.d.</a>)</span><a href="#fnref21" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn22" role="doc-endnote"><p><span class="citation" data-cites="AnatomyAISystem2018"><a href="#ref-AnatomyAISystem2018" role="doc-biblioref"><span>“Anatomy of an <span>AI System</span>”</span></a> (<a href="#ref-AnatomyAISystem2018" role="doc-biblioref">2018</a>)</span><a href="#fnref22" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn23" role="doc-endnote"><p>Signal is an encrypted messaging app. See: https://www.signal.org<a href="#fnref23" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn24" role="doc-endnote"><p>Jitsi Meet is an open source video conferencing software. See: https://meet.jit.si/<a href="#fnref24" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn25" role="doc-endnote"><p>See: https://cwc.radical-openness.org/siliconfriendcamp/<a href="#fnref25" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn26" role="doc-endnote"><p>Babycastles is a NYC based collective fostering and amplifying diverse voices in videogame culture. Babycastles provides artists support to actualize ideas and expose that work to new audiences. <span class="citation" data-cites="Babycastles"><a href="#ref-Babycastles" role="doc-biblioref"><span>“Babycastles”</span></a> (<a href="#ref-Babycastles" role="doc-biblioref">n.d.</a>)</span><a href="#fnref26" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn27" role="doc-endnote"><p>The open source forum software <em>Discourse</em> was hosted at the local servus.at data center.<a href="#fnref27" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn28" role="doc-endnote"><p>Bogna Konior, <em>The Dark Forest Theory of the Internet</em>; Rodney Brooks, Intelligence without Representation_; Matteo Pasquinelli, <em>The Arborescent Mind: The Intelligence of an Inverted Tree</em><a href="#fnref28" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn29" role="doc-endnote"><p>See: <span class="citation" data-cites="CanonCatMac2014"><a href="#ref-CanonCatMac2014" role="doc-biblioref"><span>“The <span>Canon Cat The Mac</span>’s <span>Ancestor</span>”</span></a> (<a href="#ref-CanonCatMac2014" role="doc-biblioref">2014</a>)</span><a href="#fnref29" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn30" role="doc-endnote"><p>See: <span class="citation" data-cites="RuDALLE"><a href="#ref-RuDALLE" role="doc-biblioref"><span>“<span class="nocase">ruDALL-E</span>”</span></a> (<a href="#ref-RuDALLE" role="doc-biblioref">n.d.</a>)</span><a href="#fnref30" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn31" role="doc-endnote"><p>Together with the artist yang02, So Kanno modified multiple objects that could be remotely controlled from the web. See: <span class="citation" data-cites="Avatars"><a href="#ref-Avatars" role="doc-biblioref"><span>“Avatars”</span></a> (<a href="#ref-Avatars" role="doc-biblioref">n.d.</a>)</span><a href="#fnref31" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn32" role="doc-endnote"><p>See: <span class="citation" data-cites="HumanguidedBurritoBots2019"><a href="#ref-HumanguidedBurritoBots2019" role="doc-biblioref"><span>“Human-Guided Burrito Bots Raise Questions about the Future of Robo-Delivery”</span></a> (<a href="#ref-HumanguidedBurritoBots2019" role="doc-biblioref">2019</a>)</span><a href="#fnref32" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn33" role="doc-endnote"><p>Everything is an interactive experience by David OReilly from 2017. See: <span class="citation" data-cites="EverythingDLVIDOREILLY"><a href="#ref-EverythingDLVIDOREILLY" role="doc-biblioref"><span>“Everything — <span>DΛVID OREILLY</span> • Computer Art &amp; Research”</span></a> (<a href="#ref-EverythingDLVIDOREILLY" role="doc-biblioref">n.d.</a>)</span><a href="#fnref33" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn34" role="doc-endnote"><p>See: http://lsd.blazejkotowski.com/<a href="#fnref34" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn35" role="doc-endnote"><p>Generated by GPT3 with some context<a href="#fnref35" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn36" role="doc-endnote"><p>Based on a tweet by the user mind_backup https://twitter.com/mind_backup/status/1601131037609730048<a href="#fnref36" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn37" role="doc-endnote"><p>“Hephaestus then limped out of the door’; and maidens supported the ruler, golden ones, like living ones, with youthful charming education: These have understanding in the breast, and speaking voice, Have strength, and also learned art work from the gods.” (Homer, Iliad 18, 417-420; link:<a href="https://www.projekt-gutenberg.org/homer/ilias/ilias183.html">https://www.projekt-gutenberg.org/homer/ilias/ilias183.html</a>)<a href="#fnref37" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn38" role="doc-endnote"><p>Fabers machine was first presented with a female mask in the USA and later in London under oriental motif wearing a turban.<a href="#fnref38" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn39" role="doc-endnote"><p>D.Lindsay, 1997. Link:<a href="https://www.inventionandtech.com/content/talking-head-1">https://www.inventionandtech.com/content/talking-head-1</a><a href="#fnref39" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn40" role="doc-endnote"><p>The Euphonia also inspired media artist Michael Markert to built <em>kII (Kempelen 2.0)</em>, an interactive installation in which visitors can playfully control a speech synthesizer by moving, opening and closing their hands. In doing so, he brings Kempelen’s speech apparatus into the 21st century with the help of an 8-bit PIC microcontroller and sensor technology. Like Kempelen’s apparatus it alienates the voice in such a way that it creates mostly meaningless vocal sounds that enable new gesticulatory speech interactions. (Serexhe et al. 2007, p. 74). And project description online. Link:<a href="http://www.audiocommander.de">http://www.audiocommander.de</a><a href="#fnref40" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn41" role="doc-endnote"><p>A video recording of the Voder demonstration can be found in the AP Archive under Human Voice Machine.<a href="http://www.aparchive.com/metadata/youtube/5f098b1f3e8b4d09b8de30dcecc42f99">http://www.aparchive.com/metadata/youtube/5f098b1f3e8b4d09b8de30dcecc42f99</a><a href="#fnref41" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn42" role="doc-endnote"><p>A focus on black women who worked as computers for NACA (NASA’s predecessor) can be seen in the film Hidden Figures, 2016.<a href="#fnref42" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn43" role="doc-endnote"><p>The story of the ENIAC programmers is told in the documentary The Computers, 2016.<a href="#fnref43" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn44" role="doc-endnote"><p>Project description of Reflections of HAL and Samantha online. Link:<a href="https://tillmannohm.com/reflections-of-hal-and-samantha/">https://tillmannohm.com/reflections-of-hal-and-samantha/</a><a href="#fnref44" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn45" role="doc-endnote"><p>| <span class="citation" data-cites="zarkadakesOurOwnImage2016"><a href="#ref-zarkadakesOurOwnImage2016" role="doc-biblioref">Zarkadakēs</a> (<a href="#ref-zarkadakesOurOwnImage2016" role="doc-biblioref">2016</a>)</span><a href="#fnref45" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn46" role="doc-endnote"><p>The robot puppy was featured on the TIME magazine cover with the title “Tech comes to Toyland.” Link: http://content.time.com/time/covers/europe/0,16641,20001211,00.html<a href="#fnref46" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn47" role="doc-endnote"><p><span class="citation" data-cites="shipmanAnimalConnectionHuman2010"><a href="#ref-shipmanAnimalConnectionHuman2010" role="doc-biblioref">Shipman</a> (<a href="#ref-shipmanAnimalConnectionHuman2010" role="doc-biblioref">2010</a>)</span><a href="#fnref47" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn48" role="doc-endnote"><p>Sean Riddle requested the Furby source code from the US patent office and uploaded it on his webpage. Link: http://www.seanriddle.com/furbysource.pdf<a href="#fnref48" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn49" role="doc-endnote"><p><span class="citation" data-cites="lukoseTextSpeechSynthesizerFormant2017"><a href="#ref-lukoseTextSpeechSynthesizerFormant2017" role="doc-biblioref">Lukose and Upadhya</a> (<a href="#ref-lukoseTextSpeechSynthesizerFormant2017" role="doc-biblioref">2017</a>)</span><a href="#fnref49" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn50" role="doc-endnote"><p>To make the demo work, engineers had to use a prototype Mac that was more powerful than the retail version. It said: “Hello, I am Macintosh. It sure is great to get out of that bag! Unaccustomed as I am to public speaking, I’d like to share with you a maxim I thought of the first time I met an IBM mainframe: Never trust a computer that you can’t lift! Obviously, I can talk, but right now I’d like to sit back and listen. So it is with considerable pride that I introduce a man who has been like a father to me… Steve Jobs!” Link: https://www.folklore.org/StoryView.py?project=Macintosh&amp;story=Intro_Demo.txt<a href="#fnref50" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn51" role="doc-endnote"><p>The songs of 386 DX are available on Alexei Shulgin’s website. Link: http://www.easylife.org/386dx/<a href="#fnref51" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn52" role="doc-endnote"><p>Interview by the Guardian with multiple voice actors for Siri. Link: https://www.theguardian.com/technology/2015/aug/12/siri-real-voices-apple-ios-assistant-jon-briggs-susan-bennett-karen-jacobsen<a href="#fnref52" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn53" role="doc-endnote"><p><span class="citation" data-cites="oordWaveNetGenerativeModel2016"><a href="#ref-oordWaveNetGenerativeModel2016" role="doc-biblioref">Oord et al.</a> (<a href="#ref-oordWaveNetGenerativeModel2016" role="doc-biblioref">2016</a>)</span><a href="#fnref53" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn54" role="doc-endnote"><p>Lyrebird changed their name to Descript. Link: https://www.descript.com/<a href="#fnref54" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn55" role="doc-endnote"><p><span class="citation" data-cites="PoliticiansDiscussingLyrebird"><a href="#ref-PoliticiansDiscussingLyrebird" role="doc-biblioref"><em>Politicians discussing about Lyrebird</em></a> (<a href="#ref-PoliticiansDiscussingLyrebird" role="doc-biblioref">n.d.</a>)</span><a href="#fnref55" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn56" role="doc-endnote"><p>Apparently fraudsters were able to use a synthetic voice to instruct a bank transfer of 220.000€. <span class="citation" data-cites="stuppFraudstersUsedAI2019"><a href="#ref-stuppFraudstersUsedAI2019" role="doc-biblioref">Stupp</a> (<a href="#ref-stuppFraudstersUsedAI2019" role="doc-biblioref">2019</a>)</span><a href="#fnref56" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn57" role="doc-endnote"><p>In 2019 an app with the name DeepNude got public attention due to a Vice article and was taken down by the author shortly after. <span class="citation" data-cites="coleThisHorrifyingApp2019"><a href="#ref-coleThisHorrifyingApp2019" role="doc-biblioref">Cole</a> (<a href="#ref-coleThisHorrifyingApp2019" role="doc-biblioref">2019</a>)</span><a href="#fnref57" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn58" role="doc-endnote"><p>Examples of deepfake political comedy can be found on the YouTube Channel of Speaking of AI. Link: https://www.youtube.com/channel/UCID5qusrF32kSj-oSGq3rJg<a href="#fnref58" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn59" role="doc-endnote"><p>| <span class="citation" data-cites="pierceHowAppleFinally"><a href="#ref-pierceHowAppleFinally" role="doc-biblioref">Pierce</a> (<a href="#ref-pierceHowAppleFinally" role="doc-biblioref">n.d.</a>)</span><a href="#fnref59" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn60" role="doc-endnote"><p>| <span class="citation" data-cites="WaveNetLaunchesGoogle"><a href="#ref-WaveNetLaunchesGoogle" role="doc-biblioref"><span>“WaveNet launches in the Google Assistant”</span></a> (<a href="#ref-WaveNetLaunchesGoogle" role="doc-biblioref">n.d.</a>)</span><a href="#fnref60" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn61" role="doc-endnote"><p>| <span class="citation" data-cites="wangTacotronEndtoEndSpeech2017"><a href="#ref-wangTacotronEndtoEndSpeech2017" role="doc-biblioref">Y. Wang et al.</a> (<a href="#ref-wangTacotronEndtoEndSpeech2017" role="doc-biblioref">2017</a>)</span><a href="#fnref61" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn62" role="doc-endnote"><p>Hydra was developed by Olivia Jack. Link: https://hydra.ojack.xyz<a href="#fnref62" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn63" role="doc-endnote"><p>As described in the introduction I use “weighted” instead of “neural.”<a href="#fnref63" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn64" role="doc-endnote"><p>| <span class="citation" data-cites="hintonDeepNeuralNetworks2012b"><a href="#ref-hintonDeepNeuralNetworks2012b" role="doc-biblioref">Hinton et al.</a> (<a href="#ref-hintonDeepNeuralNetworks2012b" role="doc-biblioref">2012</a>)</span><a href="#fnref64" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn65" role="doc-endnote"><p>Google AI Blog article about “Speech Recognition and Deep Learning.” Link: https://ai.googleblog.com/2012/08/speech-recognition-and-deep-learning.html<a href="#fnref65" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn66" role="doc-endnote"><p>Tatman 2017, did a study on Gender and Dialect Bias in YouTube’s Automatic Captions that shows a higher error rate for women and non-American speakers. Facebook’s AI research department created a dataset of people with different skin tones, gender and age to evaluate speech recognition models for biases and found significant differences between lighter and darker skin tones (Liu et al. 2021).<a href="#fnref66" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn67" role="doc-endnote"><p>Ref: <span class="citation" data-cites="amodeiDeepSpeechEndtoEnd2015"><a href="#ref-amodeiDeepSpeechEndtoEnd2015" role="doc-biblioref">Amodei et al.</a> (<a href="#ref-amodeiDeepSpeechEndtoEnd2015" role="doc-biblioref">2015</a>)</span><a href="#fnref67" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn68" role="doc-endnote"><p>The first Alexa recording in a court case was handed over after the defendant agreed handing over his data. The Independent, Amazon Echo could become key witness in murder investigation after data turned over to police, 2017. Link:<a href="https://www.independent.co.uk/news/world/americas/amazon-echo-murder-investigation-data-police-a7621261.html">https://www.independent.co.uk/news/world/americas/amazon-echo-murder-investigation-data-police-a7621261.html</a><a href="#fnref68" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn69" role="doc-endnote"><p>Link:<a href="https://commonvoice.mozilla.org/de">https://commonvoice.mozilla.org/de</a><a href="#fnref69" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn70" role="doc-endnote"><p>Link to LibriVox: https://librivox.org/ and <span class="citation" data-cites="panayotovLibrispeechASRCorpus2015"><a href="#ref-panayotovLibrispeechASRCorpus2015" role="doc-biblioref">Panayotov et al.</a> (<a href="#ref-panayotovLibrispeechASRCorpus2015" role="doc-biblioref">2015</a>)</span><a href="#fnref70" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn71" role="doc-endnote"><p>Link to the LDC Switchboard-1 Release 2: https://catalog.ldc.upenn.edu/LDC97S62<a href="#fnref71" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn72" role="doc-endnote"><p>The title “I’d blush if I could” is also the response Siri gives to the insult “You’re a bitch”<a href="#fnref72" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn73" role="doc-endnote"><p>Nadine Alessio’s project website. Link: <a href="http://nadinelessio.com/projects.html">http://nadinelessio.com/projects.html</a><a href="#fnref73" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn74" role="doc-endnote"><p>| <span class="citation" data-cites="vlahosTalkMeAmazon2020"><a href="#ref-vlahosTalkMeAmazon2020" role="doc-biblioref">Vlahos</a> (<a href="#ref-vlahosTalkMeAmazon2020" role="doc-biblioref">2020</a>)</span><a href="#fnref74" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn75" role="doc-endnote"><p>| <span class="citation" data-cites="vlahosTalkMeAmazon2020"><a href="#ref-vlahosTalkMeAmazon2020" role="doc-biblioref">Vlahos</a> (<a href="#ref-vlahosTalkMeAmazon2020" role="doc-biblioref">2020</a>)</span> p.117<a href="#fnref75" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn76" role="doc-endnote"><p>Tay was the name of a chat bot Microsoft intended to have a teenage personality and could be interacted with over multiple channels. Twitter trolls co-opted the bot with sexist, racist and antisemitic questions that the bot replied to with generic answers. They also made use of a repeat-after-me phrase to make it look like the bot is spewing hateful comments itself. After only 16 hours Microsoft deleted all accounts and the PR disaster was immense, with headlines happily personifying the AI that has learned to be racist. But no continuous learning algorithm was involved. Link to a blog post by Russel Cameron Thomas explaining the technology stack behind Tay. <span class="citation" data-cites="ExploringPossibilitySpace">(<a href="#ref-ExploringPossibilitySpace" role="doc-biblioref"><span>“Exploring <span>Possibility Space</span>: <span>Microsoft</span> #<span>TAYFAIL Smoking Gun</span>: <span>ALICE Open Source AI Library</span> and <span>AIML</span>”</span> n.d.</a>)</span><a href="#fnref76" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn77" role="doc-endnote"><p>| <span class="citation" data-cites="GENDERDIVERSITYGastvortrag"><a href="#ref-GENDERDIVERSITYGastvortrag" role="doc-biblioref"><span>“GENDER &amp; DIVERSITY: Gastvortrag von Ekheo - Musik und Kunst Privatuniversität der Stadt Wien”</span></a> (<a href="#ref-GENDERDIVERSITYGastvortrag" role="doc-biblioref">n.d.</a>)</span><a href="#fnref77" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn78" role="doc-endnote"><p>The Oracle was a terminal computer connected to GPT-3 with a generic Q&amp;A prompt.<a href="#fnref78" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn79" role="doc-endnote"><p>From the github repository CorentinJ/Real-Time-Voice-Cloning. Link: <span class="citation" data-cites="jemineRealTimeVoiceCloning2022">(<a href="#ref-jemineRealTimeVoiceCloning2022" role="doc-biblioref">Corentin Jemine [2019] 2022</a>)</span>. and his master thesis: <span class="citation" data-cites="jemineMasterThesisAutomatic">(<a href="#ref-jemineMasterThesisAutomatic" role="doc-biblioref">C. Jemine, n.d.</a>)</span><a href="#fnref79" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn80" role="doc-endnote"><p>| <span class="citation" data-cites="jiaTransferLearningSpeaker2019"><a href="#ref-jiaTransferLearningSpeaker2019" role="doc-biblioref">Jia et al.</a> (<a href="#ref-jiaTransferLearningSpeaker2019" role="doc-biblioref">2019</a>)</span><a href="#fnref80" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn81" role="doc-endnote"><p>The SV2TTS pipeline is based on these previous papers: <span class="citation" data-cites="wanGeneralizedEndtoEndLoss2020"><a href="#ref-wanGeneralizedEndtoEndLoss2020" role="doc-biblioref">Wan et al.</a> (<a href="#ref-wanGeneralizedEndtoEndLoss2020" role="doc-biblioref">2020</a>)</span> → <span class="citation" data-cites="wangTacotronEndtoEndSpeech2017"><a href="#ref-wangTacotronEndtoEndSpeech2017" role="doc-biblioref">Y. Wang et al.</a> (<a href="#ref-wangTacotronEndtoEndSpeech2017" role="doc-biblioref">2017</a>)</span> → <span class="citation" data-cites="oordWaveNetGenerativeModel2016"><a href="#ref-oordWaveNetGenerativeModel2016" role="doc-biblioref">Oord et al.</a> (<a href="#ref-oordWaveNetGenerativeModel2016" role="doc-biblioref">2016</a>)</span><a href="#fnref81" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn82" role="doc-endnote"><p>| <span class="citation" data-cites="CloneSyntheticAI"><a href="#ref-CloneSyntheticAI" role="doc-biblioref"><span>“Clone <span>Synthetic AI Voices</span> with <span>Neural Text</span> to <span>Speech</span>”</span></a> (<a href="#ref-CloneSyntheticAI" role="doc-biblioref">n.d.</a>)</span><a href="#fnref82" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn83" role="doc-endnote"><p>Hydra was developed by Olivia Jack. Link: https://hydra.ojack.xyz<a href="#fnref83" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn84" role="doc-endnote"><p><span class="citation" data-cites="rosenblattPerceptronProbabilisticModel1958">(<a href="#ref-rosenblattPerceptronProbabilisticModel1958" role="doc-biblioref">Rosenblatt 1958</a>)</span><a href="#fnref84" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn85" role="doc-endnote"><p>The Mark I was a electromechanical machine that used motor driven potentiometers to adjust the variable weights.<a href="#fnref85" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn86" role="doc-endnote"><p>Frank Rosenblatt died in a boating accident in 1971. A couple years prior Marvin Minsky heavily criticized the mathematics behind perceptrons and advocated for a symbolic approach. These turn of events might have lead to a lack of funding in the ‘connectionist’ AI research field and ultimately lead to a general disinterest when the symbolic approach could not keep their exaggerated promises.<a href="#fnref86" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn87" role="doc-endnote"><p><span class="citation" data-cites="mitchellArtificialIntelligenceGuide2019">(<a href="#ref-mitchellArtificialIntelligenceGuide2019" role="doc-biblioref">Mitchell 2019</a>)</span>, p. 114<a href="#fnref87" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn88" role="doc-endnote"><p><span class="citation" data-cites="fukushimaCognitronSelforganizingMultilayered1975">(<a href="#ref-fukushimaCognitronSelforganizingMultilayered1975" role="doc-biblioref">Fukushima 1975</a>)</span><a href="#fnref88" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn89" role="doc-endnote"><p><span class="citation" data-cites="fukushimaNeocognitronHierarchicalNeural1988">(<a href="#ref-fukushimaNeocognitronHierarchicalNeural1988" role="doc-biblioref">Fukushima 1988</a>)</span><a href="#fnref89" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn90" role="doc-endnote"><p><span class="citation" data-cites="tappertWhoFatherDeep2019">(<a href="#ref-tappertWhoFatherDeep2019" role="doc-biblioref">Tappert 2019</a>)</span><a href="#fnref90" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn91" role="doc-endnote"><p><span class="citation" data-cites="krizhevskyImageNetClassificationDeep2017">(<a href="#ref-krizhevskyImageNetClassificationDeep2017" role="doc-biblioref">Krizhevsky, Sutskever, and Hinton 2017</a>)</span><a href="#fnref91" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn92" role="doc-endnote"><p>“One epoch takes 35 GPU minutes but more than 35 CPU hours.” <span class="citation" data-cites="ciresanFlexibleHighPerformance">(<a href="#ref-ciresanFlexibleHighPerformance" role="doc-biblioref">Ciresan et al., n.d.</a>)</span><a href="#fnref92" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn93" role="doc-endnote"><p><span class="citation" data-cites="afifi11KHandsGender2018">(<a href="#ref-afifi11KHandsGender2018" role="doc-biblioref">Afifi 2018</a>)</span>. Data and source code available here: <span class="citation" data-cites="afifi11kHands">(<a href="#ref-afifi11kHands" role="doc-biblioref">Afifi n.d.</a>)</span><a href="#fnref93" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn94" role="doc-endnote"><p>In 2018 Parker et al jokingly tested the Galls theory using 21st century scientific methods and MRI data. <span class="citation" data-cites="dempsey-jonesNeuroscientistsPutDubious2018">(<a href="#ref-dempsey-jonesNeuroscientistsPutDubious2018" role="doc-biblioref">Dempsey-Jones 2018</a>)</span><a href="#fnref94" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn95" role="doc-endnote"><p>I didn’t do any testing of the system as I don’t know how to run MatLab code, but I can imagine that the slightly better results on the dorsal hand are the result of nail polish only applied on female hands.<a href="#fnref95" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn96" role="doc-endnote"><p><span class="citation" data-cites="hoelOperativeImagesInroads2018">(<a href="#ref-hoelOperativeImagesInroads2018" role="doc-biblioref">Hoel 2018</a>)</span><a href="#fnref96" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn97" role="doc-endnote"><p>See: <span class="citation" data-cites="matthiasschafer11kHands2018">(<a href="#ref-matthiasschafer11kHands2018" role="doc-biblioref">Matthias Schäfer 2018</a>)</span><a href="#fnref97" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn98" role="doc-endnote"><p><span class="citation" data-cites="HowReadPalms">(<a href="#ref-HowReadPalms" role="doc-biblioref"><span>“How to <span>Read Palms</span>: 9 <span>Steps</span> (with <span>Pictures</span>)”</span> n.d.</a>)</span><a href="#fnref98" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn99" role="doc-endnote"><p>One particular famous example of this is the work by Michael Kosinski and Yiluna Wang. Their flawed study tried to predict if a person is gay by scraping dating sites and training a classifier on these images. See:<span class="citation" data-cites="murphyWhyStanfordResearchers2017">(<a href="#ref-murphyWhyStanfordResearchers2017" role="doc-biblioref">Murphy 2017</a>)</span><a href="#fnref99" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn100" role="doc-endnote"><p>ImageNet started with 3,2 million images and had the goal to collect 50 million by the end of 2011. <span class="citation" data-cites="dengImageNetLargeScaleHierarchical2009">(<a href="#ref-dengImageNetLargeScaleHierarchical2009" role="doc-biblioref">Deng et al. 2009</a>)</span><a href="#fnref100" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn101" role="doc-endnote"><p><span class="citation" data-cites="gershgornDataThatTransformed2017">(<a href="#ref-gershgornDataThatTransformed2017" role="doc-biblioref">Gershgorn 2017</a>)</span><a href="#fnref101" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn102" role="doc-endnote"><p><span class="citation" data-cites="haraDataDrivenAnalysisWorkers2017">(<a href="#ref-haraDataDrivenAnalysisWorkers2017" role="doc-biblioref">Hara et al. 2017</a>)</span><a href="#fnref102" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn103" role="doc-endnote"><p>See: <span class="citation" data-cites="TechnicallyResponsibleKnowledge">(<a href="#ref-TechnicallyResponsibleKnowledge" role="doc-biblioref"><span>“Technically <span>Responsible Knowledge</span>”</span> n.d.</a>)</span><a href="#fnref103" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn104" role="doc-endnote"><p>See: <span class="citation" data-cites="KATECRAWFORDTREVOR">(<a href="#ref-KATECRAWFORDTREVOR" role="doc-biblioref"><span>“<span>KATE CRAWFORD</span> | <span>TREVOR PAGLEN</span>: <span>TRAINING HUMANS</span> – <span>Fondazione Prada</span>”</span> n.d.</a>)</span><a href="#fnref104" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn105" role="doc-endnote"><p><span class="citation" data-cites="ExcavatingAI">(<a href="#ref-ExcavatingAI" role="doc-biblioref"><span>“Excavating <span>AI</span>”</span> n.d.</a>)</span><a href="#fnref105" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn106" role="doc-endnote"><p>See: <span class="citation" data-cites="ImageNetRouletteTrevor">(<a href="#ref-ImageNetRouletteTrevor" role="doc-biblioref"><span>“<span>ImageNet Roulette</span> – <span>Trevor Paglen</span>”</span> n.d.</a>)</span><a href="#fnref106" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn107" role="doc-endnote"><p><span class="citation" data-cites="reaHowImageNetRoulette2019">(<a href="#ref-reaHowImageNetRoulette2019" role="doc-biblioref">Rea 2019</a>)</span><a href="#fnref107" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn108" role="doc-endnote"><p>See: <span class="citation" data-cites="harveyExposingAi">(<a href="#ref-harveyExposingAi" role="doc-biblioref">Harvey n.d.a</a>)</span><a href="#fnref108" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn109" role="doc-endnote"><p><span class="citation" data-cites="2ndUnconstrainedFace">(<a href="#ref-2ndUnconstrainedFace" role="doc-biblioref"><span>“2nd <span>Unconstrained Face Detection</span> and <span>Open Set Recognition Challenge</span>, <span>ECCV-2018</span>”</span> n.d.</a>)</span><a href="#fnref109" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn110" role="doc-endnote"><p>See a longer analysis on the exploitation of CC media by A. Harvey: <span class="citation" data-cites="CreativeCommonsBiometrics">(<a href="#ref-CreativeCommonsBiometrics" role="doc-biblioref"><span>“Creative <span>Commons Biometrics</span>”</span> n.d.</a>)</span><a href="#fnref110" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn111" role="doc-endnote"><p><span class="citation" data-cites="CreativeCommonsBiometrics">(<a href="#ref-CreativeCommonsBiometrics" role="doc-biblioref"><span>“Creative <span>Commons Biometrics</span>”</span> n.d.</a>)</span><a href="#fnref111" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn112" role="doc-endnote"><p>YFCC100M only contains links and metadata to images and videos under CC license onf Flickr<a href="#fnref112" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn113" role="doc-endnote"><p><span class="citation" data-cites="harveyExposingAiMegaFace">(<a href="#ref-harveyExposingAiMegaFace" role="doc-biblioref">Harvey n.d.b</a>)</span><a href="#fnref113" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn114" role="doc-endnote"><p><span class="citation" data-cites="hillHowPhotosYour2019">(<a href="#ref-hillHowPhotosYour2019" role="doc-biblioref">Hill and Krolik 2019</a>)</span><a href="#fnref114" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn115" role="doc-endnote"><p>The Open Future Foundation is a think tank that actively tries to influence european digital policy debates. See <span class="citation" data-cites="OpenFutureOpen">(<a href="#ref-OpenFutureOpen" role="doc-biblioref"><span>“Open <span>Future</span> – <span>Open Future Foundation</span>”</span> n.d.</a>)</span><a href="#fnref115" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn116" role="doc-endnote"><p><span class="citation" data-cites="karrasStyleBasedGeneratorArchitecture2019">(<a href="#ref-karrasStyleBasedGeneratorArchitecture2019" role="doc-biblioref">Karras, Laine, and Aila 2019</a>)</span><a href="#fnref116" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn117" role="doc-endnote"><p>At the time of this writing the url redirects to stability.ai, an archived version can be found. See <span class="citation" data-cites="ThisPersonDoes2019">(<a href="#ref-ThisPersonDoes2019" role="doc-biblioref"><span>“This <span>Person Does Not Exist</span>”</span> 2019</a>)</span><a href="#fnref117" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn118" role="doc-endnote"><p>See <span class="citation" data-cites="schaferThisPersonDoes2021">(<a href="#ref-schaferThisPersonDoes2021" role="doc-biblioref">Schäfer 2021</a>)</span><a href="#fnref118" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn119" role="doc-endnote"><p><span class="citation" data-cites="FFHQDatasetSearch">(<a href="#ref-FFHQDatasetSearch" role="doc-biblioref"><span>“<span>FFHQ</span> Dataset Search Form”</span> n.d.</a>)</span><a href="#fnref119" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn120" role="doc-endnote"><p>See ‘FFHQ Dataset’ at <span class="citation" data-cites="CreativeCommonsBiometrics">(<a href="#ref-CreativeCommonsBiometrics" role="doc-biblioref"><span>“Creative <span>Commons Biometrics</span>”</span> n.d.</a>)</span><a href="#fnref120" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn121" role="doc-endnote"><p><span class="citation" data-cites="karrasStyleBasedGeneratorArchitecture2019">(<a href="#ref-karrasStyleBasedGeneratorArchitecture2019" role="doc-biblioref">Karras, Laine, and Aila 2019</a>)</span><a href="#fnref121" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn122" role="doc-endnote"><p><span class="citation" data-cites="schaferThisPersonDoes2021">(<a href="#ref-schaferThisPersonDoes2021" role="doc-biblioref">Schäfer 2021</a>)</span><a href="#fnref122" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn123" role="doc-endnote"><p><span class="citation" data-cites="lavigneScrapism">(<a href="#ref-lavigneScrapism" role="doc-biblioref">Lavigne n.d.</a>)</span><a href="#fnref123" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn124" role="doc-endnote"><p>See <span class="citation" data-cites="XTRAMarkHansen">(<a href="#ref-XTRAMarkHansen" role="doc-biblioref"><span>“X-<span>TRA</span> → <span>Mark Hansen</span> and <span>Ben Rubin</span>: <span>Listening Post</span>”</span> n.d.</a>)</span> and <span class="citation" data-cites="ArsElectronicaArchiv">(<a href="#ref-ArsElectronicaArchiv" role="doc-biblioref"><span>“Ars <span>Electronica Archiv</span>”</span> n.d.</a>)</span><a href="#fnref124" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn125" role="doc-endnote"><p><span class="citation" data-cites="lavigneNewYorkApartment">(<a href="#ref-lavigneNewYorkApartment" role="doc-biblioref">Lavigne and Brain n.d.</a>)</span><a href="#fnref125" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn126" role="doc-endnote"><p><span class="citation" data-cites="samlavigneJustDiscoveredOpen2020">(<a href="#ref-samlavigneJustDiscoveredOpen2020" role="doc-biblioref">Sam Lavigne 2020</a>)</span><a href="#fnref126" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn127" role="doc-endnote"><p>See <span class="citation" data-cites="schaferMissingPictures">(<a href="#ref-schaferMissingPictures" role="doc-biblioref">Schäfer n.d.</a>)</span><a href="#fnref127" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn128" role="doc-endnote"><p>See <span class="citation" data-cites="NutzungsbedingungenInstagramHilfebereich">(<a href="#ref-NutzungsbedingungenInstagramHilfebereich" role="doc-biblioref"><span>“Nutzungsbedingungen | <span>Instagram-Hilfebereich</span>”</span> n.d.</a>)</span><a href="#fnref128" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn129" role="doc-endnote"><p>Complete list of artists can be found here https://doggg.art/artistlist.html<a href="#fnref129" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn130" role="doc-endnote"><p>See <span class="citation" data-cites="chakelianJourneyManyFaces2021">(<a href="#ref-chakelianJourneyManyFaces2021" role="doc-biblioref">Chakelian 2021</a>)</span><a href="#fnref130" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn131" role="doc-endnote"><p>See <span class="citation" data-cites="InternetYamiIchi">(<a href="#ref-InternetYamiIchi" role="doc-biblioref"><span>“The Internet Yami-Ichi”</span> n.d.</a>)</span><a href="#fnref131" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn132" role="doc-endnote"><p>See <span class="citation" data-cites="karrasAnalyzingImprovingImage2020">(<a href="#ref-karrasAnalyzingImprovingImage2020" role="doc-biblioref">Karras et al. 2020</a>)</span><a href="#fnref132" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn133" role="doc-endnote"><p>Ibid.<a href="#fnref133" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn134" role="doc-endnote"><p>See <span class="citation" data-cites="JabberwackyThoughtsArtificial2006">(<a href="#ref-JabberwackyThoughtsArtificial2006" role="doc-biblioref"><span>“Jabberwacky - <span>About Thoughts</span> - <span>An Artificial Intelligence AI</span> Chatbot, Chatterbot or Chatterbox, Learning <span>AI</span>, Database, Dynamic - Models Way Humans Learn - Simulate Natural Human Chat - Interesting, Humorous, Entertaining”</span> 2006</a>)</span> &amp; <span class="citation" data-cites="Cleverbot">(<a href="#ref-Cleverbot" role="doc-biblioref"><span>“Cleverbot”</span> n.d.</a>)</span><a href="#fnref134" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
</body>
</html>
