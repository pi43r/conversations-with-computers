<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Conversations with Computers</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      word-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
    }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  
</head>
<body>
<header id="title-block-header">
<h1 class="title">Conversations with Computers</h1>
</header>
<h1 id="piles-of-data">Piles of Data</h1>
<h2 id="self-learning-networks">Self-Learning Networks</h2>
<p>The idea behind Artificial Neural Networks has a long standing history. Using our understanding of the brain as a blueprint for mathematical operations dates back to the 1950s when the psychologist Frank Rosenblatt developed the <em>perceptron</em>.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> Inspired by nerve cells and their connections (synapses), the perceptron takes multiple input values, sums them up and outputs a 0 or 1 depending if a predefined threshold is reached. This system can be ‘trained’ by using positive and negative reinforcement to define the weights for each connection. With an apparatus, the Mark-I Perceptron,<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> that uses photoreceptors to turn light into ‘bits’ (today we would say pixels) the perceptron could ‘sense’ shapes in the form a binary matrix and distinguish between circles, squares and triangles. He proposed that a network of perceptrons could possibly even recognize faces and objects. Even though he developed the perceptron in 1964, Frank Rosenblatt never got to see his invention take off.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> Another engineer, Kunihiko Fukushima, kept refining his methods in the 70s by adding multiple layers, effectively creating the first ‘deep neural network’ where deep just means the depth of ‘hidden’ or inbetween layers connecting the input signal to the output classifier.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> He called this self-organizing system Cognitron<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> which was successful at accurately recognizing numbers and letters on a 12x12 grid. It’s successor the Neocognitron<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> took further inspiration from the visual cortex and a discoveries by Hubel &amp; Wiesel made in the 1950s that some biological neurons selectively respond to local features like lines, edges or color and others to figures like circles, squares or even human faces. This is also the core idea behind convolutional neural networks (known as ConvNet or CNN) which separate an image into a smaller grid and apply a certain filter to them, e.g. checking for edges. The french computer scientist Yann LeCun came up with ConvNets in the 1980s which are <em>the</em> driving force for AI systems today. Additionally Geoff Hinton, a cognitive psychologist and computer scientist, popularized the backpropagation algorithm in 1986 which finally made it possible for filters to tune themselves instead of relying on predefined rules.</p>
<p>Most conceptual ideas behind current deep weighted networks were already present in Frank Rosenblatt papers,<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> but weighted networks were often outperformed by rule-based systems. So what changed when Alex Krizhevsky, a student of Hinton, made a phenomenal leap in 2012 on the ImageNet classification competition? The main innovation is outlined by Krizhevsky in the paper itself: “To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation.”<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> Until AlexNet was released it was incredibly time consuming to run the conditioning process on the CPU which can only do one operation of matrix multiplication at the time. The GPU as the name Graphics Processing Unit suggests was originally designed to calculate 3D scenes and render them on a display. This involves a lot of matrix and vector operations and to accelerate them GPUs are capable at calculating large blocks of data in parallel. This means that conditioning AlexNet on 1.2 million pictures took only 6-9 days on two consumer graphics cards compared to probably weeks or months without them. However it was not the first system that utilized the GPU, it is similar to a CNN by Dan C. Cireȿan et al. released a year prior which has a reported speedup of 60 times for large weighted networks.<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a> The other roadblock for deep weighted networks is ‘overfitting.’ In the case of the ImageNet competition that would mean that the model adapts to the image so closely that it would simply reproduce the categories of the image without being able to identify new pictures which were not inside the training set. The most common way to reduce overfitting is to have a sufficiently large dataset with a high amount of variance. For AlexNet the 1000 classes and 1.2 million images were still not enough and they used data augmentation which transforms, flips or changes the color of an image to increase the training set by a factor of 2048. This means that—in theory—a larger dataset increases the robustness of the weighted network.</p>
<p>In conclusion, the conceptual framework of how weighted networks (or artificial neural nets, or perceptrons, or cognitrons) work was mostly established in the past century. Their performance today comes down to increased computing power and larger training sets. In fact datasets have become bigger and bigger and we haven’t seen the limits of what weighted networks are capable of, they seem to be mainly limited by data and computation. In the following chapter I will have a closer look at how large training sets are constructed in an academic and artistic context—both my own and a few other examples—and the ethical issues and responsibilities regarding privacy, consent, representation and ownership.</p>
<h2 id="making-data-11076-hands">Making Data, 11076 hands</h2>
<p>My first encounter with a large dataset in the field of computer vision was in late 2017 when I found <em>11k Hands</em>.<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a> As the name suggests it is a dataset of 11076 human hands compiled by the researcher Mahmoud Afifi for gender recognition and biometric identification. The images show the hands of 190 individuals, in both their palm and dorsal orientation, placed on a white background with uniform lighting. Each image is accompanied by the following metadata: age, binary gender, skin color on a scale of “very fair,” “fair,” “medium” and “dark,” left or right hand, palm or dorsal, if it has nail polish and if there are “irregularities.” A statistical analysis of the dataset shows that it contains more female than male hands, mainly people in there 20s and a majority of “medium” and “fair” skin tones. The gender bias is addressed in the paper and mitigated by filtering the training set to have an equal amount of males and females. They report that the CNN conditioned on this dataset had on average a 87% accuracy recognizing the correct gender on an image of the palm and a 90% accuracy for the dorsal side. But it is not the ‘state-of-the-art’ results that drew me to the paper, it was the gender and skin-tone classification itself that appalled me. It reminds me of phrenology in the 19th and 20th century, a popular pseudoscience that claimed that persons character and mental abilities could be determined by the shape of their skull. Phrenologists like Franz Joseph Gall went to a great length measuring and categorizing human skulls and associating certain regions to human traits. While it didn’t take scientists to debunk the idea that bumps on a head could indicate the characteristics of a person, it had a comeback in the early 20th century when it was applied to racist and sexist stereotypes and used to justify Nazi eugenics. The underlying assumption that the shape of the head has anything to do with the mental state of the person was simply wrong.<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a> Considering that gender is a social construct and not necessarily a binary choice trying to use a computer to identify if a person is male or female by analyzing their hand seems arbitrary at best and reinforcing gender stereotypes at least.<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a> Another aspect of the dataset stood out to me when I started to look through the images. These were not pictures in the traditional sense, their aesthetical value did not matter as they are simply a tool to to accomplish a task. They are not made ‘to look at’ instead these images of hands are produced for a computer to analyze. Harun Faroki called these types of images ‘operative’ in his three-part series <em>Machine/Eye</em> where he examined how military technologies like guided weapons produce images that serve only the utility of the machine. As Aud Sissel Hoel puts it: “operative images are devoid of social intent, that they are not meant for edification, and nor for contemplation.”<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a> But what happens when you contemplate on the hand dataset was remarkable to me. When I looked through the images quick enough I could not only see the motion of the test subject as the images were selected frames from a video, but I also started to imagine the person behind the hand and apply my own stereotypes and biases that far expanded the labels of the dataset. I could easily imagine the scene around the camera with a couple researchers who assembled a make-shift photo studio in the office of their lab. They greet the subject with a handshake, which is probably a fellow student, and explain the procedure quickly to get them to sign the paper that their anonymized personal data will be published for scientific research. Then the person puts both their hands under the camera spread their fingers and leave. This dichotomy between the label and my own narrative inspired me to create a video piece featuring the unaltered dataset. I used the default mac computer speech synthesis to read out the labels corresponding to each hand, and sped it up to fit into a 26-minute-long video. As viewers witness the participants holding their hands into the camera and spreading their fingers, they are met with a monotone, beat-like soundtrack of repeating words like “fair” and “medium” and occasionally “nail polish.”<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a> <img src="Matthias%20Schäfer%20-%2011k%20hands%20%5BsnJsKFxPlJ8%20-%20892x669%20-%203m35s%5D.png" alt="Example of a frame from 11k hands" /></p>
<p>At the 36th Chaos Communication Congress I organized a workshop called “Palm Reading AI” where I invited visitors to read the hands of people from the 11k hands dataset. At first I only introduced them to palmistry using wikihow as a reference.<a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a> Then I handed out some prints where a random hand from the dataset was depicted and participants had to fill in a couple of questions. Some questions were short guesses like age, gender, country of origin, for some other they had to come up with fictional stories with only the hand lines as a reference: what is the persons future? How was their childhood? How is their love life? After they filled in the form some people shared their stories and then I revealed where these hands came from and how computer scientists are using them to create models that try to predict their gender. Afterwards we had a discussion about the practice of creating large datasets and their ethical considerations. I had a longer talk with one participant that did not want to guess the age or gender of that person and I had told them that this was exactly the point of the workshop: to reflect on our own biases and stereotypes and how they translate into science.</p>
<p>After long contemplation on 11k hands and finding datasets that are much more problematic than this one, I don’t think the type of work from Afifi et al. is ‘unethical’ or needs to be redacted. They got consent from their subjects and share the dataset to the scientific community for “reasonable academic fair use.” The work on biometric identification and comparing CNNs to previous methods is interesting and novel, however as I stated above I think the premise behind the gender recognition task is flawed. Unfortunately this is very common in the computer vision field where people are (mis-)labeled that reflect and amplify societal stereotypes.<a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a> My research on the hands dataset in conjunction with esoteric practices and fortune-telling informed a later work of mine “The Chiromancer” that I built together with Giacomo Piazzi.</p>
<h2 id="in-the-wild">In the wild</h2>
<p>The process of collecting and creating data has drastically changed since the wide adoption of the internet. Where the 11k hands dataset has invited participants to their instute to take a picture specifically for the dataset, other researcher started to search and download huge collections from the internet without any consent. Take for example the ImageNet dataset, initiated by Stanford University’s AI professor Fei Fei Li, which was created to tackle object recognition tasks and eventually consisted of 14 million images.<a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a> The team queried multiple search engines with common nouns and multiple translations to get around 500-1000 images per category. The categories are derived from an older project called WordNet, created in 1985 at Princeton University, that tried to achieve a hierarchical ontology of words. For example the noun <em>human</em> is synonym to <em>homo, man and human being</em> which are subcategories of the <em>hominid</em> class, which are <em>primates</em>, which are <em>mammals</em>, which are <em>animals</em>, which is an <em>organism</em> and finally part of the <em>entity</em> class. After downloading millions of images in thousands of categories they were first automatically deduplicated but the task of checking whether the image actually depicts the search term could not be done by an algorithm. Originally Fei Fei Li wanted to hire graduate students to sort the data which was too costly and time consuming.<a href="#fn18" class="footnote-ref" id="fnref18" role="doc-noteref"><sup>18</sup></a> One of her undergraduates suggested to use a new service called Amazon Mechanical Turk where people around the world complete small task for little compensation. Even with the help of Mechanical Turk it took 2,5 years to sort and validate the first dataset with 3.2 million images in over 5000 categories. At some point ImageNet was the largest academic client for Amazon’s Turk service and after the popularization of weighted networks they became a corner stone of data annotation. Similar platforms were established and Mechanical Turk grew to an estimate of half a million workers, which were rigorously exploited and alienated as a so called click workers. One paper estimates their median hourly wage to 2$ and only the top 4% earning more than 7.25$ per hour.<a href="#fn19" class="footnote-ref" id="fnref19" role="doc-noteref"><sup>19</sup></a> If the task is rejected by the requester the worker does not get paid at all and has no way of appealing that decision. To advocate for better working conditions the designer and researcher Caroline Sinders built an open source tool for data annotation and a wage calculator to better estimate the cost of labeling.<a href="#fn20" class="footnote-ref" id="fnref20" role="doc-noteref"><sup>20</sup></a><br />
With so many different people working on labeling images, societal biases are eventually reflected in the dataset. In 2019 the artist Trevor Paglen and researcher Kate Crawford collaborated on an exhibition titled <em>Training Humans</em>, dedicated to human image datasets. One of the main exhibits was a vast collection of human images from ImageNet.<a href="#fn21" class="footnote-ref" id="fnref21" role="doc-noteref"><sup>21</sup></a><br />
As you go further down the category tree you might find images of people that fall into categories such as “Bad Person, Call Girl, Drug Addict, Closet Queen, Convict” and so on.<a href="#fn22" class="footnote-ref" id="fnref22" role="doc-noteref"><sup>22</sup></a> The artists used these absurd, racist, and misogynistic labels to train <em>ImageNet Roulette</em>,<a href="#fn23" class="footnote-ref" id="fnref23" role="doc-noteref"><sup>23</sup></a> a recognition algorithm that was accessible online and in an interactive installation. People online quickly picked up on the tool and shared images of themselves with sometimes amusing and often deeply problematic image captions.<a href="#fn24" class="footnote-ref" id="fnref24" role="doc-noteref"><sup>24</sup></a> While some of the people in the research community defended ImageNet that these offensive labels are not part of competition and make up only a small fraction of the total dataset, as a result of the media attention that followed more than 600,000 images were removed. It is now only accessible after proving to be part of a scientific institute.</p>
<p>Another pair of artists and researchers, Adam Harvey and Jules LaPlace, have been exposing image sets which often get revoked and removed after publishing their investigation.<a href="#fn25" class="footnote-ref" id="fnref25" role="doc-noteref"><sup>25</sup></a> Harvey and LaPlace focus on datasets with faces that are captured “in the wild.” One particular example was the <em>UnConstrained College Students (UCCS)</em> dataset captured at the University of Colorado Colorado Springs. According to the authors of the associated papers to the dataset they identified that current image sets created for face recognition research did not address the presence of unknown subjects. The authors of the papers associated with this dataset wanted to create a more realistic benchmark for face recognition research by introducing unknown subjects over time. To achieve this, they captured students on campus during breaks using a 800mm telephoto lense from a distance of 100-150 meters through an office window. The authors frame it as a benefit that the students are unaware of their capturing for the dataset as they casually focus on random activities and their faces are sometimes blurred and occluded. To make things worse this research was mainly funded by US defense and intelligence agencies. The public backlash was immense and as a result the dataset is no longer publicly available. The researchers in question did not apologize and argued that their subjects are anonymous as their names or other identifiers are not published.<a href="#fn26" class="footnote-ref" id="fnref26" role="doc-noteref"><sup>26</sup></a> However when researchers need unconstrained and non-consensual data, they do not often capture them directly using creepy surveillance tactics. Starting with “Labeled Faces in the wild” in 2007 the practice of collecting and labeling image data from internet sources has become normalized and is still largely unregulated. Often these image sets “in the wild” operate in a gray zone where they either depict public figures, arguing that privacy regulations do not apply to them, or that they simply link to the file and only store them temporarily for analysis. A third option uses media licensed under Creative Commons (CC) which is mostly considered free and legal data with no restrictions in the AI research community.<a href="#fn27" class="footnote-ref" id="fnref27" role="doc-noteref"><sup>27</sup></a> In a detailed report Adam Harvey lists many datasets that exploit and often misuse CC licenses to create face and object recognition datasets.<a href="#fn28" class="footnote-ref" id="fnref28" role="doc-noteref"><sup>28</sup></a> He identifies Flickr as a major source for collecting images, a popular image sharing website where users can choose from various Creative Commons, copyright and public domain licenses. Flickr actively promoted the use of CC licenses and offered unlimited free hosting if a CC license is used. Their strategy worked and by 2022 they amassed 467 million CC licensed images. In 2014 a joint research group including Yahoo Labs, the company that bought Flickr, shared one of the largest public media datasets<a href="#fn29" class="footnote-ref" id="fnref29" role="doc-noteref"><sup>29</sup></a> with the name <em>Yahoo! Flickr Creative Commons 100 Million (YFCC100M)</em>. A multitude of specified datasets were created from this corpus. One of them is the <em>MegaFace</em> face recognition dataset with 4,7 million faces from 672,057 identities. While all of the images fall under a CC license most of them prohibit their commercial use and require appropriate attribution, which was not given in the dataset. As Harvey and LaPlace verified, the <em>MegaFace</em> dataset was used by globally by commercial, military and academic organizations.<a href="#fn30" class="footnote-ref" id="fnref30" role="doc-noteref"><sup>30</sup></a> As an investigative article from the New York Times explains, most people are unaware that their images are powering face recognition research around the world, including companies with ties to Chinas surveillance on the Uyghur population.<a href="#fn31" class="footnote-ref" id="fnref31" role="doc-noteref"><sup>31</sup></a> To conclude the three common issues with CC licensed media is that they are not or wrongly attributed, the use of non-consensual biometric data is prohibited in some places (e.g. Illinois) and the use in commercial applications is often prohibited. Many of thes issues identified by Harvey and LaPlace remain until today and operate in a legal grey zone. While the notion behind uploading images under an open license for others to freely share, distribute and remix media is a noble goal, the CC license is legally weak and practically useless against opting out of statistical analysis in AI research. But we might be slowly moving into a direction where law makers catch up and create precedences that disallow the use of biometric data in certain applications, like the upcoming AI Act.<a href="#fn32" class="footnote-ref" id="fnref32" role="doc-noteref"><sup>32</sup></a></p>
<h2 id="this-person-does-exist">This Person Does Exist</h2>
<p>A dataset using Creative Commons images that I examined more closely was <em>Flickr Faces HQ (FFHQ)</em>. In 2018 the research lab of Nvidia, one of the leading companies for visual computing, published a paper introducing a machine learning architecture called StyleGAN.<a href="#fn33" class="footnote-ref" id="fnref33" role="doc-noteref"><sup>33</sup></a> They improved on generative adversarial networks (GAN) in such a way that it was possible to create controllable synthetic high-resolution images. In simple terms the system is able to abstract large amounts of images with a model that in turn outputs similar-looking pictures. In this case the model is able to generate realistic-looking photographs of human faces. In comparison to other datasets <em>FFHQ</em> is fairly small with only 70,000 images. As existing datasets were too low in resolution a new corpus was created by scraping Creative Commons, Public Domain or U.S. Government Works licensed images through Flickr’s API. The dataset itself is published under a CC-BY-NC-SA license and the instructions for use and download are very clear, making it manageable for me in terms of size and effort to discover the underlying characteristics. The dataset consists only of photographic images as it was checked by Amazon Mechanical Turk workers to make sure that statues, paintings, or photos of photos were removed. Using the open-source library dlib, the raw images were automatically aligned and cropped around the face to create a uniform square ratio of 1024px. This library also identifies 68 points outlining the chin, eyebrows, eyes, nose and mouth which are included in the metadata. So the final dataset used for training the GAN consists of a multitude of faces whe the eye and mouth positions are always in the same spot.</p>
<figure>
<img src="Screen%20Shot%202023-03-15%20at%2020.50.49.png" alt="Screenshot of This Person Does Exist" /><figcaption aria-hidden="true">Screenshot of This Person Does Exist</figcaption>
</figure>
<p>One year after Nvidia released its StyleGAN paper the software developer Phillip Wang published the website <em>thispersondoesnotexist.com</em> which shows the capabilities of the generative model to create realistic looking photographs of people.<a href="#fn34" class="footnote-ref" id="fnref34" role="doc-noteref"><sup>34</sup></a> The site quickly took off and alarmed people about potential impact of AI systems in generating cheap synthetic media.<br />
As a counter-narrative to the AI image creator, I wanted to showcase the people who were used to train this system. In 2020 I moved the cropped and aligned face images to my server and built my own website with the name <em>this-person-does-exist.com</em> which displays the faces from the training set alongside their metadata.<a href="#fn35" class="footnote-ref" id="fnref35" role="doc-noteref"><sup>35</sup></a> Looking at the individuals faces facilitates an interpersonal connection with the unknown person and evokes a feeling for the images that were used to train the generative model. At the same time it shows the creepy and strange practice of AI researchers using personal images as raw data. As Flickr is used mostly by hobbyists and professional photographers, one can find portraits of children and families, speakers at conferences or people on holiday. We can assume that, like other scraped datasets, the photographers of these images are unaware that their images are used for AI analysis. In contrast to other scraped datasets, the Nvidia researchers provide a tool to see if an image is part of the collection and allow the removal of the photograph.<a href="#fn36" class="footnote-ref" id="fnref36" role="doc-noteref"><sup>36</sup></a> According to Adam Harvey the company does not disclose if any images were requested for removal and has not updated or removed any photos since its release in 2016.<a href="#fn37" class="footnote-ref" id="fnref37" role="doc-noteref"><sup>37</sup></a></p>
<p><img src="Fig2.png" alt="70k faces from FFHQ compiled into one image" /> <img src="Fig3.png" alt="The Flickr Face - averaged FFHQ dataset" /></p>
<p>To get a sense of scale of the dataset, I compiled all face images into a grid, reducing the size of each image to 16 by 16px. This simple montage makes it possible to get a feeling for the vast amount of normalized image data. The authors claim that FFHQ includes more variety than other face image sets in terms “of age, ethnicity and image background, and also has much better coverage of accessories such as eyeglasses, sun-glasses, hats, etc.”<a href="#fn38" class="footnote-ref" id="fnref38" role="doc-noteref"><sup>38</sup></a> They admit to biases inherited from the Flickr platform but fail to mention them. Looking through the data it becomes clear that a majority of the images are taken with high resolution digital camera systems under good lighting conditions. Many of the pictures show separate the subject with a blurry background, so called ‘bokeh,’ and conform to photographic aesthetic norms of the early 2000s until today. Indeed, the dataset contains a variety of skin tones, age groups and background but they are not equally distributed. To visually find a bias in image sets I averaged the pixel values of the images. This suppresses outliers, but it allows us to see an overall trend of the dataset. The resulting composite image The Flickr Face, I belief, reveals a trend towards smiling and light-skinned people in the data set. All these are highly subjective impressions, but I would not presume any ethnicity or categorize people by skin tone. I understand that this project is doing the same as the research labs in question, but I hope by making people uncomfortable and showing the human behind AI systems, we can get a better feeling for how creepy this harvesting of faces is.</p>
<p>As a part of a growing group of artists exploring and exposing research datasets, I coined a term for this genre: Dataset Art. My paper on this subject was published in <em>Temes de Disseny</em> in 2021 and includes a couple more examples. Through their works, these artists are able to make large datasets understandable and captivating. Their art has sometimes even created enough attention to lead some institutions to remove questionable parts or entire collections. Whether through galleries or online, Dataset Art is providing an exciting new way to peek into the workings of AI systems.<a href="#fn39" class="footnote-ref" id="fnref39" role="doc-noteref"><sup>39</sup></a></p>
<h2 id="scrapism">Scrapism</h2>
<p>Web scraping is the technique of using computer programs to automatically visit links and aggregate data from the internet. It is the backbone for many of the current machine learning applications. The artist Sam Lavigne uses the practice of web scraping with goal of creating art with an emotional or critical message; a practice he calls ‘Scrapism.’<a href="#fn40" class="footnote-ref" id="fnref40" role="doc-noteref"><sup>40</sup></a> Perhaps an early work using the internet as a data source to convey emotion is <em>Listening Post</em> by Mark Hansen and Paul Ruben. The work, that got awarded the Golden Nica in 2004, uses snippets from internet chat rooms and displays them on over 200 LED signs while a crude computer voice reads them out loud. The artists are transforming live data into a sensual experience where the viewer gets to enter a dark room and listen in to the worlds chatter happening simultaneously all over the planet.<a href="#fn41" class="footnote-ref" id="fnref41" role="doc-noteref"><sup>41</sup></a> Instead of using and exposing datasets made for scientific research, Lavigne creates his own datasets by downloading and analyzing materials on the internet to revert common power structures. For example the online art work <em>New York Apartment</em> he produced together with Tega Brain for the Artport Collection of the Whitney Museum.<a href="#fn42" class="footnote-ref" id="fnref42" role="doc-noteref"><sup>42</sup></a> In this work they collected all for-sale listings on multiple real estate websites for New York City apartments and created a website that compiles all of them into one giant apartment listing. The value for this fictional apartment is over 43 billion USD and boasts 65,674 bedrooms and 55,588 bathrooms on around 3.4 million square meters. The website consists of multiple columns describing the listing with all of the clichéd language and staged photographs common in the real estate market. They extruded the floor plans into 3D models and placed them together next to each other, in a tower or pyramid formation so that you can explore this maze of apartments. The videos are cut up into thematic categories like “Welcome,” “Bedroom,” “Master” or “Pre-war” creating strange super cuts of panning and zooming shots of slick interiors. Although looking through this compilation is funny and entertaining, it reveals the absurd language of luxury commodities and reveals the inequality of who can afford to own housing in a city like New York. Another experiment by Sam Lavigne uses an open source hair detector to create a compilation of Mark Zuckerberg hair styles, in reaction to multiple people mocking the billionaire for his ‘terrible’ hair style.<a href="#fn43" class="footnote-ref" id="fnref43" role="doc-noteref"><sup>43</sup></a> This project in particular surved me as an inspiration as it shows that using web scraping and machine learning technologies can sometimes just be for silly projects on the expense of billionaires funding this tech for surveillance and personal profits.</p>
<figure>
<img src="lavigne_zuck-cut.jpg" alt="Sam Lavigne - Zuckerhair" /><figcaption aria-hidden="true">Sam Lavigne - Zuckerhair</figcaption>
</figure>
<h2 id="hidden.pictures">hidden.pictures</h2>
<p>When downloading things from the web, we often assume that everything is stored for ertenity. “The web never forgets” is a common phrase that is used to warn people before uploading sensitive and personal content online. This can be true for content that is widely shared or automatically scraped and stored on sites like archive.org, but for many files and links the web is brittle. Domain names expire and cut the link to the requested page. But even if the hypertext can be accessed they often contain links to files that do not exist on the server anymore, in the case of images the browser then decides to show a broken image icon and an empty rectangle. For the online artwork <em>hidden.pictures</em> (previously called <em>empty.photos</em>) I created a web crawler that visits random URLs and whenever it hits an image that did not load correctly it gets stored into a database. I collected thousands of broken image links together with their metadata that sometimes describes the image.<a href="#fn44" class="footnote-ref" id="fnref44" role="doc-noteref"><sup>44</sup></a> I present the viewer with a collage of randomly placed image rectangles on a blank white website. A visitor can pan around in all directions finding the default rendering of their web browers for broken images, often showcasing a ripped paper icon on the top left corner. When hovering over one of the boxes the original url is shown on the bottom left corner of the page and whenever it existed the alt-tag pops up next to the cursor. These images from thousands of blogs, shops and forums show the forgotten and neglected part of the internet. It invites for the imagination about the internet that is not getting maintained and reveals an even bigger source of data that does not exist anymore. When looking at this through the lense of weighted networks and training data, we have to ask the question what these models are actually learning when so many things get missing on the net every day. On the other hand the models themselves become a blurry snapshot of the things they were able to gather, but might not exist in the future.</p>
<figure>
<img src="emptyphotos-IMG_2261-HDR%201.jpg" alt="empty.photos exhibited at Best Off 2019" /><figcaption aria-hidden="true">empty.photos exhibited at Best Off 2019</figcaption>
</figure>
<h2 id="doggg.art">doggg.art</h2>
<p>In 2020 I created doggg.art, an exercise in <em>scrapism</em> where I downloaded and transformed content from the social media giant Instagram. Instagram has become the biggest tool for artists to find an audience and a community. Every possible niche can be found through the use hashtags like #dogart that collects drawings, photographs of personal and commercial dog related imagery. With around a million posts it is only one example for the immense creative output on the image platform. Facebook, the mother company of Instagram, owns an abundant amount of image data which they analyze and use to optimize computational models. By posting on the platform a user “grant[s] a non-exclusive, royalty-free, transferable, sub-licensable, worldwide license to host, use, distribute, modify, run, copy, publicly perform or display, translate, and create derivative works of [their] content.”<a href="#fn45" class="footnote-ref" id="fnref45" role="doc-noteref"><sup>45</sup></a> I would describe doggg.art as a generative big dada collage consisting of over 30k images from Instagram tagged with #dogart. The images were processed using a machine learning algorithm called U^2-Net, which removed the background and other elements from the pictures. A website then randomly places the cutouts on a beige background slowly fading them in, creating an ever changing digital dog meadow. I see it as a collaborative work that combines pieces by 38326 artists together credited on a seperate page with all their unique usernames.<a href="#fn46" class="footnote-ref" id="fnref46" role="doc-noteref"><sup>46</sup></a> The artwork was intended as a screensaver that reflects on on the aesthetics of the platform Instagram and how our relationship with pets extends into this online network.</p>
<figure>
<img src="doggg.art%20Digitale.jpeg" alt="doggg.art exhibited at Die Digitale Düsseldorf exhibition “Digital Jokes”" /><figcaption aria-hidden="true">doggg.art exhibited at Die Digitale Düsseldorf exhibition “Digital Jokes”</figcaption>
</figure>
<figure>
<img src="doggg.art%20Digitale%202.jpeg" alt="A visitor looking through the names of dog artists" /><figcaption aria-hidden="true">A visitor looking through the names of dog artists</figcaption>
</figure>
<h2 id="a-study-on-the-characteristics-of-douyin-meanwhile-in-china">A study on the Characteristics of Douyin / Meanwhile in China</h2>
<p>Working with and about social networks has been a big part in my artistic work. In 2019 I was invited by UNESCO to participate in a residency in the city of Changsha, the capital of the Hunan region and “City of Media Arts” in central China. While I was there I tried to make sense of the information landscape around me and got hooked to the chinese clone of TikTok. Douyin 抖音 became one of the most successful apps worldwide as the leading platform for creating and sharing short videos. Created by the Beijing based company Bytedance, it is one of the few Apps that got successful outside of the great firewall. To comply with chinese law Douyin is a completely separate App from TikTok. Even though the interface and logo looks the same, the content is completely different, not accessible from the international version. I asked S()fia Braga to join me as a collaborator, inspired by the work of <em>I stalk myself more than I should</em> where she captures Instagram stories, a feature designed to expose a video for a limited duration of 24 hours, and reappropriates them in a video installation. The aim of our work was to explore and analyze the vast digital ecosystem of Douyin from different perspectives, using their recommendation algorithm to lead us to different aspects of chinese social media. Using screen recordings to capture hours of video footage of us scrolling through our feeds. These found images are then decontextualized without alteration to give visitors the space to reflect upon them, gain insight into a walled off platform and into the algorithms designed for user engagement.<br />
We made two video installation, running an 8 hours of captured material, showing people dancing next to chinese police forces schowcasing their equipment. Sometimes we would show the blank interface showing blank search results. Search terms like Donald Trump result in videos that do not show the american president them self, indicating facial recognition being used for censorship. Additionally to the unaltered screen captured we created non-nonsensical graphs that invoke the feeling that the content is used for statistical analysis. The title <em>A study on the characteristics of Douyin</em> was taken from a sociological paper and gives the veneer of scientific legitimacy to the non-consensual stalking of capturing of our practice. In another exhibition at the Ars Electronica Festival in Linz the titled was changed to <em>Meanwhile in China</em> and the wall behind the video screen was covered with a collage of graphs and datapoints found online showcasing the exploding growth and user distribution on the platform. However the graphs themselves are stripped of any labeling, making them unreadable and useless as visualization.</p>
<figure>
<img src="IMG_1853.jpg" alt="A Study on the Characteristics of Douyin at Xie Zielong Photography Museum in Changsha" /><figcaption aria-hidden="true">A Study on the Characteristics of Douyin at Xie Zielong Photography Museum in Changsha</figcaption>
</figure>
<figure>
<img src="meanwhile_landscape.jpg" alt="Meanwhile In China at Ars Electronica 2019" /><figcaption aria-hidden="true">Meanwhile In China at Ars Electronica 2019</figcaption>
</figure>
<h2 id="recommended-hashtags-more-of-the-same">Recommended Hashtags &amp; More of the same</h2>
<p>I have had an account on Instagram since 2012 shortly after it was released on android devices. The social network was based around square images, often highly edited with filters that are inspired by old analog photos. When I joined Instagram I already had a critical view of large social networks like facebook where I had started to create online and offline performances. On Instagram I would often share references to technologies in the social sphere and where they break, like screenshots of loading spinners, weird advertisement or comments by bots. To get the attention outside of my friend network and reach even more bots I often used generic hashtags. The hashtag originated from the IRC protocol where # was used before the name of a chat room (channel) and became a popular way on twitter for tagging events, movements or topics from 2007 onwards.<a href="#fn47" class="footnote-ref" id="fnref47" role="doc-noteref"><sup>47</sup></a> Today it is a ubiquitous way of organizing and tagging content into categories, but it is mostly used to get attention inside of the network. For this, the game on social media is to add as many relevant and trending topics into the description as possible and many companies have sprung up to help users ‘optimize’ their hashtags. From 2015 I started to use hashtags for non-sensical and humorous image describtion and later on decided to use hashtag generators to create lists of irrelevant tags for my posts on Instagram as social commentary. In 2020 just after the first summer of the new pandemic reality S()fia Braga organized the <em>Internet Yami Ichi</em>, a black market for internet inspired products invented by the japanese art collective IDPW.<a href="#fn48" class="footnote-ref" id="fnref48" role="doc-noteref"><sup>48</sup></a> For this flea market I decided to print and sell the 5 years of my Instagram performance with a book titled <em>Recommended Hashtags</em>. Each of the 216 pages containing the image, hashtags and number of likes. It also had a dedication to all my followers at the end page. After downloading and printing my Instagram data, I deleted all my images on the platform and added the label ‘Post-Instagram Artist’ to my account. As users of Facebook’s platforms, we are aware that our content is being analyzed for profit. With my online performance, I aimed to subvert and add noise to the system, albeit not enough to make a real difference. Selling my book was a way to compensate for the ‘work’ I have been doing for Facebook. The book was sold in a staggering model where each edition doubled in cost from the previous one, making my old data more valuable over time, while it was completely free at the time of creation. In the following year 2021 I started a new performance titled <em>more of the same</em> on my account using the dataset I downloaded as source material for a generative adversarial network. I trained a StyleGAN model, which was originally invented to create synthetic human faces, on the images I posted from 2015 to 2020. The model however created abstract shapes, colors and textures instead of replicating my images. As ~200 pictures are a very small dataset and the images are not very uniform ranging from screenshots to photographs depicting objects or people the model could not converge on any meaningful representation. Many of the generated images are nearly symmetrical, reminiscent of Rorschach inkblots, which is the result of augmenting my image dataset by flipping the images. They also show circular blobs which is the result of the model architecture and is seen as an error of the weighted network.<a href="#fn49" class="footnote-ref" id="fnref49" role="doc-noteref"><sup>49</sup></a> In the end I wrote a small program that uploaded and posted a new generated image with the caption “more of the same” for 100 days.</p>
<figure>
<img src="more%20of%20the%20same.png" alt="Screenshot of more of the same | 200" /><figcaption aria-hidden="true">Screenshot of more of the same | 200</figcaption>
</figure>
<p><img src="Recommended%20Hashtags%200.jpg" alt="Book cover of Recommended Hashtags | 200" /> <img src="Recommended%20Hashtags%201.jpg" alt="First page of Recommended Hashtags | 200" /></p>
<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-2ndUnconstrainedFace" class="csl-entry" role="doc-biblioentry">
<span>“2nd <span>Unconstrained Face Detection</span> and <span>Open Set Recognition Challenge</span>, <span>ECCV-2018</span>.”</span> n.d. Accessed March 11, 2023. <a href="https://vast.uccs.edu/Opensetface/">https://vast.uccs.edu/Opensetface/</a>.
</div>
<div id="ref-afifi11KHandsGender2018" class="csl-entry" role="doc-biblioentry">
Afifi, Mahmoud. 2018. <span>“<span>11k Hands</span>: <span>Gender</span> Recognition and Biometric Identification Using a Large Dataset of Hand Images.”</span> September 16, 2018. <a href="https://doi.org/10.48550/arXiv.1711.04322">https://doi.org/10.48550/arXiv.1711.04322</a>.
</div>
<div id="ref-afifi11kHands" class="csl-entry" role="doc-biblioentry">
———. n.d. <span>“11k <span>Hands</span>.”</span> Accessed March 7, 2023. <a href="https://sites.google.com/view/11khands">https://sites.google.com/view/11khands</a>.
</div>
<div id="ref-ArsElectronicaArchiv" class="csl-entry" role="doc-biblioentry">
<span>“Ars <span>Electronica Archiv</span>.”</span> n.d. Accessed March 20, 2023. <a href="https://archive.aec.at/prix/showmode/88/">https://archive.aec.at/prix/showmode/88/</a>.
</div>
<div id="ref-chakelianJourneyManyFaces2021" class="csl-entry" role="doc-biblioentry">
Chakelian, Anoosh. 2021. <span>“The Journey and Many Faces of the Hash Symbol.”</span> <span>New Statesman</span>. June 9, 2021. <a href="https://www.newstatesman.com/science-tech/2014/06/history-journey-and-many-faces-hash-symbol">https://www.newstatesman.com/science-tech/2014/06/history-journey-and-many-faces-hash-symbol</a>.
</div>
<div id="ref-ciresanFlexibleHighPerformance" class="csl-entry" role="doc-biblioentry">
Ciresan, Dan C, Ueli Meier, Jonathan Masci, Luca M Gambardella, and Jurgen Schmidhuber. n.d. <span>“Flexible, <span>High Performance Convolutional Neural Networks</span> for <span>Image Classification</span>.”</span>
</div>
<div id="ref-CreativeCommonsBiometrics" class="csl-entry" role="doc-biblioentry">
<span>“Creative <span>Commons Biometrics</span>.”</span> n.d. Accessed March 11, 2023. <a href="https://adam.harvey.studio/creative-commons/">https://adam.harvey.studio/creative-commons/</a>.
</div>
<div id="ref-dempsey-jonesNeuroscientistsPutDubious2018" class="csl-entry" role="doc-biblioentry">
Dempsey-Jones, Harriet. 2018. <span>“Neuroscientists Put the Dubious Theory of ’Phrenology’ Through Rigorous Testing for the First Time.”</span> <span>The Conversation</span>. January 22, 2018. <a href="http://theconversation.com/neuroscientists-put-the-dubious-theory-of-phrenology-through-rigorous-testing-for-the-first-time-88291">http://theconversation.com/neuroscientists-put-the-dubious-theory-of-phrenology-through-rigorous-testing-for-the-first-time-88291</a>.
</div>
<div id="ref-dengImageNetLargeScaleHierarchical2009" class="csl-entry" role="doc-biblioentry">
Deng, J., W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. 2009. <span>“<span>ImageNet</span>: <span>A Large-Scale Hierarchical Image Database</span>.”</span> In <em><span>Cvpr09</span></em>.
</div>
<div id="ref-ExcavatingAI" class="csl-entry" role="doc-biblioentry">
<span>“Excavating <span>AI</span>.”</span> n.d. <span>-</span>. Accessed March 10, 2023. <a href="https://excavating.ai">https://excavating.ai</a>.
</div>
<div id="ref-FFHQDatasetSearch" class="csl-entry" role="doc-biblioentry">
<span>“<span>FFHQ</span> Dataset Search Form.”</span> n.d. Accessed March 15, 2023. <a href="https://nvlabs.github.io/ffhq-dataset/search/">https://nvlabs.github.io/ffhq-dataset/search/</a>.
</div>
<div id="ref-fukushimaCognitronSelforganizingMultilayered1975" class="csl-entry" role="doc-biblioentry">
Fukushima, Kunihiko. 1975. <span>“Cognitron: <span>A</span> Self-Organizing Multilayered Neural Network.”</span> <em>Biological Cybernetics</em> 20 (3-4): 121–36. <a href="https://doi.org/10.1007/BF00342633">https://doi.org/10.1007/BF00342633</a>.
</div>
<div id="ref-fukushimaNeocognitronHierarchicalNeural1988" class="csl-entry" role="doc-biblioentry">
———. 1988. <span>“Neocognitron: <span>A</span> Hierarchical Neural Network Capable of Visual Pattern Recognition.”</span> <em>Neural Networks</em> 1 (2): 119–30. <a href="https://doi.org/10.1016/0893-6080(88)90014-7">https://doi.org/10.1016/0893-6080(88)90014-7</a>.
</div>
<div id="ref-gershgornDataThatTransformed2017" class="csl-entry" role="doc-biblioentry">
Gershgorn, Dave. 2017. <span>“The Data That Transformed <span>AI</span> Research—and Possibly the World.”</span> <span>Quartz</span>. July 26, 2017. <a href="https://qz.com/1034972/the-data-that-changed-the-direction-of-ai-research-and-possibly-the-world/">https://qz.com/1034972/the-data-that-changed-the-direction-of-ai-research-and-possibly-the-world/</a>.
</div>
<div id="ref-haraDataDrivenAnalysisWorkers2017" class="csl-entry" role="doc-biblioentry">
Hara, Kotaro, Abi Adams, Kristy Milland, Saiph Savage, Chris Callison-Burch, and Jeffrey Bigham. 2017. <span>“A <span>Data-Driven Analysis</span> of <span>Workers</span>’ <span>Earnings</span> on <span>Amazon Mechanical Turk</span>.”</span> December 28, 2017. <a href="https://doi.org/10.48550/arXiv.1712.05796">https://doi.org/10.48550/arXiv.1712.05796</a>.
</div>
<div id="ref-harveyExposingAi" class="csl-entry" role="doc-biblioentry">
Harvey, Adam. n.d.a. <span>“Exposing.ai.”</span> <span>Exposing.ai</span>. Accessed March 11, 2023. <a href="https://exposing.ai/">https://exposing.ai/</a>.
</div>
<div id="ref-harveyExposingAiMegaFace" class="csl-entry" role="doc-biblioentry">
———. n.d.b. <span>“Exposing.ai: <span>MegaFace</span>.”</span> <span>Exposing.ai</span>. Accessed March 13, 2023. <a href="https://exposing.ai/datasets/megaface/">https://exposing.ai/datasets/megaface/</a>.
</div>
<div id="ref-hillHowPhotosYour2019" class="csl-entry" role="doc-biblioentry">
Hill, Kashmir, and Aaron Krolik. 2019. <span>“How <span>Photos</span> of <span>Your Kids Are Powering Surveillance Technology</span>.”</span> <em>The New York Times: Technology</em>, October 11, 2019. <a href="https://www.nytimes.com/interactive/2019/10/11/technology/flickr-facial-recognition.html">https://www.nytimes.com/interactive/2019/10/11/technology/flickr-facial-recognition.html</a>.
</div>
<div id="ref-hoelOperativeImagesInroads2018" class="csl-entry" role="doc-biblioentry">
Hoel, Aud Sissel. 2018. <span>“Operative <span>Images</span>. <span>Inroads</span> to a <span>New Paradigm</span> of <span>Media Theory</span>.”</span> In <em>Image – <span>Action</span> – <span>Space</span></em>, 11–28. <span>De Gruyter</span>. <a href="https://doi.org/10.1515/9783110464979-002">https://doi.org/10.1515/9783110464979-002</a>.
</div>
<div id="ref-HowReadPalms" class="csl-entry" role="doc-biblioentry">
<span>“How to <span>Read Palms</span>: 9 <span>Steps</span> (with <span>Pictures</span>).”</span> n.d. <span>wikiHow</span>. Accessed March 8, 2023. <a href="https://www.wikihow.com/Read-Palms">https://www.wikihow.com/Read-Palms</a>.
</div>
<div id="ref-ImageNetRouletteTrevor" class="csl-entry" role="doc-biblioentry">
<span>“<span>ImageNet Roulette</span> – <span>Trevor Paglen</span>.”</span> n.d. Accessed March 11, 2023. <a href="https://paglen.studio/2020/04/29/imagenet-roulette/">https://paglen.studio/2020/04/29/imagenet-roulette/</a>.
</div>
<div id="ref-karrasStyleBasedGeneratorArchitecture2019" class="csl-entry" role="doc-biblioentry">
Karras, Tero, Samuli Laine, and Timo Aila. 2019. <span>“A <span>Style-Based Generator Architecture</span> for <span>Generative Adversarial Networks</span>.”</span> March 29, 2019. <a href="https://doi.org/10.48550/arXiv.1812.04948">https://doi.org/10.48550/arXiv.1812.04948</a>.
</div>
<div id="ref-karrasAnalyzingImprovingImage2020" class="csl-entry" role="doc-biblioentry">
Karras, Tero, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. 2020. <span>“Analyzing and <span>Improving</span> the <span>Image Quality</span> of <span>StyleGAN</span>.”</span> March 23, 2020. <a href="http://arxiv.org/abs/1912.04958">http://arxiv.org/abs/1912.04958</a>.
</div>
<div id="ref-KATECRAWFORDTREVOR" class="csl-entry" role="doc-biblioentry">
<span>“<span>KATE CRAWFORD</span> | <span>TREVOR PAGLEN</span>: <span>TRAINING HUMANS</span> – <span>Fondazione Prada</span>.”</span> n.d. Accessed March 10, 2023. <a href="https://www.fondazioneprada.org/project/training-humans/?lang=en">https://www.fondazioneprada.org/project/training-humans/?lang=en</a>.
</div>
<div id="ref-krizhevskyImageNetClassificationDeep2017" class="csl-entry" role="doc-biblioentry">
Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. 2017. <span>“<span>ImageNet</span> Classification with Deep Convolutional Neural Networks.”</span> <em>Communications of the ACM</em> 60 (6): 84–90. <a href="https://doi.org/10.1145/3065386">https://doi.org/10.1145/3065386</a>.
</div>
<div id="ref-lavigneScrapism" class="csl-entry" role="doc-biblioentry">
Lavigne, Sam. n.d. <span>“Scrapism.”</span> Accessed March 15, 2023. <a href="https://scrapism.lav.io/">https://scrapism.lav.io/</a>.
</div>
<div id="ref-lavigneNewYorkApartment" class="csl-entry" role="doc-biblioentry">
Lavigne, Sam, and Tega Brain. n.d. <span>“New <span>York Apartment</span>.”</span> Accessed March 17, 2023. <a href="https://artport.whitney.org/commissions/new-york-apartment/index.html">https://artport.whitney.org/commissions/new-york-apartment/index.html</a>.
</div>
<div id="ref-matthiasschafer11kHands2018" class="csl-entry" role="doc-biblioentry">
Matthias Schäfer, dir. 2018. <em>11k Hands</em>. <a href="https://www.youtube.com/watch?v=snJsKFxPlJ8">https://www.youtube.com/watch?v=snJsKFxPlJ8</a>.
</div>
<div id="ref-mitchellArtificialIntelligenceGuide2019" class="csl-entry" role="doc-biblioentry">
Mitchell, Melanie. 2019. <em>Artificial Intelligence: A Guide for Thinking Humans</em>. <span>New York</span>: <span>Farrar, Straus and Giroux</span>.
</div>
<div id="ref-murphyWhyStanfordResearchers2017" class="csl-entry" role="doc-biblioentry">
Murphy, Heather. 2017. <span>“Why <span>Stanford Researchers Tried</span> to <span>Create</span> a <span>‘<span>Gaydar</span>’</span> <span>Machine</span>.”</span> <em>The New York Times: Science</em>, October 9, 2017. <a href="https://www.nytimes.com/2017/10/09/science/stanford-sexual-orientation-study.html">https://www.nytimes.com/2017/10/09/science/stanford-sexual-orientation-study.html</a>.
</div>
<div id="ref-NutzungsbedingungenInstagramHilfebereich" class="csl-entry" role="doc-biblioentry">
<span>“Nutzungsbedingungen | <span>Instagram-Hilfebereich</span>.”</span> n.d. Accessed March 17, 2023. <a href="https://help.instagram.com/581066165581870">https://help.instagram.com/581066165581870</a>.
</div>
<div id="ref-OpenFutureOpen" class="csl-entry" role="doc-biblioentry">
<span>“Open <span>Future</span> – <span>Open Future Foundation</span>.”</span> n.d. <span>Open Future</span>. Accessed March 13, 2023. <a href="https://openfuture.eu">https://openfuture.eu</a>.
</div>
<div id="ref-reaHowImageNetRoulette2019" class="csl-entry" role="doc-biblioentry">
Rea, Naomi. 2019. <span>“How <span>ImageNet Roulette</span>, a <span>Viral Art Project That Exposed Facial Recognition</span>’s <span>Biases</span>, <span>Is Changing Minds About AI</span>.”</span> <span>Artnet News</span>. September 23, 2019. <a href="https://news.artnet.com/art-world/imagenet-roulette-trevor-paglen-kate-crawford-1658305">https://news.artnet.com/art-world/imagenet-roulette-trevor-paglen-kate-crawford-1658305</a>.
</div>
<div id="ref-rosenblattPerceptronProbabilisticModel1958" class="csl-entry" role="doc-biblioentry">
Rosenblatt, F. 1958. <span>“The Perceptron: <span>A</span> Probabilistic Model for Information Storage and Organization in the Brain.”</span> <em>Psychological Review</em> 65 (6): 386–408. <a href="https://doi.org/10.1037/h0042519">https://doi.org/10.1037/h0042519</a>.
</div>
<div id="ref-samlavigneJustDiscoveredOpen2020" class="csl-entry" role="doc-biblioentry">
Sam Lavigne. 2020. <span>“Just Discovered an Open Source Hair Detector and Have Used It on Hundreds of Images of <span>Mark Zuckerberg</span> to Build What <span>I</span> Believe Is the Most Comprehensive Archive of <span>Zuckerberg</span> Haircuts in Existence. <span>Thank</span> You <span>AI</span> Researchers! <span class="nocase">https://t.co/QJqXLAbqyw</span>.”</span> Tweet. <span>Twitter</span>. February 16, 2020. <a href="https://twitter.com/sam_lavigne/status/1229097648818446336">https://twitter.com/sam_lavigne/status/1229097648818446336</a>.
</div>
<div id="ref-schaferThisPersonDoes2021" class="csl-entry" role="doc-biblioentry">
Schäfer, Matthias. 2021. <span>“This <span>Person Does Exist</span>.”</span> <em>Temes de Disseny</em>, no. 37 (July): 214–25. <a href="https://doi.org/10.46467/TdD37.2021.214-225">https://doi.org/10.46467/TdD37.2021.214-225</a>.
</div>
<div id="ref-schaferMissingPictures" class="csl-entry" role="doc-biblioentry">
———. n.d. <span>“Missing.pictures.”</span> Accessed March 17, 2023. <a href="https://missing.pictures/">https://missing.pictures/</a>.
</div>
<div id="ref-tappertWhoFatherDeep2019" class="csl-entry" role="doc-biblioentry">
Tappert, Charles C. 2019. <span>“Who <span>Is</span> the <span>Father</span> of <span>Deep Learning</span>?”</span> In <em>2019 <span>International Conference</span> on <span>Computational Science</span> and <span>Computational Intelligence</span> (<span>CSCI</span>)</em>, 343–48. <span>Las Vegas, NV, USA</span>: <span>IEEE</span>. <a href="https://doi.org/10.1109/CSCI49370.2019.00067">https://doi.org/10.1109/CSCI49370.2019.00067</a>.
</div>
<div id="ref-TechnicallyResponsibleKnowledge" class="csl-entry" role="doc-biblioentry">
<span>“Technically <span>Responsible Knowledge</span>.”</span> n.d. Accessed March 10, 2023. <a href="http://trk.network/essay">http://trk.network/essay</a>.
</div>
<div id="ref-InternetYamiIchi" class="csl-entry" role="doc-biblioentry">
<span>“The Internet Yami-Ichi.”</span> n.d. <span>In Kepler’s Gardens</span>. Accessed March 20, 2023. <a href="https://ars.electronica.art/keplersgardens/de/the-internet-yami-ichi/">https://ars.electronica.art/keplersgardens/de/the-internet-yami-ichi/</a>.
</div>
<div id="ref-ThisPersonDoes2019" class="csl-entry" role="doc-biblioentry">
<span>“This <span>Person Does Not Exist</span>.”</span> 2019. May 31, 2019. <a href="https://web.archive.org/web/20190531222303/https://thispersondoesnotexist.com/">https://web.archive.org/web/20190531222303/https://thispersondoesnotexist.com/</a>.
</div>
<div id="ref-XTRAMarkHansen" class="csl-entry" role="doc-biblioentry">
<span>“X-<span>TRA</span> → <span>Mark Hansen</span> and <span>Ben Rubin</span>: <span>Listening Post</span>.”</span> n.d. Accessed March 20, 2023. <a href="https://www.x-traonline.org/article/mark-hansen-and-ben-rubin-listening-post">https://www.x-traonline.org/article/mark-hansen-and-ben-rubin-listening-post</a>.
</div>
</div>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p><span class="citation" data-cites="rosenblattPerceptronProbabilisticModel1958">(<a href="#ref-rosenblattPerceptronProbabilisticModel1958" role="doc-biblioref">Rosenblatt 1958</a>)</span><a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>The Mark I was a electromechanical machine that used motor driven potentiometers to adjust the variable weights.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p>Frank Rosenblatt died in a boating accident in 1971. A couple years prior Marvin Minsky heavily criticized the mathematics behind perceptrons and advocated for a symbolic approach. These turn of events might have lead to a lack of funding in the ‘connectionist’ AI research field and ultimately lead to a general disinterest when the symbolic approach could not keep their exaggerated promises.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4" role="doc-endnote"><p><span class="citation" data-cites="mitchellArtificialIntelligenceGuide2019">(<a href="#ref-mitchellArtificialIntelligenceGuide2019" role="doc-biblioref">Mitchell 2019</a>)</span>, p. 114<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5" role="doc-endnote"><p><span class="citation" data-cites="fukushimaCognitronSelforganizingMultilayered1975">(<a href="#ref-fukushimaCognitronSelforganizingMultilayered1975" role="doc-biblioref">Fukushima 1975</a>)</span><a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6" role="doc-endnote"><p><span class="citation" data-cites="fukushimaNeocognitronHierarchicalNeural1988">(<a href="#ref-fukushimaNeocognitronHierarchicalNeural1988" role="doc-biblioref">Fukushima 1988</a>)</span><a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7" role="doc-endnote"><p><span class="citation" data-cites="tappertWhoFatherDeep2019">(<a href="#ref-tappertWhoFatherDeep2019" role="doc-biblioref">Tappert 2019</a>)</span><a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8" role="doc-endnote"><p><span class="citation" data-cites="krizhevskyImageNetClassificationDeep2017">(<a href="#ref-krizhevskyImageNetClassificationDeep2017" role="doc-biblioref">Krizhevsky, Sutskever, and Hinton 2017</a>)</span><a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9" role="doc-endnote"><p>“One epoch takes 35 GPU minutes but more than 35 CPU hours.” <span class="citation" data-cites="ciresanFlexibleHighPerformance">(<a href="#ref-ciresanFlexibleHighPerformance" role="doc-biblioref">Ciresan et al., n.d.</a>)</span><a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10" role="doc-endnote"><p><span class="citation" data-cites="afifi11KHandsGender2018">(<a href="#ref-afifi11KHandsGender2018" role="doc-biblioref">Afifi 2018</a>)</span>. Data and source code available here: <span class="citation" data-cites="afifi11kHands">(<a href="#ref-afifi11kHands" role="doc-biblioref">Afifi n.d.</a>)</span><a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11" role="doc-endnote"><p>In 2018 Parker et al jokingly tested the Galls theory using 21st century scientific methods and MRI data. <span class="citation" data-cites="dempsey-jonesNeuroscientistsPutDubious2018">(<a href="#ref-dempsey-jonesNeuroscientistsPutDubious2018" role="doc-biblioref">Dempsey-Jones 2018</a>)</span><a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn12" role="doc-endnote"><p>I didn’t do any testing of the system as I don’t know how to run MatLab code, but I can imagine that the slightly better results on the dorsal hand are the result of nail polish only applied on female hands.<a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn13" role="doc-endnote"><p><span class="citation" data-cites="hoelOperativeImagesInroads2018">(<a href="#ref-hoelOperativeImagesInroads2018" role="doc-biblioref">Hoel 2018</a>)</span><a href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn14" role="doc-endnote"><p>See: <span class="citation" data-cites="matthiasschafer11kHands2018">(<a href="#ref-matthiasschafer11kHands2018" role="doc-biblioref">Matthias Schäfer 2018</a>)</span><a href="#fnref14" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn15" role="doc-endnote"><p><span class="citation" data-cites="HowReadPalms">(<a href="#ref-HowReadPalms" role="doc-biblioref"><span>“How to <span>Read Palms</span>: 9 <span>Steps</span> (with <span>Pictures</span>)”</span> n.d.</a>)</span><a href="#fnref15" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn16" role="doc-endnote"><p>One particular famous example of this is the work by Michael Kosinski and Yiluna Wang. Their flawed study tried to predict if a person is gay by scraping dating sites and training a classifier on these images. See:<span class="citation" data-cites="murphyWhyStanfordResearchers2017">(<a href="#ref-murphyWhyStanfordResearchers2017" role="doc-biblioref">Murphy 2017</a>)</span><a href="#fnref16" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn17" role="doc-endnote"><p>ImageNet started with 3,2 million images and had the goal to collect 50 million by the end of 2011. <span class="citation" data-cites="dengImageNetLargeScaleHierarchical2009">(<a href="#ref-dengImageNetLargeScaleHierarchical2009" role="doc-biblioref">Deng et al. 2009</a>)</span><a href="#fnref17" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn18" role="doc-endnote"><p><span class="citation" data-cites="gershgornDataThatTransformed2017">(<a href="#ref-gershgornDataThatTransformed2017" role="doc-biblioref">Gershgorn 2017</a>)</span><a href="#fnref18" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn19" role="doc-endnote"><p><span class="citation" data-cites="haraDataDrivenAnalysisWorkers2017">(<a href="#ref-haraDataDrivenAnalysisWorkers2017" role="doc-biblioref">Hara et al. 2017</a>)</span><a href="#fnref19" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn20" role="doc-endnote"><p>See: <span class="citation" data-cites="TechnicallyResponsibleKnowledge">(<a href="#ref-TechnicallyResponsibleKnowledge" role="doc-biblioref"><span>“Technically <span>Responsible Knowledge</span>”</span> n.d.</a>)</span><a href="#fnref20" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn21" role="doc-endnote"><p>See: <span class="citation" data-cites="KATECRAWFORDTREVOR">(<a href="#ref-KATECRAWFORDTREVOR" role="doc-biblioref"><span>“<span>KATE CRAWFORD</span> | <span>TREVOR PAGLEN</span>: <span>TRAINING HUMANS</span> – <span>Fondazione Prada</span>”</span> n.d.</a>)</span><a href="#fnref21" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn22" role="doc-endnote"><p><span class="citation" data-cites="ExcavatingAI">(<a href="#ref-ExcavatingAI" role="doc-biblioref"><span>“Excavating <span>AI</span>”</span> n.d.</a>)</span><a href="#fnref22" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn23" role="doc-endnote"><p>See: <span class="citation" data-cites="ImageNetRouletteTrevor">(<a href="#ref-ImageNetRouletteTrevor" role="doc-biblioref"><span>“<span>ImageNet Roulette</span> – <span>Trevor Paglen</span>”</span> n.d.</a>)</span><a href="#fnref23" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn24" role="doc-endnote"><p><span class="citation" data-cites="reaHowImageNetRoulette2019">(<a href="#ref-reaHowImageNetRoulette2019" role="doc-biblioref">Rea 2019</a>)</span><a href="#fnref24" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn25" role="doc-endnote"><p>See: <span class="citation" data-cites="harveyExposingAi">(<a href="#ref-harveyExposingAi" role="doc-biblioref">Harvey n.d.a</a>)</span><a href="#fnref25" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn26" role="doc-endnote"><p><span class="citation" data-cites="2ndUnconstrainedFace">(<a href="#ref-2ndUnconstrainedFace" role="doc-biblioref"><span>“2nd <span>Unconstrained Face Detection</span> and <span>Open Set Recognition Challenge</span>, <span>ECCV-2018</span>”</span> n.d.</a>)</span><a href="#fnref26" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn27" role="doc-endnote"><p>See a longer analysis on the exploitation of CC media by A. Harvey: <span class="citation" data-cites="CreativeCommonsBiometrics">(<a href="#ref-CreativeCommonsBiometrics" role="doc-biblioref"><span>“Creative <span>Commons Biometrics</span>”</span> n.d.</a>)</span><a href="#fnref27" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn28" role="doc-endnote"><p><span class="citation" data-cites="CreativeCommonsBiometrics">(<a href="#ref-CreativeCommonsBiometrics" role="doc-biblioref"><span>“Creative <span>Commons Biometrics</span>”</span> n.d.</a>)</span><a href="#fnref28" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn29" role="doc-endnote"><p>YFCC100M only contains links and metadata to images and videos under CC license onf Flickr<a href="#fnref29" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn30" role="doc-endnote"><p><span class="citation" data-cites="harveyExposingAiMegaFace">(<a href="#ref-harveyExposingAiMegaFace" role="doc-biblioref">Harvey n.d.b</a>)</span><a href="#fnref30" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn31" role="doc-endnote"><p><span class="citation" data-cites="hillHowPhotosYour2019">(<a href="#ref-hillHowPhotosYour2019" role="doc-biblioref">Hill and Krolik 2019</a>)</span><a href="#fnref31" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn32" role="doc-endnote"><p>The Open Future Foundation is a think tank that actively tries to influence european digital policy debates. See <span class="citation" data-cites="OpenFutureOpen">(<a href="#ref-OpenFutureOpen" role="doc-biblioref"><span>“Open <span>Future</span> – <span>Open Future Foundation</span>”</span> n.d.</a>)</span><a href="#fnref32" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn33" role="doc-endnote"><p><span class="citation" data-cites="karrasStyleBasedGeneratorArchitecture2019">(<a href="#ref-karrasStyleBasedGeneratorArchitecture2019" role="doc-biblioref">Karras, Laine, and Aila 2019</a>)</span><a href="#fnref33" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn34" role="doc-endnote"><p>At the time of this writing the url redirects to stability.ai, an archived version can be found. See <span class="citation" data-cites="ThisPersonDoes2019">(<a href="#ref-ThisPersonDoes2019" role="doc-biblioref"><span>“This <span>Person Does Not Exist</span>”</span> 2019</a>)</span><a href="#fnref34" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn35" role="doc-endnote"><p>See <span class="citation" data-cites="schaferThisPersonDoes2021">(<a href="#ref-schaferThisPersonDoes2021" role="doc-biblioref">Schäfer 2021</a>)</span><a href="#fnref35" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn36" role="doc-endnote"><p><span class="citation" data-cites="FFHQDatasetSearch">(<a href="#ref-FFHQDatasetSearch" role="doc-biblioref"><span>“<span>FFHQ</span> Dataset Search Form”</span> n.d.</a>)</span><a href="#fnref36" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn37" role="doc-endnote"><p>See ‘FFHQ Dataset’ at <span class="citation" data-cites="CreativeCommonsBiometrics">(<a href="#ref-CreativeCommonsBiometrics" role="doc-biblioref"><span>“Creative <span>Commons Biometrics</span>”</span> n.d.</a>)</span><a href="#fnref37" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn38" role="doc-endnote"><p><span class="citation" data-cites="karrasStyleBasedGeneratorArchitecture2019">(<a href="#ref-karrasStyleBasedGeneratorArchitecture2019" role="doc-biblioref">Karras, Laine, and Aila 2019</a>)</span><a href="#fnref38" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn39" role="doc-endnote"><p><span class="citation" data-cites="schaferThisPersonDoes2021">(<a href="#ref-schaferThisPersonDoes2021" role="doc-biblioref">Schäfer 2021</a>)</span><a href="#fnref39" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn40" role="doc-endnote"><p><span class="citation" data-cites="lavigneScrapism">(<a href="#ref-lavigneScrapism" role="doc-biblioref">Lavigne n.d.</a>)</span><a href="#fnref40" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn41" role="doc-endnote"><p>See <span class="citation" data-cites="XTRAMarkHansen">(<a href="#ref-XTRAMarkHansen" role="doc-biblioref"><span>“X-<span>TRA</span> → <span>Mark Hansen</span> and <span>Ben Rubin</span>: <span>Listening Post</span>”</span> n.d.</a>)</span> and <span class="citation" data-cites="ArsElectronicaArchiv">(<a href="#ref-ArsElectronicaArchiv" role="doc-biblioref"><span>“Ars <span>Electronica Archiv</span>”</span> n.d.</a>)</span><a href="#fnref41" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn42" role="doc-endnote"><p><span class="citation" data-cites="lavigneNewYorkApartment">(<a href="#ref-lavigneNewYorkApartment" role="doc-biblioref">Lavigne and Brain n.d.</a>)</span><a href="#fnref42" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn43" role="doc-endnote"><p><span class="citation" data-cites="samlavigneJustDiscoveredOpen2020">(<a href="#ref-samlavigneJustDiscoveredOpen2020" role="doc-biblioref">Sam Lavigne 2020</a>)</span><a href="#fnref43" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn44" role="doc-endnote"><p>See <span class="citation" data-cites="schaferMissingPictures">(<a href="#ref-schaferMissingPictures" role="doc-biblioref">Schäfer n.d.</a>)</span><a href="#fnref44" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn45" role="doc-endnote"><p>See <span class="citation" data-cites="NutzungsbedingungenInstagramHilfebereich">(<a href="#ref-NutzungsbedingungenInstagramHilfebereich" role="doc-biblioref"><span>“Nutzungsbedingungen | <span>Instagram-Hilfebereich</span>”</span> n.d.</a>)</span><a href="#fnref45" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn46" role="doc-endnote"><p>Complete list of artists can be found here https://doggg.art/artistlist.html<a href="#fnref46" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn47" role="doc-endnote"><p>See <span class="citation" data-cites="chakelianJourneyManyFaces2021">(<a href="#ref-chakelianJourneyManyFaces2021" role="doc-biblioref">Chakelian 2021</a>)</span><a href="#fnref47" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn48" role="doc-endnote"><p>See <span class="citation" data-cites="InternetYamiIchi">(<a href="#ref-InternetYamiIchi" role="doc-biblioref"><span>“The Internet Yami-Ichi”</span> n.d.</a>)</span><a href="#fnref48" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn49" role="doc-endnote"><p>See <span class="citation" data-cites="karrasAnalyzingImprovingImage2020">(<a href="#ref-karrasAnalyzingImprovingImage2020" role="doc-biblioref">Karras et al. 2020</a>)</span><a href="#fnref49" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
</body>
</html>
