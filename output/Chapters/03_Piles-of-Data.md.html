<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Conversations with Computers</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      word-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
    }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  
</head>
<body>
<header id="title-block-header">
<h1 class="title">Conversations with Computers</h1>
</header>
<h1 id="piles-of-data">Piles of Data</h1>
<h2 id="self-learning-networks">Self-Learning Networks</h2>
<p>The idea behind Artificial Neural Networks has a long-standing history. Using our understanding of the brain as a blueprint for mathematical operations dates back to the 1950s when the psychologist Frank Rosenblatt developed the <em>perceptron</em>.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> Inspired by nerve cells and their connections (synapses), the perceptron takes multiple input values, sums them up, and outputs a 0 or 1 depending if a predefined threshold is reached. This system can be ‘trained’ by using positive and negative reinforcement to define the weights for each connection. With an apparatus, the Mark-I Perceptron,<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> that uses photoreceptors to turn light into ‘bits’ (today we would say pixels), the perceptron could ‘sense’ shapes in the form of a binary matrix and distinguish between circles, squares, and triangles. He proposed that a network of perceptrons could possibly even recognize faces and objects. Even though he developed the perceptron in 1964, Frank Rosenblatt never saw his invention take off.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> </p>
<p>Another engineer, Kunihiko Fukushima, kept refining his methods in the 70s by adding multiple layers, effectively creating the first ‘deep neural network’ where deep just means the depth of ‘hidden’ or in-between layers connecting the input signal to the output classifier.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> He called this self-organizing system Cognitron,<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> which successfully recognized numbers and letters on a 12x12 grid. Its successor, the Neocognitron,<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> took further inspiration from the visual cortex and discoveries by Hubel &amp; Wiesel made in the 1950s that some biological neurons selectively respond to local features like lines, edges, or color and others to figures like circles, squares or even human faces. This is also the core idea behind convolutional neural networks (ConvNet or CNN), which separate images into a smaller grid and apply a certain filter to them, e.g., checking for edges. The French computer scientist Yann LeCun came up with ConvNets in the 1980s, which are <em>the</em> driving force for AI systems today. Additionally, Geoff Hinton, a cognitive psychologist and computer scientist, popularized the backpropagation algorithm in 1986, which finally made it possible for filters to tune themselves instead of relying on predefined rules.</p>
<p>Most conceptual ideas behind current deep, weighted networks were already present in Frank Rosenblatt’s papers,<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> but rule-based systems often outperformed weighted networks. So what changed when Alex Krizhevsky, a student of Hinton, made a phenomenal leap in 2012 on the ImageNet classification competition?</p>
<p>Krizhevsky outlines the main innovation in the paper: “To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation.”<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> Until AlexNet was released, it was incredibly time-consuming to run the conditioning process on the CPU, which could only do one operation of matrix multiplication at a time. As the name Graphics Processing Unit suggests, the GPU was originally designed to calculate 3D scenes and render them on display, which involves a lot of matrix and vector operations. To accelerate them, GPUs are capable of calculating large blocks of data in parallel. For AlexNet, conditioning on 1.2 million pictures took only 6-9 days on two consumer graphics cards compared to probably weeks or months without them. However, it was not the first system that utilized the GPU; it is similar to a CNN by Dan C. Cireȿan et al. released a year prior, with a reported speedup of 60 times for large weighted networks.<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a></p>
<p>The other roadblock for deep, weighted networks is ‘overfitting’: in the case of the ImageNet competition, that would mean that the model adapts to the image so closely that it would simply reproduce the categories of the image without being able to identify new pictures which were not inside the training set. The most common way to reduce overfitting is to have a sufficiently large dataset with a high amount of variance. For AlexNet, the 1000 classes and 1.2 million images were still not enough, and they used data augmentation, which transforms, flips, or changes the color of an image to increase the training set by a factor of 2048. A larger dataset—in theory—increases the robustness of the weighted network.</p>
<p>In conclusion, the conceptual framework of how weighted networks (artificial neural nets, perceptrons, or cognitrons) work was mostly established in the past century. Their performance today comes from increased computing power and more extensive training sets. In fact, datasets have become larger and larger, and we haven’t seen the limits of what weighted networks are capable of. They seem to be mainly limited by data and computation.</p>
<h2 id="making-data">Making Data</h2>
<p>My first encounter with a large dataset in the field of computer vision was in late 2017 when I found <em>11k Hands</em>.<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a> As the name suggests, it is a dataset of 11076 human hands compiled by the researcher Mahmoud Afifi for gender recognition and biometric identification. The images show the hands of 190 individuals, in both their palm and dorsal orientation, placed on a white background with uniform lighting. Each image is accompanied by the following metadata: age, binary gender, skin color on a scale of “very fair,” “fair,” “medium,” and “dark,” left or right hand, palm or dorsal, if it has nail polish and if there are “irregularities.” A statistical analysis of the dataset shows that it contains more female than male hands, mainly people in their 20s and a majority of “medium” and “fair” skin tones. The gender bias is addressed in the paper and mitigated by filtering the training set to equal male and female hands. They report that the CNN conditioned on this dataset had, on average, 87% accuracy in recognizing the correct gender on an image of the palm and 90% accuracy for the dorsal side.</p>
<p>But it is not the ‘state-of-the-art’ results that drew me to the paper; it was the gender and skin tone classification itself that appalled me. It reminds me of phrenology in the 19th and 20th centuries, a popular pseudoscience claiming that a skull’s shape could determine a person’s character and mental abilities. Phrenologists like Franz Joseph Gall went to great lengths, measuring and categorizing human skulls and associating certain regions to human traits. While it didn’t take scientists to debunk the idea that bumps on a head could indicate a person’s characteristics, it came back in the early 20th century when it was applied to racist and sexist stereotypes and used to justify Nazi eugenics. The underlying assumption that the shape of the head has anything to do with the person’s mental state was simply wrong.<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a></p>
<p>The paper by Afifi et al. does not make such horrendous claims about the personality of their subjects, but considering that gender is a social construct and not necessarily a binary choice trying to use a computer to identify if a person is male or female by analyzing their hand seems arbitrary at best and reinforcing gender stereotypes at least.<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a> </p>
<p>Another aspect of the dataset stood out to me when I started to look through the images. These are not pictures in the traditional sense. Their aesthetic value does not matter because they are simply tools to accomplish a task. They are not made ‘to look at’; instead, these images of hands are produced for a computer to analyze. Harun Faroki called these types of images ‘operative’ in his three-part series <em>Machine/Eye</em> where he examined how military technologies like guided weapons produce images that serve only the utility of the machine. As Aud Sissel Hoel puts it: “operative images are devoid of social intent, that they are not meant for edification, and nor for contemplation.”<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a></p>
<p>Upon contemplating the hand dataset, I was struck by its remarkable features. When I looked through the images quickly, I could not only see the motion of the test subject as the photos were selected frames from a video, but I also started to imagine the person behind the hand and apply my subjective stereotypes and biases which far expanded the labels of the dataset. I could imagine the scene around the camera with a couple of researchers who assembled a make-shift photo studio in the office of their lab. They greet the subject with a handshake, maybe a fellow student, and explain the procedure quickly to get them to sign the paper that their anonymized personal data will be published for scientific research. Then the person puts both hands under the camera, spreads their fingers, and leaves. </p>
<p>This dichotomy between the label and my narrative inspired me to create a video piece featuring the unaltered dataset. I used the default Mac computer speech synthesis to read the labels corresponding to each hand and sped it up to fit into a 26-minute-long video. As viewers witness the participants holding their hands into the camera and spreading their fingers, they are met with a monotone, beat-like soundtrack of repeating words like “fair” and “medium” and occasionally “nail polish.”<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a></p>
<figure>
<img src="11khands.png" alt="Example of a frame from 11k hands" /><figcaption aria-hidden="true">Example of a frame from 11k hands</figcaption>
</figure>
<p>At the 36th Chaos Communication Congress, I organized a workshop called “Palm Reading AI,” inviting visitors to read people’s palms from the 11k hands dataset. At first, I only introduced them to palmistry using wikiHow as a reference.<a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a> Then I handed out some prints where a random hand from the dataset was depicted, and participants had to answer a couple of questions. Some questions were short guesses like age, gender, and country of origin; for others, they had to come up with fictional stories with only the hand lines as a reference: what is the persons future? How was their childhood? How is their love life?</p>
<p>After they filled in the form, some people shared their stories, and then I revealed where these hands came from and how computer scientists are using them to create models that try to predict their gender. Afterward, we discussed the practice of creating large datasets and their ethical considerations. I had a long talk with one participant that did not want to guess the age or gender of that person, and I told them that this was precisely the point of the workshop: to reflect on our own biases and stereotypes and how they translate into science.</p>
<p>After long contemplation on 11k hands and finding datasets that are much more problematic than this one, I don’t think the type of work from Afifi et al. is ‘unethical’ or needs to be redacted. They got consent from their subjects and shared the dataset with the scientific community for “reasonable academic fair use.” The work on biometric identification and comparing CNNs to previous methods are interesting and novel; however, as I stated above, I think the premise behind the gender recognition task is flawed. Unfortunately, this is very common in computer vision, where people are (mis-)labeled, reflecting and amplifying societal stereotypes.<a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a></p>
<p>My research on the hands dataset, in conjunction with esoteric practices and fortune-telling, informed a later work of mine, “The Chiromancer,” that I built with Giacomo Piazzi.</p>
<h2 id="in-the-wild">In the wild</h2>
<p>The process of collecting and creating data has drastically changed since the wide adoption of the internet. Where the 11k hands dataset has invited participants to their institute to take a picture specifically for the dataset, other researchers started to search and download huge collections from the internet without consent.</p>
<p>Take, for example, the ImageNet dataset, initiated by Stanford University’s AI professor Fei Fei Li, which was created to tackle object recognition tasks and eventually consisted of 14 million images.<a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a> The team queried multiple search engines with common nouns and multiple translations to get around 500-1000 images per category. The categories derive from an older project called WordNet, created in 1985 at Princeton University, that tried to achieve a hierarchical ontology of words. For example, the noun <em>human</em> is a synonym for <em>homo, man, and human being</em> which are subcategories of the <em>hominid</em> class, which are <em>primates</em>, which are <em>mammals</em>, which are <em>animals</em>, which an <em>organism</em> and finally part of the <em>entity</em> class. After downloading millions of images in thousands of categories, they were first automatically deduplicated, but the task of checking whether the image actually depicts the search term could not be done by an algorithm. Originally Fei Fei Li wanted to hire graduate students to sort the data, which was too costly and time-consuming.<a href="#fn18" class="footnote-ref" id="fnref18" role="doc-noteref"><sup>18</sup></a></p>
<p>One of her undergraduates suggested using a new service called Amazon Mechanical Turk, where people worldwide complete small tasks for little compensation. Even with the help of Mechanical Turk, it took 2,5 years to sort and validate the first dataset with 3.2 million images in over 5000 categories. At some point, ImageNet was the largest academic client for Amazon’s Turk service, and after the popularization of weighted networks, they became a cornerstone of data annotation. Similar platforms were established, and Mechanical Turk grew to half a million workers, rigorously exploited and alienated as ‘click workers.’ One paper estimates their median hourly wage to 2 USD, and only the top 4 per cent earn more than 7.25 USD per hour.<a href="#fn19" class="footnote-ref" id="fnref19" role="doc-noteref"><sup>19</sup></a> If the requester rejects the task, the worker does not get paid at all and has no way of appealing that decision. To advocate for better working conditions, the designer and researcher Caroline Sinders built an open-source tool for data annotation and a wage calculator to estimate the cost of labeling better.<a href="#fn20" class="footnote-ref" id="fnref20" role="doc-noteref"><sup>20</sup></a></p>
<p>With so many different people working on labeling images, the dataset eventually reflects societal biases. In 2019 the artist Trevor Paglen and researcher Kate Crawford collaborated on an exhibition titled <em>Training Humans</em>, dedicated to human image datasets. One of the main exhibits was a vast collection of human images from ImageNet.<a href="#fn21" class="footnote-ref" id="fnref21" role="doc-noteref"><sup>21</sup></a>   </p>
<p>As you go further down the category tree, you might find images of people labeled as “Bad Person, Call Girl, Drug Addict, Closet Queen, Convict,” and so on.<a href="#fn22" class="footnote-ref" id="fnref22" role="doc-noteref"><sup>22</sup></a> The artists used these absurd, racist, and misogynistic labels to train <em>ImageNet Roulette</em>,<a href="#fn23" class="footnote-ref" id="fnref23" role="doc-noteref"><sup>23</sup></a> a recognition algorithm accessible online and in an interactive installation. People online quickly picked up on the tool and shared images of themselves with sometimes amusing and often deeply problematic image captions.<a href="#fn24" class="footnote-ref" id="fnref24" role="doc-noteref"><sup>24</sup></a> While some of the people in the research community defended ImageNet, that these offensive labels are not part of the competition and make up only a tiny fraction of the total dataset, as a result of the media attention that followed, more than 600,000 images were removed. It is now only accessible after proving to be part of a scientific institute.</p>
<p>Another pair of artists and researchers, Adam Harvey and Jules LaPlace, have been exposing image sets which often get revoked and removed after publishing their investigation.<a href="#fn25" class="footnote-ref" id="fnref25" role="doc-noteref"><sup>25</sup></a> Harvey and LaPlace focus on datasets with faces captured “in the wild.” One particular example was the <em>UnConstrained College Students (UCCS)</em> dataset captured at the University of Colorado Colorado Springs. According to the authors of the associated papers to the dataset, they identified that current image sets created for face recognition research did not address the presence of unknown subjects. The authors of the papers associated with this dataset wanted to create a more realistic benchmark for face recognition research by introducing unknown subjects over time. To achieve this, they captured students on campus during breaks using a 800mm telephoto lens from a distance of 100-150 meters through an office window. The authors frame it as a benefit that the students are unaware of their capturing for the dataset as they casually focus on random activities, and their faces are sometimes blurred and occluded. To make things worse, this research was mainly funded by US defense and intelligence agencies. The public backlash was immense, and as a result, the dataset is no longer publicly available. The researchers did not see any ethical problems and argued that their subjects are anonymous as their names or other identifiers are not published.<a href="#fn26" class="footnote-ref" id="fnref26" role="doc-noteref"><sup>26</sup></a></p>
<p>However, when researchers need unconstrained and non-consensual data, they do not often capture them directly using creepy surveillance tactics. Starting with “Labeled Faces in the Wild” in 2007, collecting and labeling image data from internet sources has become normalized and remains largely unregulated. Often these image sets “in the wild” operate in a gray zone where they either depict public figures, arguing that privacy regulations do not apply, or simply link to the file and only store them temporarily for analysis. A third option uses media licensed under Creative Commons (CC), which is mostly considered free and legal data with no restrictions in the AI research community.<a href="#fn27" class="footnote-ref" id="fnref27" role="doc-noteref"><sup>27</sup></a> In a detailed report, Adam Harvey lists many datasets that exploit and often misuse CC licenses to create face and object recognition datasets.<a href="#fn28" class="footnote-ref" id="fnref28" role="doc-noteref"><sup>28</sup></a> He identifies Flickr as a significant source for collecting images, a popular image-sharing website where users can choose from various Creative Commons, copyright, and public domain licenses. Flickr actively promoted the use of CC licenses and offered unlimited free hosting if a CC license is used. Their strategy worked; by 2022, they amassed 467 million CC-licensed images. In 2014 a joint research group, including Yahoo Labs, the company that bought Flickr, shared one of the most extensive public media datasets<a href="#fn29" class="footnote-ref" id="fnref29" role="doc-noteref"><sup>29</sup></a> with the name <em>Yahoo! Flickr Creative Commons 100 Million (YFCC100M)</em>.</p>
<p>Other more specific datasets were created from this corpus. One is the <em>MegaFace</em> face recognition dataset with 4,7 million faces from 672,057 identities. While all the images fall under a CC license, most prohibit their commercial use and require appropriate attribution, which was not given in the dataset. As Harvey and LaPlace verified, the <em>MegaFace</em> dataset was used globally by commercial, military, and academic organizations.<a href="#fn30" class="footnote-ref" id="fnref30" role="doc-noteref"><sup>30</sup></a> As an investigative article from the New York Times explains, most people are unaware that their images are powering face recognition research worldwide, including companies with ties to China’s surveillance of the Uyghur population.<a href="#fn31" class="footnote-ref" id="fnref31" role="doc-noteref"><sup>31</sup></a> </p>
<p>To conclude, the three common issues with CC-licensed media are that they are not or wrongly attributed, the use of non-consensual biometric data is prohibited in some places (e.g., Illinois), and the use in commercial applications is often prohibited. Many of these issues identified by Harvey and LaPlace remain today and operate in a legal grey zone. While the notion behind uploading images under an open license for others to freely share, distribute, and remix media is a noble goal, the CC license is legally weak and practically useless against opting out of statistical analysis in AI research. But we might be slowly moving in a direction where lawmakers catch up and create precedences that disallow the use of biometric data in certain applications, like the upcoming AI Act.<a href="#fn32" class="footnote-ref" id="fnref32" role="doc-noteref"><sup>32</sup></a> </p>
<h2 id="this-person-does-exist">This Person Does Exist</h2>
<p>I examined a dataset using Creative Commons images more closely: <em>Flickr Faces HQ (FFHQ)</em>. In 2018 the research lab of Nvidia, one of the leading companies for visual computing, published a paper introducing a machine learning architecture called StyleGAN.<a href="#fn33" class="footnote-ref" id="fnref33" role="doc-noteref"><sup>33</sup></a> They improved generative adversarial networks (GAN) so that it was possible to create controllable synthetic high-resolution images. In simple terms, the system is able to abstract large amounts of images with a model that, in turn, outputs similar-looking pictures. In this case, the model can generate realistic-looking photographs of human faces. </p>
<p><em>FFHQ</em> is relatively small compared to other datasets, with only 70,000 images. As existing datasets were too low in resolution, a new corpus was created by scraping Creative Commons, Public Domain, or U.S. Government Works licensed images through Flickr’s API. The dataset is published under a CC-BY-NC-SA license, and the instructions for use and download are very clear, making it manageable for me in terms of size and effort to discover the underlying characteristics. The dataset consists only of photographic images, as Amazon Mechanical Turk workers checked it to remove occasional pictures of statues, paintings, or photos of photos. The raw images were automatically aligned and cropped around the face, using the open-source library dlib, to create a uniform square ratio of 1024px. This library also identifies 68 points outlining the chin, eyebrows, eyes, nose, and mouth, which are included in the metadata. So the final dataset used for training the GAN consists of a large number of faces where the eye and mouth positions are always in the same spot.</p>
<figure>
<img src="Screen_Shot_2023-03-15_at_20.50.49.png" alt="Screenshot of This Person Does Exist" /><figcaption aria-hidden="true">Screenshot of This Person Does Exist</figcaption>
</figure>
<p>One year after Nvidia released its StyleGAN paper, the software developer Phillip Wang published the website <em>thispersondoesnotexist.com</em>, which shows the capabilities of the generative model to create realistic-looking photographs of people.<a href="#fn34" class="footnote-ref" id="fnref34" role="doc-noteref"><sup>34</sup></a> The site quickly took off and alarmed people about the potential impact of AI systems in generating cheap synthetic media.  </p>
<p>As a counter-narrative to the AI image creator, I wanted to showcase the people who were used to condition this system. In 2020 I moved the cropped and aligned face images to my server and built my website with the name <em>this-person-does-exist.com</em>, which displays the faces from the training set alongside their metadata.<a href="#fn35" class="footnote-ref" id="fnref35" role="doc-noteref"><sup>35</sup></a> Looking at the individual faces facilitates an interpersonal connection with the unknown person and evokes a feeling for the images that were used to train the generative model. At the same time, it shows the creepy and strange practice of AI researchers using personal images as raw data.</p>
<p>As Flickr is used mostly by hobbyists and professional photographers, one can find portraits of children and families, speakers at conferences, or people on holiday. We can assume that, like other scraped datasets, the photographers of these images are unaware that their images are used for AI analysis. In contrast to other scraped datasets, the Nvidia researchers provide a tool to see if an image is part of the collection and allow the removal of the photograph.<a href="#fn36" class="footnote-ref" id="fnref36" role="doc-noteref"><sup>36</sup></a> According to Adam Harvey, the company does not disclose if any images were requested for removal and have not updated or removed any photos since its release in 2016.<a href="#fn37" class="footnote-ref" id="fnref37" role="doc-noteref"><sup>37</sup></a></p>
<figure>
<img src="Fig2.png" alt="70k faces from FFHQ compiled into one image" /><figcaption aria-hidden="true">70k faces from FFHQ compiled into one image</figcaption>
</figure>
<figure>
<img src="Fig3.png" alt="The Flickr Face - averaged FFHQ dataset" /><figcaption aria-hidden="true">The Flickr Face - averaged FFHQ dataset</figcaption>
</figure>
<p>To get a sense of the scale of the dataset, I compiled all face images into a grid, reducing the size of each image to 16 by 16px. This simple montage makes it possible to get a feeling for the vast amount of normalized image data.</p>
<p>The authors claim that FFHQ includes more variety than other face image sets in terms “of age, ethnicity and image background, and also has much better coverage of accessories such as eyeglasses, sun-glasses, hats, etc.”<a href="#fn38" class="footnote-ref" id="fnref38" role="doc-noteref"><sup>38</sup></a> They admit to biases inherited from the Flickr platform but fail to mention them. Looking through the data, it becomes clear that most images are taken with high-resolution digital camera systems under good lighting conditions. Many pictures separate the subject from the background by blurring the background, which conforms to photographic aesthetic norms and is easier to achieve with professional lenses with a lower aperture.</p>
<p>Indeed, the dataset contains a variety of skin tones, age groups, and backgrounds, but they are not equally distributed. I averaged the pixel values of the images together, suppressing outliers but allowing us to see an overall trend of the dataset and find a visual bias. The resulting composite image, the <em>Flickr Face</em>, reveals a tendency towards smiling and light-skinned people in the data set.</p>
<p>My project, <em>“This Person Does Exist,”</em> is equally flawed in preserving user privacy as the research labs in question. Still, by making people uncomfortable and showing the human behind AI systems, we can better understand how creepy this harvesting of faces is.</p>
<p>As a part of a growing group of artists exploring and exposing research datasets, I tried to find a new term for this genre: Dataset Art. My paper on this subject was published in <em>Temes de Disseny</em> in 2021 and included more examples from contemporary artists. Through their works, these artists make large datasets understandable and captivating. Their art has sometimes even created enough attention to lead some institutions to remove questionable parts or entire collections. Whether through galleries or online, Dataset Art provides a new way for viewers to peek into the workings of AI systems.<a href="#fn39" class="footnote-ref" id="fnref39" role="doc-noteref"><sup>39</sup></a></p>
<h2 id="scrapism">Scrapism</h2>
<p>Web scraping is the technique of using computer programs to visit links and automatically aggregate data from the internet. It is the backbone for many of the current machine-learning applications. The artist Sam Lavigne uses web scraping to create art with an emotional or critical message, a practice he calls ‘Scrapism.’<a href="#fn40" class="footnote-ref" id="fnref40" role="doc-noteref"><sup>40</sup></a></p>
<p>Perhaps an early work using the internet as a data source to convey emotion is <em>Listening Post</em> by Mark Hansen and Paul Ruben. The work was awarded the Golden Nica in 2004, uses snippets from internet chat rooms and displays them on over 200 LED signs while a crude computer voice reads them out loud. The artists are transforming live data into a sensual experience where the viewer enters a dark room and listens to the world chatter happening simultaneously all over the planet.<a href="#fn41" class="footnote-ref" id="fnref41" role="doc-noteref"><sup>41</sup></a></p>
<p>Instead of using and exposing datasets made for scientific research, Lavigne creates his own datasets by downloading and analyzing materials on the internet to revert common power structures. For example, he produced the online artwork <em>New York Apartment</em> with Tega Brain for the Artport Collection of the Whitney Museum.<a href="#fn42" class="footnote-ref" id="fnref42" role="doc-noteref"><sup>42</sup></a> In this work, they collected all for-sale listings on multiple real estate websites for New York City apartments and created a website that compiles all of them into one giant apartment listing. The value of this fictional apartment is over 43 billion USD and boasts 65,674 bedrooms and 55,588 bathrooms on around 3.4 million square meters. The website consists of multiple columns describing the listing with all of the clichéd languages and staged photographs common in the real estate market. They extruded the floor plans into 3D models and placed them together next to each other in a tower or pyramid formation so that you can explore this maze of apartments. The videos are cut up into thematic categories like “Welcome,” “Bedroom,” “Master,” or “Pre-war,” creating strange supercuts of panning and zooming shots of slick interiors.</p>
<p>Although looking through this compilation is funny and entertaining, it reveals the absurd language of luxury commodities. It shows the inequality of who can afford to own housing in a city like New York.</p>
<p>Another experiment by Sam Lavigne uses an open source hair detector to create a compilation of Mark Zuckerberg hairstyles in reaction to multiple people mocking the billionaire for his ‘terrible’ hairstyle.<a href="#fn43" class="footnote-ref" id="fnref43" role="doc-noteref"><sup>43</sup></a> This project, in particular, inspired me as it shows that using web scraping and machine learning technologies can sometimes be for silly projects at the expense of billionaires funding this tech for surveillance and personal profits.</p>
<figure>
<img src="lavigne_zuck-cut.jpg" alt="Sam Lavigne - Zuckerhair" /><figcaption aria-hidden="true">Sam Lavigne - Zuckerhair</figcaption>
</figure>
<h2 id="hidden.pictures">hidden.pictures</h2>
<p>When downloading things from the web, we often assume that everything is stored for eternity. “The web never forgets” is a common phrase to warn people before uploading sensitive and personal content online. It can be true for widely shared content or data automatically scraped and stored on sites like archive.org, but the web is brittle for many files and links. Domain names expire and cut the link to the requested page. But even if the hypertext can be accessed, they often contain links to files that no longer exist on the server; in the case of images, the browser then shows a broken image icon and an empty rectangle.</p>
<p>For the online artwork <em>hidden.pictures</em> (previously called <em>empty.photos</em>) I created a web crawler that visits random URLs, and whenever it hits an image that did not load correctly, it gets stored in a database. I collected thousands of broken image links with their metadata that sometimes describes the image.<a href="#fn44" class="footnote-ref" id="fnref44" role="doc-noteref"><sup>44</sup></a></p>
<p>I present the viewer with a collage of randomly placed image rectangles on a blank white website. A visitor can pan around in all directions, finding their web browser’s default rendering for broken images, often showcasing a ripped paper icon on the top left corner. When hovering over one of the boxes, the original URL is shown on the bottom left corner of the page, and whenever it existed, the alt-tag pops up next to the cursor.</p>
<p>These images from thousands of blogs, shops, and forums show the forgotten and neglected part of the internet. It invites the imagination about the internet that is not getting maintained and reveals an even bigger source of data; data that does not exist anymore.</p>
<p>When looking at this through the lens of weighted networks and training data, we have to ask what these models are learning when so many things get missing on the net every day. On the other hand, the models become a blurry snapshot of what they could gather but might not exist in the future.</p>
<figure>
<img src="emptyphotos-IMG_2261-HDR_1.jpg" alt="empty.photos exhibited at Best Off 2019" /><figcaption aria-hidden="true">empty.photos exhibited at Best Off 2019</figcaption>
</figure>
<h2 id="doggg.art">doggg.art</h2>
<p>In 2020 I created doggg.art, an exercise in <em>scrapism</em> where I downloaded and transformed content from the social media giant Instagram. Instagram has become the biggest tool for artists to find an audience and a community. Every possible niche can be found using hashtags like #dogart, which collects drawings and photographs of personal and commercial dog-related imagery. With around a million posts, it is only one example of the immense creative output on the image platform. Facebook, the mother company of Instagram, has access to abundant image data that they analyze and use to optimize internal computational models. By posting on the platform, a user “grant[s] a non-exclusive, royalty-free, transferable, sub-licensable, worldwide license to host, use, distribute, modify, run, copy, publicly perform or display, translate, and create derivative works of [their] content.”<a href="#fn45" class="footnote-ref" id="fnref45" role="doc-noteref"><sup>45</sup></a></p>
<p>I would describe doggg.art as a generative big dada collage consisting of over 30k images from Instagram tagged with #dogart. The images were processed using a machine learning algorithm called U^2-Net, which removed the background and other elements from the pictures. A website then randomly places the cutouts on a beige background, slowly fading them, creating an ever-changing digital dog meadow. It is a collaborative work that combines pieces by 38326 artists, credited on a separate page with all their unique usernames.<a href="#fn46" class="footnote-ref" id="fnref46" role="doc-noteref"><sup>46</sup></a></p>
<p>The artwork is intended as a screensaver reflecting on the aesthetics of the platform Instagram and how our relationship with pets extends into this online network.</p>
<figure>
<img src="doggg.art_Digitale.jpeg" alt="doggg.art exhibited at Die Digitale Düsseldorf exhibition “Digital Jokes”" /><figcaption aria-hidden="true">doggg.art exhibited at Die Digitale Düsseldorf exhibition “Digital Jokes”</figcaption>
</figure>
<figure>
<img src="doggg.art_Digitale_2.jpeg" alt="A visitor looking through the names of dog artists" /><figcaption aria-hidden="true">A visitor looking through the names of dog artists</figcaption>
</figure>
<h2 id="a-study-on-the-characteristics-of-douyin-meanwhile-in-china">A study on the Characteristics of Douyin / Meanwhile in China</h2>
<p>Working with and about social networks has been a big part of my artistic work. In 2019 I was invited by UNESCO to participate in a residency in the city of Changsha, the capital of the Hunan region and the “City of Media Arts” in central China. While there, I tried to make sense of the information landscape around me and got hooked on the Chinese clone of TikTok. </p>
<p>Douyin 抖音 became one of the most successful apps worldwide, the leading platform for creating and sharing short videos. Created by the Beijing-based company Bytedance, it is one of the few Apps that got successful outside of the great firewall. To comply with Chinese law, Douyin is an entirely separate App from TikTok. Even though the interface and logo look the same, the content is completely different and not accessible from the international version.</p>
<p>I asked S()fia Braga to join me as a collaborator, inspired by the work of <em>I stalk myself more than I should</em> where she captures Instagram stories, a feature designed to expose a video for a limited duration of 24 hours, and reappropriates them in a video installation. </p>
<p>Our work aimed to explore and analyze the vast digital ecosystem of Douyin from different perspectives, using their recommendation algorithm to lead us to various aspects of Chinese social media. We are using screen recordings to capture hours of video footage of us scrolling through our feeds. These found images are then decontextualized without alteration to give visitors the space to reflect upon them and gain insight into a walled-off platform and the algorithms designed for user engagement.  </p>
<p>We made two video installations, running 8 hours of captured material, showing people dancing next to Chinese police forces showcasing their equipment. Sometimes we would show the blank interface showing blank search results. Search terms like Donald Trump result in videos that do not show the American president themself, indicating that Douyin uses facial recognition for censorship.</p>
<p>In addition to the unaltered screen captured, we created nonsensical graphs that invoke the feeling that the content is used for statistical analysis. The title <em>A study on the characteristics of Douyin</em> was taken from a sociological paper and gave the veneer of scientific legitimacy to the non-consensual stalking and capturing of our practice. In another exhibition at the Ars Electronica Festival in Linz, we changed the title to <em>Meanwhile in China</em>. We covered the wall behind the video screen with a collage of graphs and data points found online showcasing the exploding growth and user distribution on the platform. However, the charts are stripped of any labeling, making them unreadable and useless as visualization.</p>
<figure>
<img src="IMG_1853.jpg" alt="A Study on the Characteristics of Douyin at Xie Zielong Photography Museum in Changsha" /><figcaption aria-hidden="true">A Study on the Characteristics of Douyin at Xie Zielong Photography Museum in Changsha</figcaption>
</figure>
<figure>
<img src="meanwhile_landscape.jpg" alt="Meanwhile In China at Ars Electronica 2019" /><figcaption aria-hidden="true">Meanwhile In China at Ars Electronica 2019</figcaption>
</figure>
<h2 id="recommended-hashtags-more-of-the-same">Recommended Hashtags &amp; More of the same</h2>
<p>I have had an account on Instagram since 2012, shortly after it was released on Android devices. The social network was based around square images, often highly edited with filters inspired by old analog photos. When I joined Instagram, I already had a critical view of large social networks like Facebook, where I started creating online and offline performances.</p>
<p>On Instagram, I would often share references to technologies in the social sphere and where they break, like screenshots of loading spinners, weird advertisements, or comments by bots. I often used generic hashtags to get attention outside my friend network and reach even more bots and strangers. The hashtag originated from the IRC protocol, where # was used before the name of a chat room (channel) and became a popular way on Twitter for tagging events, movements, or topics from 2007 onwards.<a href="#fn47" class="footnote-ref" id="fnref47" role="doc-noteref"><sup>47</sup></a> Today, it is a ubiquitous way of organizing and tagging content into categories, but it is most often used to get attention inside the network. For this, the game on social media is to add as many relevant and trending topics into the description as possible. Many companies have sprung up to help users ‘optimize’ their hashtags. From 2015 onward, I started using hashtags for nonsensical and humorous image descriptions and later decided to use hashtag generators to create lists of irrelevant tags for my posts on Instagram as social commentary.</p>
<p>In 2020 just after the first summer of the new COVID pandemic, S()fia Braga organized the <em>Internet Yami Ichi</em>, a black market for internet-inspired products invented by the Japanese art collective IDPW.<a href="#fn48" class="footnote-ref" id="fnref48" role="doc-noteref"><sup>48</sup></a> </p>
<p>I decided to print and sell the five years of my Instagram performance for this flea market with a book titled <em>Recommended Hashtags</em>, each of the 216 pages containing the image, hashtags, and number of likes. It also had a dedication to all my followers on the last page. After downloading and printing my Instagram data, I deleted all my images on the platform and added the label ‘Post-Instagram Artist’ to my account. </p>
<p>As users of Facebook’s platforms, we know our content is being analyzed for profit. With my online performance, I aimed to subvert and add noise to the system, albeit not enough to make a real difference. Selling my book was a way to compensate for the ‘work’ I have been doing for Facebook. I sold the book in a staggering model where each edition doubled in price from the previous one, making my old data more valuable over time while it was completely free at the time of creation. </p>
<p>In the following year, 2021, I started a new performance titled <em>more of the same</em> on my account using the dataset I downloaded as source material for a generative adversarial network. I trained a StyleGAN model, originally invented to create synthetic human faces, on the images I posted from 2015 to 2020. The model created abstract shapes, colors, and textures instead of replicating my images. As ~200 pictures are a very small dataset and the images are not very uniform ranging from screenshots to photographs depicting objects or people, the model could not converge on any meaningful representation. Many of the generated images are nearly symmetrical, reminiscent of Rorschach inkblots, which results from augmenting my image dataset by flipping the images. They also show circular blobs because of the model architecture and are seen as an error of the weighted network.<a href="#fn49" class="footnote-ref" id="fnref49" role="doc-noteref"><sup>49</sup></a></p>
<p>Ultimately, I wrote a small program that uploaded and posted a newly generated image with the caption “more of the same” for 100 days. </p>
<figure>
<img src="more_of_the_same.png" alt="Screenshot of more of the same | 200" /><figcaption aria-hidden="true">Screenshot of more of the same | 200</figcaption>
</figure>
<p><img src="Recommended_Hashtags_0.jpg" alt="Book cover of Recommended Hashtags | 200" /> <img src="Recommended_Hashtags_1.jpg" alt="First page of Recommended Hashtags | 200" /></p>
<h2 id="reflection">Reflection</h2>
<p>The idea behind computer programs that simulate the human brain goes back to the 1950s and not much has changed in the basic architecture of such systems, a few other things had to come together to make weighted networks perform so well. One crucial aspect was scale, which allowed researchers to access vast amounts of digital data uploaded and indexed on platforms and search engines. Another factor was the ability to programmatically exploit workers for minimal tasks like labeling images. Lastly, the utilization of parallel processing on graphics cards played a significant role in processing large datasets for an extended amount of time. I shared my personal experience how I got in contact with operational images through the viewing of 11,000 hands and investigated how creative commons licenses are used to deflect responsibility around privacy and copyright from researchers. As an example I looked through images of Flickr faces that served as a dataset for new synthetic image models. I also examined two art forms that emerged from these technological changes: Dataset Art and Scrapism. Dataset Art involves the re-framing of scientific datasets exposing them to a wider public and Scrapism is the practice of downloading assets from the internet for artistic purposes. I placed some of my works that fit into these categories and gave examples by other artists, such as Adam Harvey, Kate Crawford &amp; Trevor Paglen or Sam Lavigne. These projects all demonstrate how the internet shifted from a place to a corpus.</p>
<p>How the models derived from online data are reflecting us and how we find meaning in their outputs will be the focus of the next chapter. I will specifically take a closer look into text models which compress ever larger amounts of internet writing into weighted networks.</p>
<hr />
<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-2ndUnconstrainedFace" class="csl-entry" role="doc-biblioentry">
<span>“2nd <span>Unconstrained Face Detection</span> and <span>Open Set Recognition Challenge</span>, <span>ECCV-2018</span>.”</span> n.d. Accessed March 11, 2023. <a href="https://vast.uccs.edu/Opensetface/">https://vast.uccs.edu/Opensetface/</a>.
</div>
<div id="ref-afifi11KHandsGender2018" class="csl-entry" role="doc-biblioentry">
Afifi, Mahmoud. 2018. <span>“<span>11k Hands</span>: <span>Gender</span> Recognition and Biometric Identification Using a Large Dataset of Hand Images.”</span> September 16, 2018. <a href="https://doi.org/10.48550/arXiv.1711.04322">https://doi.org/10.48550/arXiv.1711.04322</a>.
</div>
<div id="ref-afifi11kHands" class="csl-entry" role="doc-biblioentry">
———. n.d. <span>“11k <span>Hands</span>.”</span> Accessed March 7, 2023. <a href="https://sites.google.com/view/11khands">https://sites.google.com/view/11khands</a>.
</div>
<div id="ref-ArsElectronicaArchiv" class="csl-entry" role="doc-biblioentry">
<span>“Ars <span>Electronica Archiv</span>.”</span> n.d. Accessed March 20, 2023. <a href="https://archive.aec.at/prix/showmode/88/">https://archive.aec.at/prix/showmode/88/</a>.
</div>
<div id="ref-chakelianJourneyManyFaces2021" class="csl-entry" role="doc-biblioentry">
Chakelian, Anoosh. 2021. <span>“The Journey and Many Faces of the Hash Symbol.”</span> <span>New Statesman</span>. June 9, 2021. <a href="https://www.newstatesman.com/science-tech/2014/06/history-journey-and-many-faces-hash-symbol">https://www.newstatesman.com/science-tech/2014/06/history-journey-and-many-faces-hash-symbol</a>.
</div>
<div id="ref-ciresanFlexibleHighPerformance2011" class="csl-entry" role="doc-biblioentry">
Cireşan, Dan C., Ueli Meier, Jonathan Masci, Luca M. Gambardella, and Jürgen Schmidhuber. 2011. <span>“Flexible, High Performance Convolutional Neural Networks for Image Classification.”</span> In <em>Proceedings of the <span>Twenty-Second</span> International Joint Conference on <span>Artificial Intelligence</span> - <span>Volume Volume Two</span></em>, 1237–42. <span>IJCAI</span>’11. <span>Barcelona, Catalonia, Spain</span>: <span>AAAI Press</span>.
</div>
<div id="ref-dempsey-jonesNeuroscientistsPutDubious2018" class="csl-entry" role="doc-biblioentry">
Dempsey-Jones, Harriet. 2018. <span>“Neuroscientists Put the Dubious Theory of ’Phrenology’ Through Rigorous Testing for the First Time.”</span> <span>The Conversation</span>. January 22, 2018. <a href="http://theconversation.com/neuroscientists-put-the-dubious-theory-of-phrenology-through-rigorous-testing-for-the-first-time-88291">http://theconversation.com/neuroscientists-put-the-dubious-theory-of-phrenology-through-rigorous-testing-for-the-first-time-88291</a>.
</div>
<div id="ref-dengImageNetLargeScaleHierarchical2009" class="csl-entry" role="doc-biblioentry">
Deng, J., W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. 2009. <span>“<span>ImageNet</span>: <span>A Large-Scale Hierarchical Image Database</span>.”</span> In <em><span>Cvpr09</span></em>.
</div>
<div id="ref-ExcavatingAI" class="csl-entry" role="doc-biblioentry">
<span>“Excavating <span>AI</span>.”</span> n.d. <span>-</span>. Accessed March 10, 2023. <a href="https://excavating.ai">https://excavating.ai</a>.
</div>
<div id="ref-FFHQDatasetSearch" class="csl-entry" role="doc-biblioentry">
<span>“<span>FFHQ</span> Dataset Search Form.”</span> n.d. Accessed March 15, 2023. <a href="https://nvlabs.github.io/ffhq-dataset/search/">https://nvlabs.github.io/ffhq-dataset/search/</a>.
</div>
<div id="ref-fukushimaCognitronSelforganizingMultilayered1975" class="csl-entry" role="doc-biblioentry">
Fukushima, Kunihiko. 1975. <span>“Cognitron: <span>A</span> Self-Organizing Multilayered Neural Network.”</span> <em>Biological Cybernetics</em> 20 (3-4): 121–36. <a href="https://doi.org/10.1007/BF00342633">https://doi.org/10.1007/BF00342633</a>.
</div>
<div id="ref-fukushimaNeocognitronHierarchicalNeural1988" class="csl-entry" role="doc-biblioentry">
———. 1988. <span>“Neocognitron: <span>A</span> Hierarchical Neural Network Capable of Visual Pattern Recognition.”</span> <em>Neural Networks</em> 1 (2): 119–30. <a href="https://doi.org/10.1016/0893-6080(88)90014-7">https://doi.org/10.1016/0893-6080(88)90014-7</a>.
</div>
<div id="ref-gershgornDataThatTransformed2017" class="csl-entry" role="doc-biblioentry">
Gershgorn, Dave. 2017. <span>“The Data That Transformed <span>AI</span> Research—and Possibly the World.”</span> <span>Quartz</span>. July 26, 2017. <a href="https://qz.com/1034972/the-data-that-changed-the-direction-of-ai-research-and-possibly-the-world/">https://qz.com/1034972/the-data-that-changed-the-direction-of-ai-research-and-possibly-the-world/</a>.
</div>
<div id="ref-haraDataDrivenAnalysisWorkers2017" class="csl-entry" role="doc-biblioentry">
Hara, Kotaro, Abi Adams, Kristy Milland, Saiph Savage, Chris Callison-Burch, and Jeffrey Bigham. 2017. <span>“A <span>Data-Driven Analysis</span> of <span>Workers</span>’ <span>Earnings</span> on <span>Amazon Mechanical Turk</span>.”</span> December 28, 2017. <a href="https://doi.org/10.48550/arXiv.1712.05796">https://doi.org/10.48550/arXiv.1712.05796</a>.
</div>
<div id="ref-harveyCreativeCommonsBiometrics2022" class="csl-entry" role="doc-biblioentry">
Harvey, Adam. 2022. <span>“Creative <span>Commons Biometrics</span>.”</span> May 25, 2022. <a href="https://adam.harvey.studio/creative-commons/">https://adam.harvey.studio/creative-commons/</a>.
</div>
<div id="ref-harveyExposingAi2021" class="csl-entry" role="doc-biblioentry">
Harvey, Adam, and Jules LaPlace. 2021a. <span>“Exposing.ai.”</span> <span>Exposing.ai</span>. 2021. <a href="https://exposing.ai/">https://exposing.ai/</a>.
</div>
<div id="ref-harveyExposingAiMegaFace2021" class="csl-entry" role="doc-biblioentry">
———. 2021b. <span>“Exposing.ai: <span>MegaFace</span>.”</span> <span>Exposing.ai</span>. 2021. <a href="https://exposing.ai/datasets/megaface/">https://exposing.ai/datasets/megaface/</a>.
</div>
<div id="ref-hillHowPhotosYour2019" class="csl-entry" role="doc-biblioentry">
Hill, Kashmir, and Aaron Krolik. 2019. <span>“How <span>Photos</span> of <span>Your Kids Are Powering Surveillance Technology</span>.”</span> <em>The New York Times: Technology</em>, October 11, 2019. <a href="https://www.nytimes.com/interactive/2019/10/11/technology/flickr-facial-recognition.html">https://www.nytimes.com/interactive/2019/10/11/technology/flickr-facial-recognition.html</a>.
</div>
<div id="ref-hoelOperativeImagesInroads2018" class="csl-entry" role="doc-biblioentry">
Hoel, Aud Sissel. 2018. <span>“Operative <span>Images</span>. <span>Inroads</span> to a <span>New Paradigm</span> of <span>Media Theory</span>.”</span> In <em>Image – <span>Action</span> – <span>Space</span></em>, 11–28. <span>De Gruyter</span>. <a href="https://doi.org/10.1515/9783110464979-002">https://doi.org/10.1515/9783110464979-002</a>.
</div>
<div id="ref-HowReadPalms" class="csl-entry" role="doc-biblioentry">
<span>“How to <span>Read Palms</span>: 9 <span>Steps</span> (with <span>Pictures</span>).”</span> n.d. <span>wikiHow</span>. Accessed March 8, 2023. <a href="https://www.wikihow.com/Read-Palms">https://www.wikihow.com/Read-Palms</a>.
</div>
<div id="ref-ImageNetRouletteTrevor" class="csl-entry" role="doc-biblioentry">
<span>“<span>ImageNet Roulette</span> – <span>Trevor Paglen</span>.”</span> n.d. Accessed March 11, 2023. <a href="https://paglen.studio/2020/04/29/imagenet-roulette/">https://paglen.studio/2020/04/29/imagenet-roulette/</a>.
</div>
<div id="ref-karrasStyleBasedGeneratorArchitecture2019" class="csl-entry" role="doc-biblioentry">
Karras, Tero, Samuli Laine, and Timo Aila. 2019. <span>“A <span>Style-Based Generator Architecture</span> for <span>Generative Adversarial Networks</span>.”</span> March 29, 2019. <a href="https://doi.org/10.48550/arXiv.1812.04948">https://doi.org/10.48550/arXiv.1812.04948</a>.
</div>
<div id="ref-karrasAnalyzingImprovingImage2020" class="csl-entry" role="doc-biblioentry">
Karras, Tero, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. 2020. <span>“Analyzing and <span>Improving</span> the <span>Image Quality</span> of <span>StyleGAN</span>.”</span> March 23, 2020. <a href="http://arxiv.org/abs/1912.04958">http://arxiv.org/abs/1912.04958</a>.
</div>
<div id="ref-KATECRAWFORDTREVOR" class="csl-entry" role="doc-biblioentry">
<span>“<span>KATE CRAWFORD</span> | <span>TREVOR PAGLEN</span>: <span>TRAINING HUMANS</span> – <span>Fondazione Prada</span>.”</span> n.d. Accessed March 10, 2023. <a href="https://www.fondazioneprada.org/project/training-humans/?lang=en">https://www.fondazioneprada.org/project/training-humans/?lang=en</a>.
</div>
<div id="ref-krizhevskyImageNetClassificationDeep2017" class="csl-entry" role="doc-biblioentry">
Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. 2017. <span>“<span>ImageNet</span> Classification with Deep Convolutional Neural Networks.”</span> <em>Communications of the ACM</em> 60 (6): 84–90. <a href="https://doi.org/10.1145/3065386">https://doi.org/10.1145/3065386</a>.
</div>
<div id="ref-lavigneScrapism" class="csl-entry" role="doc-biblioentry">
Lavigne, Sam. n.d. <span>“Scrapism.”</span> Accessed March 15, 2023. <a href="https://scrapism.lav.io/">https://scrapism.lav.io/</a>.
</div>
<div id="ref-lavigneNewYorkApartment" class="csl-entry" role="doc-biblioentry">
Lavigne, Sam, and Tega Brain. n.d. <span>“New <span>York Apartment</span>.”</span> Accessed March 17, 2023. <a href="https://artport.whitney.org/commissions/new-york-apartment/index.html">https://artport.whitney.org/commissions/new-york-apartment/index.html</a>.
</div>
<div id="ref-matthiasschafer11kHands2018" class="csl-entry" role="doc-biblioentry">
Matthias Schäfer, dir. 2018. <em>11k Hands</em>. <a href="https://www.youtube.com/watch?v=snJsKFxPlJ8">https://www.youtube.com/watch?v=snJsKFxPlJ8</a>.
</div>
<div id="ref-mitchellArtificialIntelligenceGuide2019" class="csl-entry" role="doc-biblioentry">
Mitchell, Melanie. 2019. <em>Artificial Intelligence: A Guide for Thinking Humans</em>. <span>New York</span>: <span>Farrar, Straus and Giroux</span>.
</div>
<div id="ref-murphyWhyStanfordResearchers2017" class="csl-entry" role="doc-biblioentry">
Murphy, Heather. 2017. <span>“Why <span>Stanford Researchers Tried</span> to <span>Create</span> a <span>‘<span>Gaydar</span>’</span> <span>Machine</span>.”</span> <em>The New York Times: Science</em>, October 9, 2017. <a href="https://www.nytimes.com/2017/10/09/science/stanford-sexual-orientation-study.html">https://www.nytimes.com/2017/10/09/science/stanford-sexual-orientation-study.html</a>.
</div>
<div id="ref-NutzungsbedingungenInstagramHilfebereich" class="csl-entry" role="doc-biblioentry">
<span>“Nutzungsbedingungen | <span>Instagram-Hilfebereich</span>.”</span> n.d. Accessed March 17, 2023. <a href="https://help.instagram.com/581066165581870">https://help.instagram.com/581066165581870</a>.
</div>
<div id="ref-OpenFutureOpen" class="csl-entry" role="doc-biblioentry">
<span>“Open <span>Future</span> – <span>Open Future Foundation</span>.”</span> n.d. <span>Open Future</span>. Accessed March 13, 2023. <a href="https://openfuture.eu">https://openfuture.eu</a>.
</div>
<div id="ref-reaHowImageNetRoulette2019" class="csl-entry" role="doc-biblioentry">
Rea, Naomi. 2019. <span>“How <span>ImageNet Roulette</span>, a <span>Viral Art Project That Exposed Facial Recognition</span>’s <span>Biases</span>, <span>Is Changing Minds About AI</span>.”</span> <span>Artnet News</span>. September 23, 2019. <a href="https://news.artnet.com/art-world/imagenet-roulette-trevor-paglen-kate-crawford-1658305">https://news.artnet.com/art-world/imagenet-roulette-trevor-paglen-kate-crawford-1658305</a>.
</div>
<div id="ref-rosenblattPerceptronProbabilisticModel1958" class="csl-entry" role="doc-biblioentry">
Rosenblatt, F. 1958. <span>“The Perceptron: <span>A</span> Probabilistic Model for Information Storage and Organization in the Brain.”</span> <em>Psychological Review</em> 65 (6): 386–408. <a href="https://doi.org/10.1037/h0042519">https://doi.org/10.1037/h0042519</a>.
</div>
<div id="ref-samlavigneJustDiscoveredOpen2020" class="csl-entry" role="doc-biblioentry">
Sam Lavigne. 2020. <span>“Just Discovered an Open Source Hair Detector and Have Used It on Hundreds of Images of <span>Mark Zuckerberg</span> to Build What <span>I</span> Believe Is the Most Comprehensive Archive of <span>Zuckerberg</span> Haircuts in Existence. <span>Thank</span> You <span>AI</span> Researchers! <span class="nocase">https://t.co/QJqXLAbqyw</span>.”</span> Tweet. <span>Twitter</span>. February 16, 2020. <a href="https://twitter.com/sam_lavigne/status/1229097648818446336">https://twitter.com/sam_lavigne/status/1229097648818446336</a>.
</div>
<div id="ref-schaferMissingPictures2018" class="csl-entry" role="doc-biblioentry">
Schäfer, Matthias. 2018. <span>“Missing.pictures.”</span> <span>missing.pictures</span>. 2018. <a href="https://missing.pictures/">https://missing.pictures/</a>.
</div>
<div id="ref-schaferThisPersonDoes2021" class="csl-entry" role="doc-biblioentry">
———. 2021. <span>“This <span>Person Does Exist</span>.”</span> <em>Temes de Disseny</em>, no. 37 (July): 214–25. <a href="https://doi.org/10.46467/TdD37.2021.214-225">https://doi.org/10.46467/TdD37.2021.214-225</a>.
</div>
<div id="ref-tappertWhoFatherDeep2019" class="csl-entry" role="doc-biblioentry">
Tappert, Charles C. 2019. <span>“Who <span>Is</span> the <span>Father</span> of <span>Deep Learning</span>?”</span> In <em>2019 <span>International Conference</span> on <span>Computational Science</span> and <span>Computational Intelligence</span> (<span>CSCI</span>)</em>, 343–48. <span>Las Vegas, NV, USA</span>: <span>IEEE</span>. <a href="https://doi.org/10.1109/CSCI49370.2019.00067">https://doi.org/10.1109/CSCI49370.2019.00067</a>.
</div>
<div id="ref-TechnicallyResponsibleKnowledge" class="csl-entry" role="doc-biblioentry">
<span>“Technically <span>Responsible Knowledge</span>.”</span> n.d. Accessed March 10, 2023. <a href="http://trk.network/essay">http://trk.network/essay</a>.
</div>
<div id="ref-InternetYamiIchi" class="csl-entry" role="doc-biblioentry">
<span>“The Internet Yami-Ichi.”</span> n.d. <span>In Kepler’s Gardens</span>. Accessed March 20, 2023. <a href="https://ars.electronica.art/keplersgardens/de/the-internet-yami-ichi/">https://ars.electronica.art/keplersgardens/de/the-internet-yami-ichi/</a>.
</div>
<div id="ref-ThisPersonDoes2019" class="csl-entry" role="doc-biblioentry">
<span>“This <span>Person Does Not Exist</span>.”</span> 2019. May 31, 2019. <a href="https://web.archive.org/web/20190531222303/https://thispersondoesnotexist.com/">https://web.archive.org/web/20190531222303/https://thispersondoesnotexist.com/</a>.
</div>
<div id="ref-XTRAMarkHansen" class="csl-entry" role="doc-biblioentry">
<span>“X-<span>TRA</span> → <span>Mark Hansen</span> and <span>Ben Rubin</span>: <span>Listening Post</span>.”</span> n.d. Accessed March 20, 2023. <a href="https://www.x-traonline.org/article/mark-hansen-and-ben-rubin-listening-post">https://www.x-traonline.org/article/mark-hansen-and-ben-rubin-listening-post</a>.
</div>
</div>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p><span class="citation" data-cites="rosenblattPerceptronProbabilisticModel1958"><a href="#ref-rosenblattPerceptronProbabilisticModel1958" role="doc-biblioref">Rosenblatt</a> (<a href="#ref-rosenblattPerceptronProbabilisticModel1958" role="doc-biblioref">1958</a>)</span> .<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>The Mark I was an electromechanical machine that used motor-driven potentiometers to adjust the variable weights.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p>Frank Rosenblatt died in a boating accident in 1971. A couple of years prior, Marvin Minsky heavily criticized the mathematics behind perceptrons and advocated for a symbolic approach. These events might have led to a lack of funding in the ‘connectionist’ AI research field and ultimately to a general disinterest when the symbolic approach could not keep its exaggerated promises.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4" role="doc-endnote"><p><span class="citation" data-cites="mitchellArtificialIntelligenceGuide2019"><a href="#ref-mitchellArtificialIntelligenceGuide2019" role="doc-biblioref">Mitchell</a> (<a href="#ref-mitchellArtificialIntelligenceGuide2019" role="doc-biblioref">2019</a>)</span>, p. 114.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5" role="doc-endnote"><p><span class="citation" data-cites="fukushimaCognitronSelforganizingMultilayered1975"><a href="#ref-fukushimaCognitronSelforganizingMultilayered1975" role="doc-biblioref">Fukushima</a> (<a href="#ref-fukushimaCognitronSelforganizingMultilayered1975" role="doc-biblioref">1975</a>)</span> .<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6" role="doc-endnote"><p><span class="citation" data-cites="fukushimaNeocognitronHierarchicalNeural1988"><a href="#ref-fukushimaNeocognitronHierarchicalNeural1988" role="doc-biblioref">Fukushima</a> (<a href="#ref-fukushimaNeocognitronHierarchicalNeural1988" role="doc-biblioref">1988</a>)</span> .<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7" role="doc-endnote"><p><span class="citation" data-cites="tappertWhoFatherDeep2019"><a href="#ref-tappertWhoFatherDeep2019" role="doc-biblioref">Tappert</a> (<a href="#ref-tappertWhoFatherDeep2019" role="doc-biblioref">2019</a>)</span> .<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8" role="doc-endnote"><p><span class="citation" data-cites="krizhevskyImageNetClassificationDeep2017"><a href="#ref-krizhevskyImageNetClassificationDeep2017" role="doc-biblioref">Krizhevsky, Sutskever, and Hinton</a> (<a href="#ref-krizhevskyImageNetClassificationDeep2017" role="doc-biblioref">2017</a>)</span> .<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9" role="doc-endnote"><p>“One epoch takes 35 GPU minutes but more than 35 CPU hours.” <span class="citation" data-cites="ciresanFlexibleHighPerformance2011"><a href="#ref-ciresanFlexibleHighPerformance2011" role="doc-biblioref">Cireşan et al.</a> (<a href="#ref-ciresanFlexibleHighPerformance2011" role="doc-biblioref">2011</a>)</span>.<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10" role="doc-endnote"><p><span class="citation" data-cites="afifi11KHandsGender2018"><a href="#ref-afifi11KHandsGender2018" role="doc-biblioref">Afifi</a> (<a href="#ref-afifi11KHandsGender2018" role="doc-biblioref">2018</a>)</span>; data and source code are available at <span class="citation" data-cites="afifi11kHands"><a href="#ref-afifi11kHands" role="doc-biblioref">Afifi</a> (<a href="#ref-afifi11kHands" role="doc-biblioref">n.d.</a>)</span>.<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11" role="doc-endnote"><p>See <span class="citation" data-cites="dempsey-jonesNeuroscientistsPutDubious2018"><a href="#ref-dempsey-jonesNeuroscientistsPutDubious2018" role="doc-biblioref">Dempsey-Jones</a> (<a href="#ref-dempsey-jonesNeuroscientistsPutDubious2018" role="doc-biblioref">2018</a>)</span>. Parker et al. jokingly tested the Galls theory using 21st-century scientific methods and MRI data.<a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn12" role="doc-endnote"><p>I didn’t test the system as I don’t know how to run MatLab code, but I can imagine that the slightly better results on the dorsal hand result from nail polish only being applied on female hands. <a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn13" role="doc-endnote"><p><span class="citation" data-cites="hoelOperativeImagesInroads2018"><a href="#ref-hoelOperativeImagesInroads2018" role="doc-biblioref">Hoel</a> (<a href="#ref-hoelOperativeImagesInroads2018" role="doc-biblioref">2018</a>)</span> .<a href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn14" role="doc-endnote"><p>See <span class="citation" data-cites="matthiasschafer11kHands2018"><a href="#ref-matthiasschafer11kHands2018" role="doc-biblioref">Matthias Schäfer</a> (<a href="#ref-matthiasschafer11kHands2018" role="doc-biblioref">2018</a>)</span> .<a href="#fnref14" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn15" role="doc-endnote"><p><span class="citation" data-cites="HowReadPalms"><a href="#ref-HowReadPalms" role="doc-biblioref"><span>“How to <span>Read Palms</span>: 9 <span>Steps</span> (with <span>Pictures</span>)”</span></a> (<a href="#ref-HowReadPalms" role="doc-biblioref">n.d.</a>)</span> .<a href="#fnref15" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn16" role="doc-endnote"><p>See <span class="citation" data-cites="murphyWhyStanfordResearchers2017"><a href="#ref-murphyWhyStanfordResearchers2017" role="doc-biblioref">Murphy</a> (<a href="#ref-murphyWhyStanfordResearchers2017" role="doc-biblioref">2017</a>)</span>. A particularly famous example by Michael Kosinski and Yiluna Wang. Their flawed study tried to predict if a person is gay by scraping dating sites and training a classifier on these images.<a href="#fnref16" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn17" role="doc-endnote"><p>See <span class="citation" data-cites="dengImageNetLargeScaleHierarchical2009"><a href="#ref-dengImageNetLargeScaleHierarchical2009" role="doc-biblioref">Deng et al.</a> (<a href="#ref-dengImageNetLargeScaleHierarchical2009" role="doc-biblioref">2009</a>)</span>. ImageNet started with 3,2 million images and aimed to collect 50 million by the end of 2011.<a href="#fnref17" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn18" role="doc-endnote"><p><span class="citation" data-cites="gershgornDataThatTransformed2017"><a href="#ref-gershgornDataThatTransformed2017" role="doc-biblioref">Gershgorn</a> (<a href="#ref-gershgornDataThatTransformed2017" role="doc-biblioref">2017</a>)</span> .<a href="#fnref18" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn19" role="doc-endnote"><p><span class="citation" data-cites="haraDataDrivenAnalysisWorkers2017"><a href="#ref-haraDataDrivenAnalysisWorkers2017" role="doc-biblioref">Hara et al.</a> (<a href="#ref-haraDataDrivenAnalysisWorkers2017" role="doc-biblioref">2017</a>)</span> .<a href="#fnref19" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn20" role="doc-endnote"><p>See <span class="citation" data-cites="TechnicallyResponsibleKnowledge"><a href="#ref-TechnicallyResponsibleKnowledge" role="doc-biblioref"><span>“Technically <span>Responsible Knowledge</span>”</span></a> (<a href="#ref-TechnicallyResponsibleKnowledge" role="doc-biblioref">n.d.</a>)</span>.<a href="#fnref20" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn21" role="doc-endnote"><p>See <span class="citation" data-cites="KATECRAWFORDTREVOR"><a href="#ref-KATECRAWFORDTREVOR" role="doc-biblioref"><span>“<span>KATE CRAWFORD</span> | <span>TREVOR PAGLEN</span>: <span>TRAINING HUMANS</span> – <span>Fondazione Prada</span>”</span></a> (<a href="#ref-KATECRAWFORDTREVOR" role="doc-biblioref">n.d.</a>)</span>.<a href="#fnref21" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn22" role="doc-endnote"><p><span class="citation" data-cites="ExcavatingAI"><a href="#ref-ExcavatingAI" role="doc-biblioref"><span>“Excavating <span>AI</span>”</span></a> (<a href="#ref-ExcavatingAI" role="doc-biblioref">n.d.</a>)</span> .<a href="#fnref22" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn23" role="doc-endnote"><p>See <span class="citation" data-cites="ImageNetRouletteTrevor"><a href="#ref-ImageNetRouletteTrevor" role="doc-biblioref"><span>“<span>ImageNet Roulette</span> – <span>Trevor Paglen</span>”</span></a> (<a href="#ref-ImageNetRouletteTrevor" role="doc-biblioref">n.d.</a>)</span>.<a href="#fnref23" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn24" role="doc-endnote"><p><span class="citation" data-cites="reaHowImageNetRoulette2019"><a href="#ref-reaHowImageNetRoulette2019" role="doc-biblioref">Rea</a> (<a href="#ref-reaHowImageNetRoulette2019" role="doc-biblioref">2019</a>)</span> .<a href="#fnref24" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn25" role="doc-endnote"><p>See <span class="citation" data-cites="harveyExposingAi2021"><a href="#ref-harveyExposingAi2021" role="doc-biblioref">Harvey and LaPlace</a> (<a href="#ref-harveyExposingAi2021" role="doc-biblioref">2021a</a>)</span>.<a href="#fnref25" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn26" role="doc-endnote"><p><span class="citation" data-cites="2ndUnconstrainedFace"><a href="#ref-2ndUnconstrainedFace" role="doc-biblioref"><span>“2nd <span>Unconstrained Face Detection</span> and <span>Open Set Recognition Challenge</span>, <span>ECCV-2018</span>”</span></a> (<a href="#ref-2ndUnconstrainedFace" role="doc-biblioref">n.d.</a>)</span> .<a href="#fnref26" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn27" role="doc-endnote"><p>See <span class="citation" data-cites="harveyCreativeCommonsBiometrics2022"><a href="#ref-harveyCreativeCommonsBiometrics2022" role="doc-biblioref">Harvey</a> (<a href="#ref-harveyCreativeCommonsBiometrics2022" role="doc-biblioref">2022</a>)</span> for a more extensive analysis on the exploitation of CC media.<a href="#fnref27" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn28" role="doc-endnote"><p><span class="citation" data-cites="harveyCreativeCommonsBiometrics2022"><a href="#ref-harveyCreativeCommonsBiometrics2022" role="doc-biblioref">Harvey</a> (<a href="#ref-harveyCreativeCommonsBiometrics2022" role="doc-biblioref">2022</a>)</span> .<a href="#fnref28" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn29" role="doc-endnote"><p>YFCC100M only contains links and metadata to images and videos under CC license on Flickr.<a href="#fnref29" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn30" role="doc-endnote"><p><span class="citation" data-cites="harveyExposingAiMegaFace2021"><a href="#ref-harveyExposingAiMegaFace2021" role="doc-biblioref">Harvey and LaPlace</a> (<a href="#ref-harveyExposingAiMegaFace2021" role="doc-biblioref">2021b</a>)</span> .<a href="#fnref30" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn31" role="doc-endnote"><p><span class="citation" data-cites="hillHowPhotosYour2019"><a href="#ref-hillHowPhotosYour2019" role="doc-biblioref">Hill and Krolik</a> (<a href="#ref-hillHowPhotosYour2019" role="doc-biblioref">2019</a>)</span> .<a href="#fnref31" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn32" role="doc-endnote"><p>The Open Future Foundation is a think tank that actively tries to influence European digital policy debates. See <span class="citation" data-cites="OpenFutureOpen"><a href="#ref-OpenFutureOpen" role="doc-biblioref"><span>“Open <span>Future</span> – <span>Open Future Foundation</span>”</span></a> (<a href="#ref-OpenFutureOpen" role="doc-biblioref">n.d.</a>)</span>.<a href="#fnref32" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn33" role="doc-endnote"><p><span class="citation" data-cites="karrasStyleBasedGeneratorArchitecture2019"><a href="#ref-karrasStyleBasedGeneratorArchitecture2019" role="doc-biblioref">Karras, Laine, and Aila</a> (<a href="#ref-karrasStyleBasedGeneratorArchitecture2019" role="doc-biblioref">2019</a>)</span><a href="#fnref33" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn34" role="doc-endnote"><p>See <span class="citation" data-cites="ThisPersonDoes2019"><a href="#ref-ThisPersonDoes2019" role="doc-biblioref"><span>“This <span>Person Does Not Exist</span>”</span></a> (<a href="#ref-ThisPersonDoes2019" role="doc-biblioref">2019</a>)</span>. At the time of this writing, the URL redirects to stability.ai, but there is an archived version.<a href="#fnref34" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn35" role="doc-endnote"><p>See <span class="citation" data-cites="schaferThisPersonDoes2021"><a href="#ref-schaferThisPersonDoes2021" role="doc-biblioref">Schäfer</a> (<a href="#ref-schaferThisPersonDoes2021" role="doc-biblioref">2021</a>)</span>.<a href="#fnref35" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn36" role="doc-endnote"><p><span class="citation" data-cites="FFHQDatasetSearch"><a href="#ref-FFHQDatasetSearch" role="doc-biblioref"><span>“<span>FFHQ</span> Dataset Search Form”</span></a> (<a href="#ref-FFHQDatasetSearch" role="doc-biblioref">n.d.</a>)</span> .<a href="#fnref36" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn37" role="doc-endnote"><p>See ‘FFHQ Dataset’ at <span class="citation" data-cites="harveyCreativeCommonsBiometrics2022"><a href="#ref-harveyCreativeCommonsBiometrics2022" role="doc-biblioref">Harvey</a> (<a href="#ref-harveyCreativeCommonsBiometrics2022" role="doc-biblioref">2022</a>)</span>.<a href="#fnref37" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn38" role="doc-endnote"><p><span class="citation" data-cites="karrasStyleBasedGeneratorArchitecture2019"><a href="#ref-karrasStyleBasedGeneratorArchitecture2019" role="doc-biblioref">Karras, Laine, and Aila</a> (<a href="#ref-karrasStyleBasedGeneratorArchitecture2019" role="doc-biblioref">2019</a>)</span> .<a href="#fnref38" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn39" role="doc-endnote"><p><span class="citation" data-cites="schaferThisPersonDoes2021"><a href="#ref-schaferThisPersonDoes2021" role="doc-biblioref">Schäfer</a> (<a href="#ref-schaferThisPersonDoes2021" role="doc-biblioref">2021</a>)</span> .<a href="#fnref39" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn40" role="doc-endnote"><p><span class="citation" data-cites="lavigneScrapism"><a href="#ref-lavigneScrapism" role="doc-biblioref">Lavigne</a> (<a href="#ref-lavigneScrapism" role="doc-biblioref">n.d.</a>)</span> .<a href="#fnref40" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn41" role="doc-endnote"><p>See <span class="citation" data-cites="XTRAMarkHansen"><a href="#ref-XTRAMarkHansen" role="doc-biblioref"><span>“X-<span>TRA</span> → <span>Mark Hansen</span> and <span>Ben Rubin</span>: <span>Listening Post</span>”</span></a> (<a href="#ref-XTRAMarkHansen" role="doc-biblioref">n.d.</a>)</span> and <span class="citation" data-cites="ArsElectronicaArchiv"><a href="#ref-ArsElectronicaArchiv" role="doc-biblioref"><span>“Ars <span>Electronica Archiv</span>”</span></a> (<a href="#ref-ArsElectronicaArchiv" role="doc-biblioref">n.d.</a>)</span>.<a href="#fnref41" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn42" role="doc-endnote"><p><span class="citation" data-cites="lavigneNewYorkApartment"><a href="#ref-lavigneNewYorkApartment" role="doc-biblioref">Lavigne and Brain</a> (<a href="#ref-lavigneNewYorkApartment" role="doc-biblioref">n.d.</a>)</span> .<a href="#fnref42" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn43" role="doc-endnote"><p><span class="citation" data-cites="samlavigneJustDiscoveredOpen2020"><a href="#ref-samlavigneJustDiscoveredOpen2020" role="doc-biblioref">Sam Lavigne</a> (<a href="#ref-samlavigneJustDiscoveredOpen2020" role="doc-biblioref">2020</a>)</span> .<a href="#fnref43" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn44" role="doc-endnote"><p><span class="citation" data-cites="schaferMissingPictures2018"><a href="#ref-schaferMissingPictures2018" role="doc-biblioref">Schäfer</a> (<a href="#ref-schaferMissingPictures2018" role="doc-biblioref">2018</a>)</span> .<a href="#fnref44" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn45" role="doc-endnote"><p>See <span class="citation" data-cites="NutzungsbedingungenInstagramHilfebereich"><a href="#ref-NutzungsbedingungenInstagramHilfebereich" role="doc-biblioref"><span>“Nutzungsbedingungen | <span>Instagram-Hilfebereich</span>”</span></a> (<a href="#ref-NutzungsbedingungenInstagramHilfebereich" role="doc-biblioref">n.d.</a>)</span>.<a href="#fnref45" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn46" role="doc-endnote"><p>Complete list of artists can be found here <a href="https://doggg.art/artistlist.html">doggg.art/artistlist.html</a>.<a href="#fnref46" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn47" role="doc-endnote"><p>See <span class="citation" data-cites="chakelianJourneyManyFaces2021"><a href="#ref-chakelianJourneyManyFaces2021" role="doc-biblioref">Chakelian</a> (<a href="#ref-chakelianJourneyManyFaces2021" role="doc-biblioref">2021</a>)</span>.<a href="#fnref47" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn48" role="doc-endnote"><p>See <span class="citation" data-cites="InternetYamiIchi"><a href="#ref-InternetYamiIchi" role="doc-biblioref"><span>“The Internet Yami-Ichi”</span></a> (<a href="#ref-InternetYamiIchi" role="doc-biblioref">n.d.</a>)</span>.<a href="#fnref48" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn49" role="doc-endnote"><p>See <span class="citation" data-cites="karrasAnalyzingImprovingImage2020"><a href="#ref-karrasAnalyzingImprovingImage2020" role="doc-biblioref">Karras et al.</a> (<a href="#ref-karrasAnalyzingImprovingImage2020" role="doc-biblioref">2020</a>)</span>.<a href="#fnref49" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
</body>
</html>
