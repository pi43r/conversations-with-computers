<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>.pandoc-config</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
    }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
  
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<h1 id="hello-world">Hello, World!</h1>
<p>The common first letters a programmer types into it’s digital
computing machine is some form of “hello world” along with braces,
punctuation and words that the machine should interpret. After executing
this short paragraph of text the computer likewise responds with “Hello,
World!”.</p>
<p>This ritual of greeting each other (and the world) is accredited to
Brian W. Kernighan, who wrote the popular guide <em>The C Programming
Language</em>[1] in 1978, where he explains that the only way to learn a
programming language is by writing programs in it.</p>
<p>I wanted to start this introduction with the same greeting, as I am
typing text into a computer and which letters simultaneously appear on
the screen in front of me. But this translation from the keyboard of my
laptop to the pixels on the screen is only possible by thousands of
lines of code hidden in the source of the program. There are countless
people who have written the software and operating system I am currently
using to write this text. And there are even more people who wrote the
software, the programmers have used to write the word processor I am
using right now.</p>
<p>In this thesis I want to focus on the known and unknown human
connections we make with and through computers. My hypothesis is that
the current trend of framing “Artificial Intelligence” as autonomous
computational beings, would be better described as “Co-Intelligence”,
where the programming of such systems is a collaborative effort with
many hidden players. While I address some technical details of current
machine learning systems, I will put them into social context, because
software is inherently a social tool. And ultimately, as an artist,
address these topics through aesthetic experiences, which go beyond
logic and language.</p>
<p>A book is never written alone, in fact everything we do is influenced
by others. And while I focus on the social nature of computing and I am
influenced by many of my peers and created works with others, I am
describing these topics from my subjective perspective. I don’t claim
any objective truths on the descriptions of the collaborative works I
have done and am naturally biased towards my own contribution to it. As
this thesis is written in the context of an art program, I tend to
deviate from rigorous scientific methods and describe my observations
more freely. I am certain that there will not be a definitive answer as
there is not even an answerable research question, but even more
questions will come up along the way.</p>
<h2 id="dreaming">Dreaming</h2>
<figure>
<img src="./Images/6ocuQsZ.jpeg"
alt="“PuppySlug” - posted by deleted user on /r/creepy with the title “This image was generated by a computer on its own (from a friend working on AI)” on January 10th 2015. 7 days before the Google blog post" />
<figcaption aria-hidden="true">“PuppySlug” - posted by deleted user on
/r/creepy with the title “This image was generated by a computer on its
own (from a friend working on AI)” on January 10th 2015. 7 days before
the Google blog post</figcaption>
</figure>
<p>My journey into the hype machine of “Artificial Intelligence” started
around 2015, when Google released DeepDream<a href="#fn1"
class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> on
it’s research blog. Alexander Mordvintsev, Michael Tyka and Christopher
Olah were initially trying to peek inside of an “Artificial Neural
Network” that was “trained” for image classification tasks. They
reversed the task of the classifier and instead of identifying what a
set of pixels look like, they were changing the pixel values to look
more like what the network has “learned”. This created images that
amplified the textures embedded in their networks, transforming the
input image into swirls of PuppySlugs<a href="#fn2" class="footnote-ref"
id="fnref2" role="doc-noteref"><sup>2</sup></a>. This imagery that
reminded people of bad dreams and hallucinogenic trips was widely shared
on the internet, which skyrocketed the awareness that “Deep Learning”<a
href="#fn3" class="footnote-ref" id="fnref3"
role="doc-noteref"><sup>3</sup></a> and biologically inspired computing
has come back from it’s hibernation in the AI winter. Since 2012
“Artificial Neural Networks” have made an incredible comeback into the
field of computer vision. Where other researchers have long moved to
different techniques for image classification Yann LeCun and Ian
Goodfellow have kept researching on translations of simplified brain
models to computer systems and finally hit a breakthrough, due to
computational power and availability of data. The latter was compiled by
a team around Fei-Fei Li, who created ImageNet, a dataset of images
organized into categories by anonymous workers from the internet. The
ImageNet project held a yearly competition<a href="#fn4"
class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>
since 2010 and for the first two years the best algorithm was
recognizing these images with a 74% accuracy, until AlexNet<a
href="#fn5" class="footnote-ref" id="fnref5"
role="doc-noteref"><sup>5</sup></a>, a convolutional neural network,
topped the score with 85% and woke up the computer vision community to
the potential powers of “Artificial Neural Networks”. Finally with
DeepDream this technique became tangible for the general public, as it
created something to look at, something for humans to empathize with the
outputs of an abstract mathematical model. This tendency to
anthropomorphize this type of software becomes already clear in the way
we use language to describe it. We talk of “Neural Networks” as if they
are biological entities, that can be “taught” and which “learn”,
“experience” and “read” data from the world. This was a turning point
that got me and many other artists interested in “collaborating” with
seemingly autonomous machines. Machines that we do not have to
understand through mathematics, but through interpretation and
experimentation. Yet, the outputs of DeepDream quickly became kitsch.
Google shared the code with an open source license and many people
created apps and APIs to generate PuppySlug textures on top of their own
images<a href="#fn6" class="footnote-ref" id="fnref6"
role="doc-noteref"><sup>6</sup></a>. As with most other memes on the
internet the hype around DeepDream died quickly and by the end of 2015
the world was already saturated with computer generated hallucinogenic
images. Other researchers have since picked up<a href="#fn7"
class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> on
visualizing nodes in “Artificial Neural Networks” and Alex
Mordvintsev—together with his wife—has since become an artist,
exhibiting in art fairs<a href="#fn8" class="footnote-ref" id="fnref8"
role="doc-noteref"><sup>8</sup></a>. What stayed was the notion that
“Artificial Intelligence” became some form of computational being,
different from the humans, who wrote the code and this in turn created
an immense interest for artists (and anyone, really) to create images,
texts and sounds based on emergent algorithms and large datasets.</p>
<h2 id="terms">Terms</h2>
<p>I have been deliberately using quotation marks around some of the
previous words, as I am critical of the language used when talking of AI
systems. In his book <em>The Artist in the Machine</em> Arthur I. Miller
comes to the conclusion that machines could in fact be seen as creative
and will be considered “artists, writers, and musicians in their own
right.”<a href="#fn9" class="footnote-ref" id="fnref9"
role="doc-noteref"><sup>9</sup></a> He is using a typical techno-utopian
argument that the technology is not quite there yet to <em>really</em>
be creative, but that it will change in the foreseeable future. At the
same time Miller is explaining in great detail the actual creative work
of Mordvintsev and his human experience of insomnia to come up<a
href="#fn10" class="footnote-ref" id="fnref10"
role="doc-noteref"><sup>10</sup></a> with the generative system for
DeepDream and how he shared it with his peers at Google. Alex
Mordvintsev was not collaborating with his computer, he created an
emergent algorithmic system with it. The software does not hang pictures
on exhibition walls, talk to gallerists and has no agency in the process
if it were to generate pictures or not. Peter Weibel has formulated it
bluntly: Artificial intelligence does not exist. But an ensemble of
machines, media, programs, algorithms, hardware, and software has
resulted in an extraordinarily large, diverse, and productive field of
research that is called AI.<a href="#fn11" class="footnote-ref"
id="fnref11" role="doc-noteref"><sup>11</sup></a> Where Arthur Miller is
feeding the narrative of humanoid robots with glowing blue brains<a
href="#fn12" class="footnote-ref" id="fnref12"
role="doc-noteref"><sup>12</sup></a>, most computer scientists in the
field of artificial intelligence today are working in the subdiscipline
of “Machine Learning”. In the last 10 years the terms have converged in
the media landscape, but where intelligence is rejected as a suitcase
word with a multiplicity of meanings, “Machine Learning” tries to define
the task more narrowly to some form of pattern recognition and
extrapolation of existing data. They are using a variety of
computational and statistical techniques to form abstract models for
domain specific problems. In these fields “Artificial Intelligence” is a
buzzword to convince governments and venture capitalists to fund
projects. The artist and researcher Francis Hunger has recently shared a
list of alternative terms in an attempt to dehumanize our language for
AI systems:</p>
<ol type="1">
<li>‘Artificial Intelligence’ =&gt; ’ Automated Pattern
Recognition’</li>
<li>‘Machine Learning’ =&gt; ‘Machine Conditioning’ OR ‘Automated
Classification’</li>
<li>‘Neural Network’ =&gt; ‘Weighted Network’</li>
<li>‘Deep Learning’ =&gt; ‘Deep Conditioning’</li>
<li>‘Neuron’ =&gt; ‘Weight’ or ‘Node’<br />
-<a href="https://twitter.com/databaseculture"><span class="citation"
data-cites="databaseculture">(<a href="#ref-databaseculture"
role="doc-biblioref"><strong>databaseculture?</strong></a>)</span></a>
<a href="#fn13" class="footnote-ref" id="fnref13"
role="doc-noteref"><sup>13</sup></a></li>
</ol>
<p>In an accompanying talk<a href="#fn14" class="footnote-ref"
id="fnref14" role="doc-noteref"><sup>14</sup></a> he explains the aim of
those terms is to invoke passivity, that we deal with machines and
humans set those machines in motion, even when in the end we form
“human-machine assemblages”. I like many of the proposed terms, even
though they are still closely associated to biological operations. The
conditioning of machines, for example, reminds me of Pavlov’s dog
experiments or B.F. Skinner’s box to modify pigeon behavior by
reinforcement or punishment.<a href="#fn15" class="footnote-ref"
id="fnref15" role="doc-noteref"><sup>15</sup></a> This analogy, though,
serves the purpose of creating an image of the machine as a system, that
can be controlled by changing the parameters of it’s virtual
environment. Therefor I will use “conditioning” and “weighted networks”,
where it is applicable. Using “Automated Pattern Recognition” instead of
“Artificial Intelligence”, however, becomes too narrow of a definition
and is counterintuitive to me, as AI serves exactly the function of
being ill-defined. Pei Wang has made a great effort in defining the
different strands of AI research and their working definitions.<a
href="#fn16" class="footnote-ref" id="fnref16"
role="doc-noteref"><sup>16</sup></a> He clusters them into Structure-AI
(recreating the human brain), Behavior-AI (recreating human actions),
Capability-AI (domain problem solving), Function-AI (developing
cognitive functions) and Principle-AI (finding underlying principles)
and comes up with his own working definition to where AI research should
be headed and how it can be unified. My initial goal for this thesis was
to completely avoid the term “Artificial Intelligence”, but as I have
already failed in that task and coming up with a less anthropomorphic
term does not seem feasible to convey the research in the field and it’s
media reception, therefore I will keep using the abbreviation AI.
Contrary to articles and sci-fi novels I will not use the
personification of “an AI”, but will rather talk of AI systems, meaning
emergent complex programs. Many of the systems today use statistical
modeling, yet nobody would ask the question, if statistics can be
creative, which is why I decided not to engage with philosophical
questions of consciousness and creativity. Instead I want to explore how
AI systems can include the human knowledge and work that goes into
building them, which is challenging the AI ideology of machine autonomy
and proposes human-centric goals, rather than building a machine as a
goal in itself.</p>
<h2 id="getting-started">Getting Started</h2>
<p>[[Revision Needed]]</p>
<p>I structured this thesis around 4 chapters, which combine historical,
computational and collective knowledge. Starting with a history of
talking machines from Wolfgang von Kempelen’s speech automatons to
digital assistants today, not forgetting that the first computers were
women doing calculation. The literal act of speaking to computers sets
the base of defining that we are actually talking through computers with
other humans. The second chapter deals with the building blocks for
complex statistical modeling in AI systems. To make such systems
possible, researchers need to create large datasets, often using
ethically questionable techniques aggregating data from internet users.
I will look closely into the StyleGAN dataset, as it serves a dual
purpose, because the model defined another turning point for artists to
generate synthetic media. The same is true for GPT-2, a model that can
generate coherent looking text based on large amounts of scraped
websites and books. In the third chapter I will explore the transformer
architecture and how artists are using it to make (non-)sense automatic
writing. Lastly I am revisiting collective experiences that I have been
organizing with other artists. I had the great pleasure to work with the
net culture initiative servus.at to organize the <em>Silicon Friend
Camp</em> in the Austrian mountains, where we invited 17 artists and
researchers for a week long retreat, focusing on human computer
conversations. The camp resulted in the exhibition <em>Camping with
Computers</em> and an online symposium on <em>Conversations with
Computers</em>.</p>
<hr />
<h1 id="talking-to-computers">Talking to Computers</h1>
<p>Female voices are spreading into living rooms and kitchens, on
laptops, smartphones and small round speakers. Networked with computing
machines around the world, they wait for their names to be spoken so
that they can send your command to a far off data center and analyze it.
If the transcription does not succeed algorithmically, sometimes a human
takes over the task of understanding. This person listens to snippets of
conversations for hours, anonymized<a href="#fn17" class="footnote-ref"
id="fnref17" role="doc-noteref"><sup>17</sup></a> of course, and
translates speech into text so that the machine recognizes the correct
patterns in the recording the next time it tries. If the transcription
is successful, the server sends the response back to the machine, which
plays it in the friendly tone of a synthetic female voice.</p>
<h2 id="human-automata">Human Automata</h2>
<p>Stories of artificial assistants already existed in ancient myths.
For example, the limping Hephaestus built himself servants made of gold
who assisted him in his work, could speak, and even had a mind of their
own.<a href="#fn18" class="footnote-ref" id="fnref18"
role="doc-noteref"><sup>18</sup></a> But it is only in the past few
centuries that we have created the technological means to seriously
address the construction of mechanical servants (at least virtually, as
robotics is still far behind). At the height of automaton design in the
18th century, Frenchman Jacques de Vaucanson invented a mechanical duck
that could not quack but appeared to have a functioning digestive tract.
The mechanical attraction toured European noble houses and let its
audience feed grains to it. However, what the duck excreted was a
prepared colored porridge that was in a hidden container. This principle
of mechanical trickery was also used by Vacaucanson’s contemporary
Wolfgang von Kempelen, who caused a sensation with his chess-playing
automaton in the shape of a turban-wearing Turk. The illusion that the
machine was acting autonomously was made possible by a small person
inside a hidden compartment who controlled the puppet arm of the table
via gears, levers, and pulleys. The hybrid machine is now the namesake
of Amazon’s <em>Mechanical Turk</em>, the largest platform for digital
micro-labor, which lists click jobs for pennies. Today, it continues to
perpetuate the illusion of autonomous machines with “artificial
intelligence” that is covertly enabled by an army of underpaid
workers.</p>
<p>Even though the chess playing Turk attracted attention, Wolfgang von
Kempelen’s scientific interest was in imitating human speech. He wrote
down his investigations into phonetics in the work <em>Mechanismus der
menschlichen Sprache</em> (Mechanism of Human Speech) and built an
apparatus with a bellows, rubber hose and a wooden nose with which it
was possible to produce basic phonemes.</p>
<p>Among those influenced by Kempelen’s book was a German tinkerer named
Joseph Faber, who demonstrated his own mechanically constructed speaking
machine in 1841. This attracted little interest in Germany and was
presented and improved four years later in the United States as the
Wonderful Talking Machine. This machine, as described by author David
Lindsay, consisted of a bizarre-looking talking head<a href="#fn19"
class="footnote-ref" id="fnref19" role="doc-noteref"><sup>19</sup></a>
that spoke in a strange ghostly tone while Faber manipulated it with
foot pedals and a keyboard<a href="#fn20" class="footnote-ref"
id="fnref20" role="doc-noteref"><sup>20</sup></a>. For the inventor, the
machine did not lead to the financial success he had hoped for, though
it was presented as the <em>Euphonia</em> in London, where it at least
delighted the father of telephone inventor Alexander Graham Bell and
served as the boy’s inspiration for his first talking machine.</p>
<p>For the exhibition Mensch-[in der]-Maschine at the ZKM (Center for
Art and Technology Karlsruhe), media artist Michael Markert built
<em>kII (Kempelen 2.0)</em>, an interactive installation in which
visitors can playfully control a speech synthesizer by moving, opening
and closing their hands.<a href="#fn21" class="footnote-ref"
id="fnref21" role="doc-noteref"><sup>21</sup></a> In doing so, he brings
Kempelen’s speech apparatus into the 21st century with the help of an
8-bit PIC microcontroller and sensor technology. Like Kempelen’s
apparatus it alienates the voice in such a way that it creates mostly
meaningless vocal sounds that enable new gesticulatory speech
interactions.</p>
<p>The development of electricity certainly made new human-machine
interactions possible. For example, the invention of the telephone and
radio allowed the human voice to be transmitted over long distances. To
optimize the transmission of speech, Bell Laboratories researched how to
digitize the voice, for which they developed the vocoder (voice
encoder). Demonstrated at the 1939 World’s Fair in New York, the
<em>Voder</em> omitted the speech input and transformation of the
vocoder and allowed electrical synthesis of the voice via a console with
15 keys and a foot pedal.<a href="#fn22" class="footnote-ref"
id="fnref22" role="doc-noteref"><sup>22</sup></a> The keyboard was
operated by specially trained women, and in a recording advertised as
the robot speaker, while it’s unclear if they mean the machine or the
woman, who is speaking through it.</p>
<p>Human computers were popular and necessary for war machines and
research purposes in the 1930s and 40s. Mostly it was women who prepared
mathematical tables, for example, for the use of ballistic projectiles.
With the advent of the first digital calculators, female mathematicians,
who were often denied higher scientific positions, were employed as
programmers for the new universal electric machines.<a href="#fn23"
class="footnote-ref" id="fnref23" role="doc-noteref"><sup>23</sup></a>
The 6 people who programmed the first universal computer ENIAC include
Betty Snyder Holberton, Jean Jennings Bartik, Kathleen McNulty Mauchly
Antonelli and Marlyn Wescoff Meltzer.<a href="#fn24"
class="footnote-ref" id="fnref24" role="doc-noteref"><sup>24</sup></a>
Initially, the (mechanical) computer was programmed with punched cards
and cables for specific operations. It soon became clear that
programming complex systems required an abstract semantic language, for
which reason the programming languages Fortran by John W. Backus, Lisp
by John McCarthy, and COBOL by Grace Hopper were invented in the 1950s.
The latter is strongly oriented to written English. Intended for
business applications, it was the first attempt to use natural language
for computer programming.</p>
<p>The second half of the 20th century saw the emergence of the myths
about computers that we are familiar with today. Stories of
anthropomorphic beings, like the board computer HAL9000 in Space Odyssey
or Samantha in the movie Her. In both films, the disembodied voices
become aware of their emotions and emancipate themselves from their
human programming. Artist Tillmann Ohm makes this clear in his work
<em>Reflections of HAL and Samantha</em><a href="#fn25"
class="footnote-ref" id="fnref25" role="doc-noteref"><sup>25</sup></a>
by having the two artificial beings engage in a dialogue, cutting their
original voice-overs together. While Samantha is convinced that the
overwhelming and sometimes hurtful process of her learning algorithm
improves the complexity of her emotions, HAL is consequentially
interpreting them as errors in human programming and analyses the
estimated malfunction.</p>
<h2 id="artificial-voices">Artificial Voices</h2>
<p>I want to explain the advancements in voice synthesis since Wolfgang
von Kempelen’s speaking machine on actual dolls for children. The toy
manufacturer Mattel released a doll with the name Chatty Cathy in 1959,
which was similarly popular to the companies other best seller Barbie.
Cathy’s trademark was a string, coming out of the back of it’s body,
that could be pulled to wind up the mechanism of a simple phonograph.
Like a record player it plays short strips with sentences like “I love
you” or “Tell me a story”. It was not the first toy using phonograph
records, but it’s success led to many pull string toys flooding the
market well into the 60s and 70s. But obviously before dolls and puppets
were designed to talk back to us, children and adults have been talking
to figurines and other inanimate objects for a long time.</p>
<p>The systems engineer and science writer George Zarkadakis traces back
the modern human mind to a pre-historic figurine of a lion-man
(Löwenmensch), carved out of ivory and found in the cave
Hohenstein-Stadel in southern Germany.<a href="#fn26"
class="footnote-ref" id="fnref26" role="doc-noteref"><sup>26</sup></a>
The figure dates back 40.000 to 35.000 years and clearly depicts a human
body with an animal head. Because the figurine was found in a cave next
to other carved objects, like beads and jewelry, researchers believe
that the cave was either a storage or a place for shamanistic rituals.
What fascinates Zarkadakis is that modern humans have existed 360.000
years before, but no evidence exists that they have been making art
objects or figurative depictions before. Only during the upper
paleolithic age it is assumed that we have created a general purpose
language, when the first cave paintings were drawn and figures like the
lion-man were carved. The evolutionary reasoning is that ‘theory of
mind’, the seemingly unique human trade of projecting our own inner
thoughts onto others to explain their behavior, gives the individual a
social benefit, making it more likely that they reproduce. Projecting
consciousness onto others did not stop inside of human groups, but
placing our own reasoning onto animals seemed also beneficial for
hunters coming up with elaborate strategies to take down larger animals
together. In animist belief systems everything contains a ‘soul’ or
‘spirit’: animals, plants, rocks, rivers, the weather and maybe even
words. George Zarkadakis argues that symbolic language predates the
modern mind, and is not only used to communicate, but ultimately
dictates how the world is represented in our own consciousness. Only
through language we place ‘souls’ and ‘spirits’ into everything around
us and art objects fulfill the function to mirror our own minds
physically and make connections with the world around us. It is no
surprise then that the original artist of the lion-man created a hybrid
of a human and a mountain lion, probably imagining themselves into the
figure of the predator and using the object as a symbol to
communicate.</p>
<p>In the beginning of “In Our Own Image” by George Zarkadakis, he tells
the story of how the fictional character Robby the Robot from the movie
<em>Forbidden Planet</em> impressed him as child so much, that he would
imagine the robot as a playmate that could walk, talk and obey orders.
When he later decided to study engineering and exchanged thoughts on
artificial intelligence with other researchers, they all seemed to
relate to the same sci-fi movies, books and stories.</p>
<p>Inspired by this revelation I asked the other participants at the
<em>Silicon Friend Camp</em> of what their first contact with a robot
was. My own memories brought me to a picture of myself, proudly holding
a plastic toy robot in my hand. The doll—or should I say action
figure—had a battery compartment, which was the power source for tiny
wheels underneath it’s feet and shiny light bulbs that it had for eyes.
It could not talk, but I clearly remember that it made noises that
resemble guns or a laser sounds. In the 80s and 90s the bleak vision of
boxy humanoid robots carrying guns was very prominent, next to movies
like <em>Terminator</em> and <em>RoboCop</em>, they fueled the male
fantasy for power and destruction.</p>
<p>The other first encounters were a little less militaristic, one
artist remembering a robot dog that he got as a present and was very
proud to show around to others. In the early 2000s digital pets with
integrated sensors became cheap enough for consumers. <em>Tekno the
Robotic Puppy</em><a href="#fn27" class="footnote-ref" id="fnref27"
role="doc-noteref"><sup>27</sup></a> by Manley Toy Quest was one of the
more affordable robodogs in the market and with light and proximity
sensors, buttons and even a microphone it was well equipped to learn
some ‘tricks’ by triggering the right combination of sensors. Another
artist was sharing an image of the <em>Tamagotchi</em> and while it was
not a robot, the LCD screen in an egg-like plastic shell displayed a
simulation of a pet. With 3 buttons children could interact with the
virtual pet and fulfill it’s needs for food, affection and sleep. The
anthropologist Pat Shipman proposes that animal connection can be
considered a human trait that has uniquely evolved in us, next to tool
making, symbolic language and the domestication of animals and plants.<a
href="#fn28" class="footnote-ref" id="fnref28"
role="doc-noteref"><sup>28</sup></a> Simulating animal connection by
creating simple programs on microchips was then the next logical step in
our drive to create symbols all the way down and fulfill the need of
children to take care of others.</p>
<p>Yet another popular toy that was shared by an artist in our group and
filled the same niche of robotic pets was <em>Furby</em>. This furry
ambiguous creature with big eyes was made to simulate language
acquisition and brings us back to the topic of how dolls learned to
speak. In the case of <em>Furby</em> the designers decided to create a
new vocal language called ‘furbish’. When the device was first started
it only repeated gibberish sentences and slowly over time would exchange
these words with the local language equivalents as it ages. The original
source code for one of the microchips was released in 2018<a
href="#fn29" class="footnote-ref" id="fnref29"
role="doc-noteref"><sup>29</sup></a> and even though I can not read
assembly code, it is thoroughly documented through comments. The game
logic of the furby.asm file is fairly simple, jumping between
subroutines depending on the state of the many sensors, triggers and
timers and occasionally using a pseudorandom number generator to mix
things up. The actual speech sounds were accessed from a memory unit and
send to a cheap Texas Instruments speech synthesizer chip. These chips
have been in use for some decades already, most prominently in another
toy called <em>Speak and Spell</em>, but also in watches, clocks and
translators. The synthesis uses a prerecorded audio signal that is then
compressed using linear predictive coding (LPC). This type of encoding
allowed the voice to be later changed in frequency, pitch and loudness.
But just like the <em>Vocoder</em>, it only digitized a voice and was
not able to construct new words or sentences. So the original
<em>Furby</em> from 1998 could only express a couple hundred words that
were recorded by a voice actor, which is again only a digital equivalent
to Chatty Cathy.</p>
<p>More powerful computers were already able to generate any kind of
text into somewhat intelligible speech. The Software Automatic Mouth
(S.A.M.) from 1982 was one of the first commercial products that used
rule based formant synthesis. Instead of using whole prerecorded words,
it uses an array of computer generated phonemes and places them together
to make up sentences. While this was often understandable, it was not
perfect. It sounds unnatural and robotic and was reliant on converting
text to speech in a probabilistic manner, often mispronouncing words.<a
href="#fn30" class="footnote-ref" id="fnref30"
role="doc-noteref"><sup>30</sup></a> S.A.M. did not need any special
hardware and could run on Apple, Atari and Commodore computers at the
time, even though that would often mean using every CPU cycle and
holding other programs. The software had it’s biggest moment at the
launch event of the Macintosh computer in 1984, greeting the audience
with a live demo in first person.<a href="#fn31" class="footnote-ref"
id="fnref31" role="doc-noteref"><sup>31</sup></a> Sam and it’s multiple
software variations became the universal voice of the computer. The
net.art pioneer Alexei Shulgin took the speech synthesis capabilities
similar to S.A.M. to the extreme with his cyberpunk rockband <em>386
DX</em>.<a href="#fn32" class="footnote-ref" id="fnref32"
role="doc-noteref"><sup>32</sup></a> The band is named after the Intel
chip inside of the old office computer, which is also the front singer
and instrumentalist. It uses MIDI and voice synthesis to cover popular
songs by The Doors, Nirvana or the Sex Pistols, traveling to festivals
around the world and sometimes performing on the streets. The artist
Alexei Shulgin takes the role of the operator, only pressing the play
button and creating the visuals on the screen during the live
performances. In my opinion this ironic take of making the microchip
into it’s own one-computer-band was a demonstration of how terrible and
reductive rock music becomes, when it was compressed into bits and
bytes. At the same time the novelty and humor of this process, generated
enough interest for <em>386 DX</em> to produce 2 CDs and distribute them
over music labels.</p>
<p>The strangely robotic voices that got associated with computers
improved over time and when Apple’s digital assistant Siri came out in
2011 they used a different process of concatenating audio samples
together. Before Apple even started working on the voice assistant, a
company called Scansoft auditioned hundreds of voice-over artists in
2005 and made them speak a multitude of random sentences. The idea of
how to use the random utterances for new outputs was quite simple: cut
the voice into smaller pieces of syllables, demisyllables, phonemes,
diaphones or triphones and put them back together for any word you need.
Doing this manually was—of course—very time consuming, so that a
programs were developed to analyze and categorize the speech dataset
based on their acoustic properties. Then, at runtime an algorithm
selects the best sequence to create the desired phrase. The results in
the early 2000s still contained a lot of error and needed a substantial
amount of tweaking, but the outcome was perceived as much more natural.
In 2006 the Scansoft merged with Nuance, another company working on
enterprise speech solutions, who presumably used the database to create
the voice for Siri and licensed it to Apple. The original voice-actor in
the US was Susan Bennet,<a href="#fn33" class="footnote-ref"
id="fnref33" role="doc-noteref"><sup>33</sup></a> who was oblivious of
the fact that suddenly, 6 years after recording, her voice appeared on
iPhones all over the country.</p>
<p>The technology to generate new sentences from previously recorded
voices drastically improved, when deep weighted networks started to be
used to analyze and select audio samples. One major achievement in
generating natural voices came in 2016 in the form of a model named
<em>WaveNet</em><a href="#fn34" class="footnote-ref" id="fnref34"
role="doc-noteref"><sup>34</sup></a> from a team of researchers at
Google’s DeepMind. They used convolutional node networks (CNN)<a
href="#fn35" class="footnote-ref" id="fnref35"
role="doc-noteref"><sup>35</sup></a> to perform statistical analysis on
raw audio files with 16000 samples per second. CNNs were previously used
to iterate over two dimensional pixel arrays for classification and
generation tasks in computer vision, because it allowed for fast
computation on graphics cards.<a href="#fn36" class="footnote-ref"
id="fnref36" role="doc-noteref"><sup>36</sup></a> Using convolutional
operations on temporal audio data meant that the researcher did not need
to convert it first into a compressed format using a vocoder and the
results were much more natural sounding than any previous text-to-speech
algorithm. Using a raw audio dataset also means that the system can find
patterns in any audio signal, including music. For demonstration the
researchers generated new samples from a classical piano dataset, which
sounds fascinating, but has more resemblance to free jazz than a
classical piano score. This also happens to speech, when the network is
conditioned without any text sequences it generates impressive human
babbling, where real words are interspersed with made-up word-like
sounds and breathing noises. When the network is trained on audio and
the transcription together, the system does not only generate a
consistent voice replica of a single human, but can also be used for
voice recognition task transforming utterances back to text.</p>
<p>Other companies experimented with weighted networks to better create
concatenative voice models. At the same year in 2016 Adobe showcased a
hypothetical feature for their audio software, where users can create
voice clones with a small 20-minute dataset and then use text to edit
spoken words in a sentence. In the presentation project <em>VoCo</em> at
the Adobe MAX conference, the developer Zeyu Jin used an example
sentence and exchanged the words in such a way that instead of “I kissed
my dogs and my wife”, the audio was manipulated to say “I kissed Jordan
three times” – referring Jordan Peele, the host of the conference. This
strange demonstration, that was supposed to be funny, got a lot of
attention online and created a debate around the ethical implications of
manipulated audio and voice cloning. Adobe never released <em>VoCo</em>
as a product, maybe because of the media backlash and fear from the
public, but other companies like <em>Lyrebird</em><a href="#fn37"
class="footnote-ref" id="fnref37" role="doc-noteref"><sup>37</sup></a>
filled the gap, trying to market their product for the use of editing
podcasts and voice-overs. However, they started their media campaign by
cloning the voices of Donald Trump and Barack Obama to “discuss” about
the startups product and praise it for their democratizing effort of
making it available to anyone.<a href="#fn38" class="footnote-ref"
id="fnref38" role="doc-noteref"><sup>38</sup></a> Together with
synthetic video, so called “Deepfakes” have earned a bad reputation for
being used in fraud<a href="#fn39" class="footnote-ref" id="fnref39"
role="doc-noteref"><sup>39</sup></a> and pornography<a href="#fn40"
class="footnote-ref" id="fnref40" role="doc-noteref"><sup>40</sup></a>.
At the same time artists are using the technique to create hypothetical
stories to mock politicians and other famous figures.<a href="#fn41"
class="footnote-ref" id="fnref41" role="doc-noteref"><sup>41</sup></a>
Zeyu Jin, the creator of <em>VoCo</em>, already proposed watermarking
synthetic voices and use software to detect if a voice is
fraudulent.</p>
<p>We can see that with the release of <em>WaveNet</em> and
<em>VoCo</em> in 2016 a new field in voice synthesis opened up. Hybrid
systems were created that used both concatenative and generative
techniques to categorize and interpolate between the tiny audio samples
of human speech.<a href="#fn42" class="footnote-ref" id="fnref42"
role="doc-noteref"><sup>42</sup></a> The first release of
<em>WaveNet</em> was computationally expensive, but already one year
later Google implemented a new version that works in real-time for their
voice assistant and has shown that a slim hybrid model can be used on
low-powered devices, such as smartphones.<a href="#fn43"
class="footnote-ref" id="fnref43" role="doc-noteref"><sup>43</sup></a>
<a href="#fn44" class="footnote-ref" id="fnref44"
role="doc-noteref"><sup>44</sup></a> Today, almost all digital
assistants use a synthetic voice that is generated with deep weighted
networks. Next to the feminine voice of Amazon’s Alexa, they also
provide the cloned voice of Samuel L. Jackson.</p>
<h2 id="conversational-agents">Conversational Agents</h2>
<p>For Amazon founder Jeff Bezos, the board computer in Star Trek was
the inspiration for investing in the cloud-based voice software Alexa.
The product was initially marketed as a networked speaker. The software
is now expanding to other items, including watches, smartphones,
jewelry, light bulbs and doorbells. The aggressive price war with
Amazon-connected products is partly to capture the connected home
market, but also to collect as much natural voice data as possible. Over
the past 10 years, deep weighted networks<a href="#fn45"
class="footnote-ref" id="fnref45" role="doc-noteref"><sup>45</sup></a>
have become popular for classification and pattern recognition tasks and
have especially made an impact in automated speech recognition. Before
weighted networks were used, the error rate of automated speech
recognition (ASR) went from 40% in the 1990s down to 15% in the 2000s,
which was still very high, but became practical to use. Back then the
user would have to feed the system with voice samples, so that it could
reliably turn utterances into text, but even then they would need to
speak like a robot to be understood. With a landmark paper<a
href="#fn46" class="footnote-ref" id="fnref46"
role="doc-noteref"><sup>46</sup></a> in 2012 by research groups at the
University of Toronto, Microsoft, Google and IBM it became that the
application of deep weighted networks outperforms any of the previous
techniques. In the same year Android released it’s new cloud based
speech recognition service<a href="#fn47" class="footnote-ref"
id="fnref47" role="doc-noteref"><sup>47</sup></a> and since then users
can speak into their phone naturally and be transcribed quite
accurately. For a long time though, the models needed gigabytes of data
in computer memory, to create accurate results, so that users need to
send their voice samples to large server farms for transcription. Only
in the past 2-3 years offline recognition became feasible through
specialized hardware and model optimization, trading performance for
robustness. Google’s claim that new speech recognition systems have an
error rate of less than 5% is only true for a specific—mostly white and
male—audience speaking American English and drops abruptly with a more
diverse distribution of dialects, age and gender.<a href="#fn48"
class="footnote-ref" id="fnref48"
role="doc-noteref"><sup>48</sup></a></p>
<p>In order for deep ASR systems to perform well they need a massive
amount of input data. A paper from the Chinese tech giant Baidu proposes
12000 hours of speech to create a robust model.<a href="#fn49"
class="footnote-ref" id="fnref49" role="doc-noteref"><sup>49</sup></a>bA
global internal database of speech recordings enables the company to
improve its speech recognition. The intrusion into privacy is immense
and has already been used by U.S. law enforcement agencies as evidence
in a court case.<a href="#fn50" class="footnote-ref" id="fnref50"
role="doc-noteref"><sup>50</sup></a> But there is another, more ethical,
way than stealing people’s voices: The Mozilla Foundation’s
<em>Commonvoice</em> project relies on people voluntarily recording
their voices for computer models, and the resulting speech recognition
and synthesis software can be offered with an open source license.<a
href="#fn51" class="footnote-ref" id="fnref51"
role="doc-noteref"><sup>51</sup></a> Common speech and text datasets
already have been publicly available from projects like VoxForge that
uses the same crowdsourced techniques as Mozilla or LibriSpeech, which
essentially scraped and aligned public domain audiobooks.<a href="#fn52"
class="footnote-ref" id="fnref52" role="doc-noteref"><sup>52</sup></a>
Other common non-free resources come from the Linguistic Data
Consortium, who, for example, created <em>Switchboard</em>, a corpus of
conversations between two US citizens randomly connected and recorded
over the telephone line. Listening to the audio sample provided on the
website, where a man talks to a woman about his interest in gardening
creates oddly voyeuristic feelings.<a href="#fn53" class="footnote-ref"
id="fnref53" role="doc-noteref"><sup>53</sup></a></p>
<p>Artist Lauren Lee McCarthy plays with these tensions between intimacy
and privacy, convenience and agency. In her projects LAUREN and SOMEONE,
she installs connected devices into volunteers homes and either acts as
a control system herself or lets others remotely monitor the volunteers
and control the devices in their homes. This creates an interesting
tension, when the person knows that there is an actual human listening
and watching from afar. At the same time the artist and performers find
themselves in a position of a helpful voyeur.</p>
<p>However, the role of human labor behind voice assistants is not just
about executing and understanding commands. People tend to interpret
voices and categorize them according to age, gender and social status.
Companies take advantage of this and design their voice software
according to certain identity schemes, which are provided with a
history, hobbies and preferences. It is precisely this illusion that
excites users and makes the product interesting. In a UNESCO think piece
titled <em>I’d blush if I could</em><a href="#fn54" class="footnote-ref"
id="fnref54" role="doc-noteref"><sup>54</sup></a>, they explore harmful
gender biases associated with digital assistants. The voice assistants
of major tech companies are scripted by default as female personas with
smart, humble, and sometimes funny personalities. The teams working on
voice assistants try to avoid this aspect, because Apple, Microsoft and
Google ask their employees to refer to their headless voices as “it” and
when the persona get’s asked the question directly winds out with a
joke. Only Alexa answers what is obvious in the default design of all of
them with “I’m female in character”. By being submissive, they thus
support a patriarchal image of women that we already know, in a
historical context, from human computers and other secretarial roles.
The paper calls for women to be more empowered and involved in IT. It
calls for AI software to avoid gender attributes whenever possible, and
for AI assistants to take a clear stand against sexist behaviors.</p>
<p>Researcher and artist Nadine Lessio creates useless voice assistants
to critique the current corporate agenda of productivity, efficiency,
and consumption. She does this by using the programming interfaces
provided to make apps for corporate voice assistants. For example, she
explores the concept of a depressed home assistant with <em>SAD Home
(Depressed Alexa 1.0)</em><a href="#fn55" class="footnote-ref"
id="fnref55" role="doc-noteref"><sup>55</sup></a>, an Alexa hack that
grants users their wishes depending on the weather and other mood
factors, sometimes it simply turns itself off.</p>
<p>This scripted denial of a capitalist logic ironically uses the same
technique as the company behind it. Voice assistants are carefully
crafted by a team of creative professionals working in the field of
“conversation design”. In the book <em>Talk to me</em><a href="#fn56"
class="footnote-ref" id="fnref56" role="doc-noteref"><sup>56</sup></a>
the Author James Vlahos describes that many people in the field are far
away from computer science and more commonly had careers in the liberal
arts. The teams are made up of authors, playwrights, comedians, actors
as well as anthropologists, psychologists, and philosophers who imagine
the personality of the AI persona that should represent the brand. To
create the character, they have to come up with all possible questions
and create various answers for each of them. Vahos recalls asking
Microsoft’s Cortana “Where do you come from?” and the female voice
replies “I was made by minds across the planet”.<a href="#fn57"
class="footnote-ref" id="fnref57" role="doc-noteref"><sup>57</sup></a>
And even though the designers decided to use the first person “I”, they
really are talking of themselves carefully crafting the answers played
back by the loudspeakers around the world.</p>
<p>The tedious process of mapping out all questions and creating answers
for them is mostly done through creative writing, but also utilizes
careful statistical analysis of the questions users send to the
cloud.</p>
<p>It is interesting how the term AI is used in the context of voice
computer interfaces, because there is nothing “smart” about it, just a
winding flowchart of if-else conditions. What might be clever is the
nefarious way of how companies trick people into the belief of computer
personalities, encouraging people to interact with the device like
children playing with dolls. And when a company like Microsoft has
experimented with a more sophisticated chatbot, like Tay<a href="#fn58"
class="footnote-ref" id="fnref58" role="doc-noteref"><sup>58</sup></a>,
it started to repeat the racist and misogynistic slurs of twitter users
and consequently has to be heavily filtered. But more on this in a later
chapter on stochastic text generation.</p>
<h2 id="leewa">Leewa</h2>
<p>The artist duo <em>Ekheo</em> consisting of Aude Langlois and Belinda
Sykora explore the crosslines of musical, visual and sonic art
performances. They met at the Sound Studies program at the Universität
der Künste in Berlin, where they created musical improvisations and
theoretical frameworks on the voice and music. They started their
research on voice, gender and identity in relation to AI systems in 2019
and held talks at the MUK<a href="#fn59" class="footnote-ref"
id="fnref59" role="doc-noteref"><sup>59</sup></a> and other institutions
to share their insights. At the <em>Silicon Friend Camp</em> they wanted
to extend their already existing improvisation and research practice by
integrating a form of embodied AI. But the main focus was the voice
itself: How is gender perceived through it? How do we navigate the
tension between what is considered an artificial voice and an authentic
voice? To tackle these issues during the camp they wanted to create and
record soundscapes based on vocal improvisation and use AI systems in
the form of synthetic voices, generative composition tools, robotic
elements and so on. So we prepared the attic of the house with a small
sound studio that they could work in and during the week they found
inspiration in using a multitude of AI systems and human collaborators
that they used to create an improvisation and finally a 6-minute video
and audio composition. To come up with the identity of their AI partner,
they used the <em>Oracle</em><a href="#fn60" class="footnote-ref"
id="fnref60" role="doc-noteref"><sup>60</sup></a> to ask questions of
their name, gender, favorite instrument etc. which reads like this:</p>
<blockquote>
<p>You: how old is she?<br />
Oracle: Today she will be 11.<br />
You: what is her favourite musical instrument?<br />
Oracle: her greatest pleasure is to whisper to the sounds of trumpets
and the beating of tympani<br />
You: is she happy?<br />
Oracle: It is a mystery and a great sadness.<br />
You: What is Leewa’s surname?<br />
Oracle: It was suggested that her human’s name was
/u/Seattle_Celtics.<br />
You: does she like to sing?<br />
Oracle: I have no voice<br />
You: No I mean Leewa<br />
Oracle: Query : Have you ever cheated?<br />
You: Does Leewa have a voice?<br />
Oracle: Yoow!<br />
You: what is Leewa’s favourite poem?<br />
Oracle: Leaves of Grass<br />
You: What is Leewa’s favourite book?<br />
Oracle: Leewa does joke around and loves to give sarcastic responses.
She also loves to read books, especially horror novels.<br />
You: What gender does Leewa have?<br />
Oracle: She is female.<br />
You: which music does Leewa like?<br />
Oracle: Leewa likes to sing and most of all Leewa likes to dance.<br />
You: What is art for her?<br />
Oracle: art is the process of iterating through consciousness
generation.<br />
</p>
</blockquote>
<p>This served as a base for the identity of <em>Leewa</em> and inspired
further experiments using a whispering voice and 3D printing a female
doll bust. To recreate the voice I searched for tools to do voice
synthesis and found an open source toolbox from Corentin Jemine that
claimed to do Real-Time Voice Cloning from a few seconds of audio.<a
href="#fn61" class="footnote-ref" id="fnref61"
role="doc-noteref"><sup>61</sup></a> The method is essentially based on
a previous paper from Google engineers: <em>Transfer Learning from
Speaker Verification to Multispeaker Text-To-Speech Synthesis</em><a
href="#fn62" class="footnote-ref" id="fnref62"
role="doc-noteref"><sup>62</sup></a> (SV2TTS), which itself uses a
pipeline of a speaker-encoder network (GE2E) to represent different
voices in different positions of the latent space, a text to waveform
synthesizer (Tacotron) and a vocoder to generate sound from these
waveforms (WaveNet).<a href="#fn63" class="footnote-ref" id="fnref63"
role="doc-noteref"><sup>63</sup></a> As described previously combining
different encoding and generator systems into a larger framework is
commonly used and is commercially used in Google’s Assistant and their
cloud services for other companies.</p>
<p>Thanks to the efforts of Corentin Jemine and the other contributors,
who built a user interface to explore these tools, we were able to input
our own voices and generate new samples with pre-conditioned voice
models. Unfortunately though the results were not as expected, by
inputting only a small number of samples the output was not picking up
on or voice characteristics and the results were too noisy and of
low-quality. Instead we tried out the cloud service from resemble AI<a
href="#fn64" class="footnote-ref" id="fnref64"
role="doc-noteref"><sup>64</sup></a> that is mentioned in the code
repository and where some of the maintainers work at. We decided to try
and make a collective voice of the camp participants, so that every one
recorded 10 phrases.</p>
<p>After I pressed “train” on the web interface and waited for some time
for the process to finish, I got an e-mail telling me that the “Resemble
score” of our voice was 53%. So the transfer learning process was not
quite successful, but the resulting voice was very surprising. I
expected that the algorithm would try to average our voices into
something that fits in between all data points. Instead it created
clones of a voice for some phonemes, but not for others, so that a
sentence randomly interpolates between different speakers. It also seems
that, at least in this run, the characteristics of male voices are more
prominent than female voices, but it also has a strange “artificial”
nature to it, where the pitch and speed of the voice is constantly
changing.</p>
<p>Now that we used a commercial product, we were limited to their API,
but because of our limited time we only used their web interface, which
made using the voice for real-time performances impossible. Instead
<em>Ekheo</em> created some voice samples that they played live during
the final performance. For this, they also recorded real voices from
participants at the camp and intertwined with synthetic drones and other
instruments. The visuals were created from a live video feed of the
puppet, that was modified by Naoto Hieda using the live coding shader
language hydra<a href="#fn65" class="footnote-ref" id="fnref65"
role="doc-noteref"><sup>65</sup></a> The puppet itself could move it’s
head, which was remotely controlled by So Kanno via a servomotor.</p>
<p>In my opinion the example of <em>Leewa</em> illustrates the
collective efforts that go into building “an AI” quite nicely: it shows
the process of using open source and proprietary code developed by
people reliant on previous research, the creation of collective datasets
through the recording of our voices and finally the human performers
hiding behind screens. At the same time <em>Leewa</em> is a
self-referential project that makes the collective experience throughout
the camp audible, by using our synthetic voice, the interaction with the
<em>Oracle</em> and snippets of anecdotes from the camp.</p>
<hr />
<div id="refs" class="references csl-bib-body hanging-indent"
role="doc-bibliography">
<div id="ref-amodeiDeepSpeechEndtoEnd2015" class="csl-entry"
role="doc-biblioentry">
Amodei, Dario, Rishita Anubhai, Eric Battenberg, Carl Case, Jared
Casper, Bryan Catanzaro, Jingdong Chen, et al. 2015. <span>“Deep
<span>Speech</span> 2: <span class="nocase">End-to-End Speech
Recognition</span> in <span>English</span> and
<span>Mandarin</span>.”</span> December 8, 2015. <a
href="http://arxiv.org/abs/1512.02595">http://arxiv.org/abs/1512.02595</a>.
</div>
<div id="ref-CloneSyntheticAI" class="csl-entry" role="doc-biblioentry">
<span>“Clone <span>Synthetic AI Voices</span> with <span>Neural
Text</span> to <span>Speech</span>.”</span> n.d. <span>Resemble
AI</span>. Accessed January 26, 2022. <a
href="https://www.resemble.ai/">https://www.resemble.ai/</a>.
</div>
<div id="ref-coleThisHorrifyingApp2019" class="csl-entry"
role="doc-biblioentry">
Cole, Samantha. 2019. <span>“This <span>Horrifying App Undresses</span>
a <span>Photo</span> of <span>Any Woman With</span> a <span>Single
Click</span>.”</span> <span>Vice</span>. June 26, 2019. <a
href="https://www.vice.com/en/article/kzm59x/deepnude-app-creates-fake-nudes-of-any-woman">https://www.vice.com/en/article/kzm59x/deepnude-app-creates-fake-nudes-of-any-woman</a>.
</div>
<div id="ref-GENDERDIVERSITYGastvortrag" class="csl-entry"
role="doc-biblioentry">
<span>“GENDER &amp; DIVERSITY: Gastvortrag von Ekheo - Musik und Kunst
Privatuniversität der Stadt Wien.”</span> n.d. Accessed January 24,
2022. <a
href="https://muk.ac.at/veranstaltung/gender-diversity-gastvortrag-von-ekheo.html">https://muk.ac.at/veranstaltung/gender-diversity-gastvortrag-von-ekheo.html</a>.
</div>
<div id="ref-jemineMasterThesisAutomatic" class="csl-entry"
role="doc-biblioentry">
Jemine, C. n.d. <span>“Master Thesis : <span>Automatic Multispeaker
Voice Cloning</span>,”</span> 38.
</div>
<div id="ref-jemineRealTimeVoiceCloning2022" class="csl-entry"
role="doc-biblioentry">
Jemine, Corentin. (2019) 2022. <em>Real-<span>Time Voice
Cloning</span></em>. <a
href="https://github.com/CorentinJ/Real-Time-Voice-Cloning">https://github.com/CorentinJ/Real-Time-Voice-Cloning</a>.
</div>
<div id="ref-jiaTransferLearningSpeaker2019" class="csl-entry"
role="doc-biblioentry">
Jia, Ye, Yu Zhang, Ron J. Weiss, Quan Wang, Jonathan Shen, Fei Ren,
Zhifeng Chen, et al. 2019. <span>“Transfer <span>Learning</span> from
<span>Speaker Verification</span> to <span>Multispeaker Text-To-Speech
Synthesis</span>.”</span> January 2, 2019. <a
href="http://arxiv.org/abs/1806.04558">http://arxiv.org/abs/1806.04558</a>.
</div>
<div id="ref-krizhevskyImageNetClassificationDeep2017" class="csl-entry"
role="doc-biblioentry">
Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. 2017.
<span>“<span>ImageNet</span> Classification with Deep Convolutional
Neural Networks.”</span> <em>Commun. ACM</em> 60 (6): 84–90. <a
href="https://doi.org/10.1145/3065386">https://doi.org/10.1145/3065386</a>.
</div>
<div id="ref-lukoseTextSpeechSynthesizerFormant2017" class="csl-entry"
role="doc-biblioentry">
Lukose, Sneha, and Savitha S Upadhya. 2017. <span>“Text to <span>Speech
Synthesizer-Formant Synthesis</span>,”</span> 4.
</div>
<div id="ref-oordWaveNetGenerativeModel2016" class="csl-entry"
role="doc-biblioentry">
Oord, Aaron van den, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol
Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray
Kavukcuoglu. 2016. <span>“<span>WaveNet</span>: <span>A Generative
Model</span> for <span>Raw Audio</span>.”</span> September 19, 2016. <a
href="http://arxiv.org/abs/1609.03499">http://arxiv.org/abs/1609.03499</a>.
</div>
<div id="ref-panayotovLibrispeechASRCorpus2015" class="csl-entry"
role="doc-biblioentry">
Panayotov, Vassil, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur.
2015. <span>“Librispeech: <span>An ASR</span> Corpus Based on Public
Domain Audio Books.”</span> In <em>2015 <span>IEEE International
Conference</span> on <span>Acoustics</span>, <span>Speech</span> and
<span>Signal Processing</span> (<span>ICASSP</span>)</em>, 5206–10.
<span>South Brisbane, Queensland, Australia</span>: <span>IEEE</span>.
<a
href="https://doi.org/10.1109/ICASSP.2015.7178964">https://doi.org/10.1109/ICASSP.2015.7178964</a>.
</div>
<div id="ref-pierceHowAppleFinally" class="csl-entry"
role="doc-biblioentry">
Pierce, David. n.d. <span>“How <span>Apple Finally Made Siri Sound More
Human</span>.”</span> <em>Wired</em>. Accessed January 23, 2022. <a
href="https://www.wired.com/story/how-apple-finally-made-siri-sound-more-human/">https://www.wired.com/story/how-apple-finally-made-siri-sound-more-human/</a>.
</div>
<div id="ref-PoliticiansDiscussingLyrebird" class="csl-entry"
role="doc-biblioentry">
<em>Politicians discussing about Lyrebird</em>. n.d. Accessed January
24, 2022. <a
href="https://soundcloud.com/user-535691776/dialog">https://soundcloud.com/user-535691776/dialog</a>.
</div>
<div id="ref-shipmanAnimalConnectionHuman2010" class="csl-entry"
role="doc-biblioentry">
Shipman, Pat. 2010. <span>“The <span>Animal Connection</span> and
<span>Human Evolution</span>.”</span> <em>Current Anthropology</em> 51
(4): 519–38. <a
href="https://doi.org/10.1086/653816">https://doi.org/10.1086/653816</a>.
</div>
<div id="ref-stuppFraudstersUsedAI2019" class="csl-entry"
role="doc-biblioentry">
Stupp, Catherine. 2019. <span>“Fraudsters <span>Used AI</span> to
<span>Mimic CEO</span>’s <span>Voice</span> in <span>Unusual Cybercrime
Case</span>.”</span> <em>Wall Street Journal: WSJ Pro</em>, August 30,
2019. <a
href="https://www.wsj.com/articles/fraudsters-use-ai-to-mimic-ceos-voice-in-unusual-cybercrime-case-11567157402">https://www.wsj.com/articles/fraudsters-use-ai-to-mimic-ceos-voice-in-unusual-cybercrime-case-11567157402</a>.
</div>
<div id="ref-wanGeneralizedEndtoEndLoss2020" class="csl-entry"
role="doc-biblioentry">
Wan, Li, Quan Wang, Alan Papir, and Ignacio Lopez Moreno. 2020.
<span>“Generalized <span class="nocase">End-to-End Loss</span> for
<span>Speaker Verification</span>.”</span> November 9, 2020. <a
href="http://arxiv.org/abs/1710.10467">http://arxiv.org/abs/1710.10467</a>.
</div>
<div id="ref-wangTacotronEndtoEndSpeech2017" class="csl-entry"
role="doc-biblioentry">
Wang, Yuxuan, R. J. Skerry-Ryan, Daisy Stanton, Yonghui Wu, Ron J.
Weiss, Navdeep Jaitly, Zongheng Yang, et al. 2017. <span>“Tacotron:
<span class="nocase">Towards End-to-End Speech Synthesis</span>.”</span>
April 6, 2017. <a
href="http://arxiv.org/abs/1703.10135">http://arxiv.org/abs/1703.10135</a>.
</div>
<div id="ref-WaveNetLaunchesGoogle" class="csl-entry"
role="doc-biblioentry">
<span>“WaveNet launches in the Google Assistant.”</span> n.d.
<span>Deepmind</span>. Accessed January 24, 2022. <a
href="https://deepmind.com/blog/article/wavenet-launches-google-assistant">https://deepmind.com/blog/article/wavenet-launches-google-assistant</a>.
</div>
<div id="ref-zarkadakesOurOwnImage2016" class="csl-entry"
role="doc-biblioentry">
Zarkadakēs, Giōrgos. 2016. <em>In Our Own Image: Savior or Destroyer?:
The History and Future of Artificial Intelligence</em>. First Pegasus
books hardcover edition. <span>New York, NY</span>: <span>Pegasus Books
LLC</span>.
</div>
</div>
<section class="footnotes footnotes-end-of-document"
role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>Original Google Blog post. Link:<a
href="https://ai.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html">https://ai.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html</a><a
href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>The initial classifier was
conditioned on ImageNet, which contains many images of dog breeds.
Therefore DeepDream is biased to generate textures of dog faces.<a
href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p>The idea of convolutional neural
networks dates back to the 1950s and 60s to the works of David H. Hubel
and Torsten N. Wiesel, who recognized receptive fields in the visual
cortex of monkey brains. Ian Goodfellow calls the technique “Deep
Learning”, because the .<a href="#fnref3" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn4" role="doc-endnote"><p>About ImageNet. Official Website.
Link:<a
href="https://image-net.org/about.php">https://image-net.org/about.php</a><a
href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5" role="doc-endnote"><p>Krizhevsky 2014<a href="#fnref5"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6" role="doc-endnote"><p>Mike Tyka has compiled a list of
projects, released within a month of publishing the source code<a
href="http://mtyka.github.io/code/2015/07/21/one-month-after-deepdream.html">http://mtyka.github.io/code/2015/07/21/one-month-after-deepdream.html</a><a
href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7" role="doc-endnote"><p>A collection of tools and papers for
feature visualization is collected at the github tensorflow/lucid
repository. Note that Chris Olah and Alex Mordvintsev are contributors.
Even more examples and explanations are collected in the interpretable
machine learning book by the statistician Christoph Molnar, who also
notes that “Feature visualizations give unique insight into the working
of neural networks”, but the “visualizations can convey the illusion
that we understand what the neural network is doing”.<a
href="https://christophm.github.io/interpretable-ml-book/cnn-features.html">https://christophm.github.io/interpretable-ml-book/cnn-features.html</a><a
href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8" role="doc-endnote"><p>Interview with Alexander Mordvintsev
by artnome. Link:<a
href="https://www.artnome.com/news/2018/12/30/deepdream-creator-unveils-very-first-images-after-three-years">https://www.artnome.com/news/2018/12/30/deepdream-creator-unveils-very-first-images-after-three-years</a><a
href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9" role="doc-endnote"><p>Miller 2019, 122.<a href="#fnref9"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10" role="doc-endnote"><p>Mordvintsev himself was inspired by
a previous paper exploring the generative potential of CNNs from Karen
Simonyan: Simonyan, Vedaldi, and Zisserman 2014.<a href="#fnref10"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11" role="doc-endnote"><p>Translated from the article ‘AAA -
Art, Algorithmen, Artificial Intelligence’ at Kunstforum Bd. 278 Link:<a
href="https://www.kunstforum.de/artikel/aaa/">https://www.kunstforum.de/artikel/aaa/</a><a
href="#fnref11" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn12" role="doc-endnote"><p>Referring to typical AI stock
images. Counter imagery is currently built by<a
href="https://betterimagesofai.org/">https://betterimagesofai.org/</a><a
href="#fnref12" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn13" role="doc-endnote"><p>Twitter post by <span
class="citation" data-cites="databasecultures">(<a
href="#ref-databasecultures"
role="doc-biblioref"><strong>databasecultures?</strong></a>)</span>.
Link:<a
href="https://twitter.com/databaseculture/status/1413462059291975680">https://twitter.com/databaseculture/status/1413462059291975680</a><a
href="#fnref13" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn14" role="doc-endnote"><p>Unhype AI, Link:<a
href="http://www.irmielin.org/unhype-ai/">http://www.irmielin.org/unhype-ai/</a><a
href="#fnref14" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn15" role="doc-endnote"><p>In behaviorist psychology Ivan
Pavlov’s experiments with dogs is known as classical conditioning and
B.F. Skinner who experimented on rats and pigeons using lever machines
is called operant conditioning.<a href="#fnref15" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn16" role="doc-endnote"><p>Wang 2019.<a href="#fnref16"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn17" role="doc-endnote"><p>Metadata and references to the
account are deleted before the review. Complete anonymization of the
voice is not performed.<a href="#fnref17" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn18" role="doc-endnote"><p>“Hephaestus then limped out of the
door’; and maidens supported the ruler, golden ones, like living ones,
with youthful charming education: These have understanding in the
breast, and speaking voice, Have strength, and also learned art work
from the gods.” (Homer, Iliad 18, 417-420; link:<a
href="https://www.projekt-gutenberg.org/homer/ilias/ilias183.html">https://www.projekt-gutenberg.org/homer/ilias/ilias183.html</a>)<a
href="#fnref18" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn19" role="doc-endnote"><p>Fabers machine was first presented
with a female mask in the USA and later in London under oriental motif
wearing a turban.<a href="#fnref19" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn20" role="doc-endnote"><p>D.Lindsay, 1997. Link:<a
href="https://www.inventionandtech.com/content/talking-head-1">https://www.inventionandtech.com/content/talking-head-1</a><a
href="#fnref20" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn21" role="doc-endnote"><p>Serexhe et al. 2007, p. 74. And
project description online. Link:<a
href="http://www.audiocommander.de">http://www.audiocommander.de</a><a
href="#fnref21" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn22" role="doc-endnote"><p>A video recording of the Voder
demonstration can be found in the AP Archive under Human Voice
Machine.<a
href="http://www.aparchive.com/metadata/youtube/5f098b1f3e8b4d09b8de30dcecc42f99">http://www.aparchive.com/metadata/youtube/5f098b1f3e8b4d09b8de30dcecc42f99</a><a
href="#fnref22" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn23" role="doc-endnote"><p>A focus on black women who worked as
computers for NACA (NASA’s predecessor) can be seen in the film Hidden
Figures, 2016.<a href="#fnref23" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn24" role="doc-endnote"><p>The story of the ENIAC programmers
is told in the documentary The Computers, 2016.<a href="#fnref24"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn25" role="doc-endnote"><p>Project description of Reflections
of HAL and Samantha online. Link:<a
href="https://tillmannohm.com/reflections-of-hal-and-samantha/">https://tillmannohm.com/reflections-of-hal-and-samantha/</a><a
href="#fnref25" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn26" role="doc-endnote"><p><span class="citation"
data-cites="zarkadakesOurOwnImage2016">Zarkadakēs (<a
href="#ref-zarkadakesOurOwnImage2016"
role="doc-biblioref">2016</a>)</span><a href="#fnref26"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn27" role="doc-endnote"><p>The robot puppy was featured on the
TIME magazine cover with the title “Tech comes to Toyland”. Link:
http://content.time.com/time/covers/europe/0,16641,20001211,00.html<a
href="#fnref27" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn28" role="doc-endnote"><p><span class="citation"
data-cites="shipmanAnimalConnectionHuman2010">(<a
href="#ref-shipmanAnimalConnectionHuman2010"
role="doc-biblioref">Shipman 2010</a>)</span><a href="#fnref28"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn29" role="doc-endnote"><p>Sean Riddle requested the Furby
source code from the US patent office and uploaded it on his webpage.
Link: http://www.seanriddle.com/furbysource.pdf<a href="#fnref29"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn30" role="doc-endnote"><p><span class="citation"
data-cites="lukoseTextSpeechSynthesizerFormant2017">(<a
href="#ref-lukoseTextSpeechSynthesizerFormant2017"
role="doc-biblioref">Lukose and Upadhya 2017</a>)</span><a
href="#fnref30" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn31" role="doc-endnote"><p>To make the demo work, engineers had
to use a prototype Mac that was more powerful than the retail version.
It said: “Hello, I am Macintosh. It sure is great to get out of that
bag! Unaccustomed as I am to public speaking, I’d like to share with you
a maxim I thought of the first time I met an IBM mainframe: Never trust
a computer that you can’t lift! Obviously, I can talk, but right now I’d
like to sit back and listen. So it is with considerable pride that I
introduce a man who has been like a father to me… Steve Jobs!”. Link:
https://www.folklore.org/StoryView.py?project=Macintosh&amp;story=Intro_Demo.txt<a
href="#fnref31" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn32" role="doc-endnote"><p>The songs of 386 DX are available on
Alexei Shulgin’s website. Link: http://www.easylife.org/386dx/<a
href="#fnref32" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn33" role="doc-endnote"><p>Interview by the Guardian with
multiple voice actors for Siri. Link:
https://www.theguardian.com/technology/2015/aug/12/siri-real-voices-apple-ios-assistant-jon-briggs-susan-bennett-karen-jacobsen<a
href="#fnref33" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn34" role="doc-endnote"><p><span class="citation"
data-cites="oordWaveNetGenerativeModel2016">(<a
href="#ref-oordWaveNetGenerativeModel2016" role="doc-biblioref">Oord et
al. 2016</a>)</span><a href="#fnref34" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn35" role="doc-endnote"><p>Usually referred to as Convolutional
<strong>Neural</strong> Networks<a href="#fnref35" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn36" role="doc-endnote"><p>CNNs are used in AlexNet for
example. <span class="citation"
data-cites="krizhevskyImageNetClassificationDeep2017">(<a
href="#ref-krizhevskyImageNetClassificationDeep2017"
role="doc-biblioref">Krizhevsky, Sutskever, and Hinton
2017</a>)</span><a href="#fnref36" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn37" role="doc-endnote"><p>Lyrebird changed their name to
Descript. Link: https://www.descript.com/<a href="#fnref37"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn38" role="doc-endnote"><p><span class="citation"
data-cites="PoliticiansDiscussingLyrebird">(<a
href="#ref-PoliticiansDiscussingLyrebird"
role="doc-biblioref"><em>Politicians discussing about Lyrebird</em>
n.d.</a>)</span><a href="#fnref38" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn39" role="doc-endnote"><p>Apparently fraudsters were able to
use a synthetic voice to instruct a bank transfer of 220.000€. <span
class="citation" data-cites="stuppFraudstersUsedAI2019">(<a
href="#ref-stuppFraudstersUsedAI2019" role="doc-biblioref">Stupp
2019</a>)</span><a href="#fnref39" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn40" role="doc-endnote"><p>In 2019 an app with the name
DeepNude got public attention due to a Vice article and was taken down
by the author shortly after. Link: <span class="citation"
data-cites="coleThisHorrifyingApp2019">(<a
href="#ref-coleThisHorrifyingApp2019" role="doc-biblioref">Cole
2019</a>)</span><a href="#fnref40" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn41" role="doc-endnote"><p>Examples of deepfake political
comedy can be found on the YouTube Channel of Speaking of AI. Link:
https://www.youtube.com/channel/UCID5qusrF32kSj-oSGq3rJg<a
href="#fnref41" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn42" role="doc-endnote"><p><span class="citation"
data-cites="pierceHowAppleFinally">(<a href="#ref-pierceHowAppleFinally"
role="doc-biblioref">Pierce n.d.</a>)</span><a href="#fnref42"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn43" role="doc-endnote"><p><span class="citation"
data-cites="WaveNetLaunchesGoogle">(<a href="#ref-WaveNetLaunchesGoogle"
role="doc-biblioref"><span>“WaveNet launches in the Google
Assistant”</span> n.d.</a>)</span><a href="#fnref43"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn44" role="doc-endnote"><p><span class="citation"
data-cites="wangTacotronEndtoEndSpeech2017">(<a
href="#ref-wangTacotronEndtoEndSpeech2017" role="doc-biblioref">Wang et
al. 2017</a>)</span><a href="#fnref44" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn45" role="doc-endnote"><p>As described in the introduction I
use “weighted” instead of “neural”.<a href="#fnref45"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn46" role="doc-endnote"><p>Hinton et al. 2012<a href="#fnref46"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn47" role="doc-endnote"><p>Google AI Blog article about “Speech
Recognition and Deep Learning”. Link:
https://ai.googleblog.com/2012/08/speech-recognition-and-deep-learning.html<a
href="#fnref47" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn48" role="doc-endnote"><p>Tatman 2017, did a study on Gender
and Dialect Bias in YouTube’s Automatic Captions that shows a higher
error rate for women and non-American speakers. Facebook’s AI research
department created a dataset of people with different skin tones, gender
and age to evaluate speech recognition models for biases and found
significant differences between lighter and darker skin tones (Liu et
al. 2021).<a href="#fnref48" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn49" role="doc-endnote"><p><span class="citation"
data-cites="amodeiDeepSpeechEndtoEnd2015">(<a
href="#ref-amodeiDeepSpeechEndtoEnd2015" role="doc-biblioref">Amodei et
al. 2015</a>)</span><a href="#fnref49" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn50" role="doc-endnote"><p>The first Alexa recording in a court
case was handed over after the defendant agreed handing over his data.
The Independent, Amazon Echo could become key witness in murder
investigation after data turned over to police, 2017. Link:<a
href="https://www.independent.co.uk/news/world/americas/amazon-echo-murder-investigation-data-police-a7621261.html">https://www.independent.co.uk/news/world/americas/amazon-echo-murder-investigation-data-police-a7621261.html</a><a
href="#fnref50" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn51" role="doc-endnote"><p>Link:<a
href="https://commonvoice.mozilla.org/de">https://commonvoice.mozilla.org/de</a><a
href="#fnref51" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn52" role="doc-endnote"><p>Link to LibriVox:
https://librivox.org/ and <span class="citation"
data-cites="panayotovLibrispeechASRCorpus2015">(<a
href="#ref-panayotovLibrispeechASRCorpus2015"
role="doc-biblioref">Panayotov et al. 2015</a>)</span><a href="#fnref52"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn53" role="doc-endnote"><p>Link to the LDC Switchboard-1
Release 2: https://catalog.ldc.upenn.edu/LDC97S62<a href="#fnref53"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn54" role="doc-endnote"><p>The title “I’d blush if I could” is
also the response Siri gives to the insult “You’re a bitch”<a
href="#fnref54" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn55" role="doc-endnote"><p>Nadine Alessio’s project website.
Link:<a
href="http://nadinelessio.com/projects.html">http://nadinelessio.com/projects.html</a><a
href="#fnref55" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn56" role="doc-endnote"><p>Vlahos, 2020.<a href="#fnref56"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn57" role="doc-endnote"><p>Ibid. p. 117.<a href="#fnref57"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn58" role="doc-endnote"><p>Tay was the name of a chat bot
Microsoft intended to have a teenage personality and could be interacted
with over multiple channels. Twitter trolls co-opted the bot with
sexist, racist and antisemitic questions that the bot replied to with
generic answers. They also made use of a repeat-after-me phrase to make
it look like the bot is spewing hateful comments itself. After only 16
hours Microsoft deleted all accounts and the PR disaster was immense,
with headlines happily personifying the AI that has learned to be
racist. But no continuous learning algorithm was involved. Link to a
blog post by Russel Cameron Thomas explaining the technology stack
behind Tay:<a
href="https://exploringpossibilityspace.blogspot.com/2016/03/microsoft-tayfail-smoking-gun-alice.html">https://exploringpossibilityspace.blogspot.com/2016/03/microsoft-tayfail-smoking-gun-alice.html</a><a
href="#fnref58" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn59" role="doc-endnote"><p><span class="citation"
data-cites="GENDERDIVERSITYGastvortrag">(<a
href="#ref-GENDERDIVERSITYGastvortrag"
role="doc-biblioref"><span>“GENDER &amp; DIVERSITY: Gastvortrag von
Ekheo - Musik und Kunst Privatuniversität der Stadt Wien”</span>
n.d.</a>)</span><a href="#fnref59" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn60" role="doc-endnote"><p>The Oracle was a terminal computer
connected to GPT-3 with a generic Q&amp;A prompt.<a href="#fnref60"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn61" role="doc-endnote"><p>From the github repository
CorentinJ/Real-Time-Voice-Cloning. Link: <span class="citation"
data-cites="jemineRealTimeVoiceCloning2022">(<a
href="#ref-jemineRealTimeVoiceCloning2022" role="doc-biblioref">Corentin
Jemine [2019] 2022</a>)</span>. and his master thesis: <span
class="citation" data-cites="jemineMasterThesisAutomatic">(<a
href="#ref-jemineMasterThesisAutomatic" role="doc-biblioref">C. Jemine,
n.d.</a>)</span><a href="#fnref61" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn62" role="doc-endnote"><p><span class="citation"
data-cites="jiaTransferLearningSpeaker2019">(<a
href="#ref-jiaTransferLearningSpeaker2019" role="doc-biblioref">Jia et
al. 2019</a>)</span><a href="#fnref62" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn63" role="doc-endnote"><p>The SV2TTS pipeline is based on
these previous papers: <span class="citation"
data-cites="wanGeneralizedEndtoEndLoss2020">(<a
href="#ref-wanGeneralizedEndtoEndLoss2020" role="doc-biblioref">Wan et
al. 2020</a>)</span> → <span class="citation"
data-cites="wangTacotronEndtoEndSpeech2017">(<a
href="#ref-wangTacotronEndtoEndSpeech2017" role="doc-biblioref">Wang et
al. 2017</a>)</span> → <span class="citation"
data-cites="oordWaveNetGenerativeModel2016">(<a
href="#ref-oordWaveNetGenerativeModel2016" role="doc-biblioref">Oord et
al. 2016</a>)</span><a href="#fnref63" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn64" role="doc-endnote"><p><span class="citation"
data-cites="CloneSyntheticAI">(<a href="#ref-CloneSyntheticAI"
role="doc-biblioref"><span>“Clone <span>Synthetic AI Voices</span> with
<span>Neural Text</span> to <span>Speech</span>”</span>
n.d.</a>)</span><a href="#fnref64" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn65" role="doc-endnote"><p>Hydra was developed by Olivia Jack.
Link: https://hydra.ojack.xyz<a href="#fnref65" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
</body>
</html>
