
@online{AAA,
  title = {AAA},
  url = {https://www.kunstforum.de/artikel/aaa/},
  urldate = {2022-01-16},
  abstract = {AAA Art, Algorithmen, Artificial Intelligence von Peter Weibel Künstliche Intelligenz (K.I.) bzw. Artificial Intelligence (A.I.) gibt es nicht. Aber ein Ensemble von Maschinen, Medien, Programmen, Algorithmen, Hardware und Software hat zu einem außerordentlich großen, vielteiligen und produktiven Forschungsfeld geführt, das A.I. genannt wird. Anlässlich einer Konferenz im Dartmouth College haben 1956 John McCarthy und Marvin … Continued},
  langid = {german},
  organization = {{www.kunstforum.de}},
  file = {/home/ms/Zotero/storage/3MJK7YXC/aaa.html}
}

@unpublished{amodeiDeepSpeechEndtoEnd2015,
  title = {Deep {{Speech}} 2: {{End-to-End Speech Recognition}} in {{English}} and {{Mandarin}}},
  shorttitle = {Deep {{Speech}} 2},
  author = {Amodei, Dario and Anubhai, Rishita and Battenberg, Eric and Case, Carl and Casper, Jared and Catanzaro, Bryan and Chen, Jingdong and Chrzanowski, Mike and Coates, Adam and Diamos, Greg and Elsen, Erich and Engel, Jesse and Fan, Linxi and Fougner, Christopher and Han, Tony and Hannun, Awni and Jun, Billy and LeGresley, Patrick and Lin, Libby and Narang, Sharan and Ng, Andrew and Ozair, Sherjil and Prenger, Ryan and Raiman, Jonathan and Satheesh, Sanjeev and Seetapun, David and Sengupta, Shubho and Wang, Yi and Wang, Zhiqian and Wang, Chong and Xiao, Bo and Yogatama, Dani and Zhan, Jun and Zhu, Zhenyao},
  date = {2015-12-08},
  eprint = {1512.02595},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1512.02595},
  urldate = {2022-01-21},
  abstract = {We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech—two vastly different languages. Because it replaces entire pipelines of hand-engineered components with neural networks, end-to-end learning allows us to handle a diverse variety of speech including noisy environments, accents and different languages. Key to our approach is our application of HPC techniques, resulting in a 7x speedup over our previous system [26]. Because of this efficiency, experiments that previously took weeks now run in days. This enables us to iterate more quickly to identify superior architectures and algorithms. As a result, in several cases, our system is competitive with the transcription of human workers when benchmarked on standard datasets. Finally, using a technique called Batch Dispatch with GPUs in the data center, we show that our system can be inexpensively deployed in an online setting, delivering low latency when serving users at scale.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/home/ms/Zotero/storage/BVKR3QKN/Amodei et al. - 2015 - Deep Speech 2 End-to-End Speech Recognition in En.pdf}
}

@online{AnatomyAISystem2018,
  title = {Anatomy of an {{AI System}}},
  date = {2018},
  url = {http://www.anatomyof.ai},
  urldate = {2022-04-23},
  abstract = {Anatomy of an AI System - The Amazon Echo as an anatomical map of human labor, data and planetary resources. By Kate Crawford and Vladan Joler (2018)},
  langid = {english},
  organization = {{Anatomy of an AI System}},
  file = {/home/ms/Zotero/storage/N6H2MIPY/anatomyof.ai.html}
}

@online{Avatars,
  title = {Avatars},
  url = {https://www.kanno.so/project/avatars},
  urldate = {2022-04-28},
  langid = {american},
  organization = {{So Kanno}},
  file = {/home/ms/Zotero/storage/7BKLUWLQ/avatars.html}
}

@online{Babycastles,
  title = {Babycastles},
  url = {https://www.babycastles.com},
  urldate = {2022-01-30},
  abstract = {The online home of Babycastles},
  langid = {american},
  organization = {{Babycastles}},
  file = {/home/ms/Zotero/storage/KCKCXQLJ/www.babycastles.com.html}
}

@article{bostromAreYouLiving2001,
  title = {Are {{You Living}} in a {{Computer Simulation}}?},
  author = {Bostrom, Nick},
  date = {2001},
  pages = {14},
  url = {https://www.simulation-argument.com/simulation.pdf},
  langid = {english},
  file = {/home/ms/Zotero/storage/C5HAHFTN/Bostrom - Are You Living in a Computer Simulation.pdf}
}

@article{brownConditionedReflexesPavlov1928,
  title = {Conditioned {{Reflexes}}. {{By I}}. {{P}}. {{Pavlov}} . {{Translated}} and Edited by {{G}}. {{V}}. {{AnrepM}}.{{D}}., {{D}}.{{Sc}}., ({{Oxford University Press}}: {{Humphrey Milford}}. 1927. {{Pp}}. Xv + 430. {{Price}} 28s.)},
  shorttitle = {Conditioned {{Reflexes}}. {{By I}}. {{P}}. {{Pavlov}} . {{Translated}} and Edited by {{G}}. {{V}}. {{AnrepM}}.{{D}}., {{D}}.{{Sc}}., ({{Oxford University Press}}},
  author = {Brown, William},
  date = {1928-07},
  journaltitle = {Philosophy},
  volume = {3},
  number = {11},
  pages = {380--383},
  publisher = {{Cambridge University Press}},
  issn = {1469-817X, 0031-8191},
  doi = {10.1017/S0031819100028242},
  url = {https://www.cambridge.org/core/journals/philosophy/article/abs/conditioned-reflexes-by-i-p-pavlov-translated-and-edited-by-g-v-anrepmd-dsc-oxford-university-press-humphrey-milford-1927-pp-xv-430-price-28s/7A78701537AC3B74C66E676EAA3B9DDA},
  urldate = {2022-01-16},
  abstract = {//static.cambridge.org/content/id/urn\%3Acambridge.org\%3Aid\%3Aarticle\%3AS0031819100028242/resource/name/firstPage-S0031819100028242a.jpg},
  langid = {english},
  file = {/home/ms/Zotero/storage/UMYRDUMF/7A78701537AC3B74C66E676EAA3B9DDA.html}
}

@online{CanonCatMac2014,
  title = {The {{Canon Cat The Mac}}'s {{Ancestor}}},
  date = {2014-10-25},
  url = {https://web.archive.org/web/20141025053109/http://www.jagshouse.com/swyft.html},
  urldate = {2022-04-28},
  file = {/home/ms/Zotero/storage/95X6KXX2/swyft.html}
}

@online{CloneSyntheticAI,
  title = {Clone {{Synthetic AI Voices}} with {{Neural Text}} to {{Speech}}},
  url = {https://www.resemble.ai/},
  urldate = {2022-01-26},
  abstract = {Custom AI Voice Generator. Create realistic text-to-speech AI voices with Resemble's voice cloning software. Real-time API's and 44kHz audio.},
  langid = {american},
  organization = {{Resemble AI}},
  file = {/home/ms/Zotero/storage/8T6K4TDH/www.resemble.ai.html}
}

@online{coleThisHorrifyingApp2019,
  title = {This {{Horrifying App Undresses}} a {{Photo}} of {{Any Woman With}} a {{Single Click}}},
  author = {Cole, Samantha},
  date = {2019-06-26T21:48:06},
  url = {https://www.vice.com/en/article/kzm59x/deepnude-app-creates-fake-nudes-of-any-woman},
  urldate = {2022-01-24},
  abstract = {The \$50 DeepNude app dispenses with the idea that deepfakes were about anything besides claiming ownership over women’s bodies.},
  langid = {english},
  organization = {{Vice}},
  keywords = {AI,App Which Shows Women Naked,Dangers of Deepfake,Deepfakes,DeepNude app,Machine Learning},
  file = {/home/ms/Zotero/storage/85GVHWFL/deepnude-app-creates-fake-nudes-of-any-woman.html}
}

@online{DescartesWasWrong2017,
  title = {Descartes Was Wrong: ‘A Person Is a Person through Other Persons’ | {{Aeon Ideas}}},
  shorttitle = {Descartes Was Wrong},
  date = {2017-04-07},
  url = {https://aeon.co/ideas/descartes-was-wrong-a-person-is-a-person-through-other-persons},
  urldate = {2022-01-30},
  abstract = {What’s a better way to understand human psychology – ‘I think, therefore I am’ or ‘A person is a person through other persons’?},
  langid = {english},
  organization = {{Aeon}},
  file = {/home/ms/Zotero/storage/6UQPGPZU/descartes-was-wrong-a-person-is-a-person-through-other-persons.html}
}

@book{europeancommission.jointresearchcentre.AIWatchDefining2020,
  title = {{{AI}} Watch: Defining {{Artificial Intelligence}} : Towards an Operational Definition and Taxonomy of Artificial Intelligence.},
  shorttitle = {{{AI}} Watch},
  author = {{European Commission. Joint Research Centre.}},
  date = {2020},
  publisher = {{Publications Office}},
  location = {{LU}},
  url = {https://data.europa.eu/doi/10.2760/382730},
  urldate = {2022-01-16},
  langid = {english}
}

@online{EverythingDLVIDOREILLY,
  title = {Everything — {{DΛVID OREILLY}} • Computer Art \& Research},
  url = {https://www.davidoreilly.com/everything/},
  urldate = {2022-04-28},
  file = {/home/ms/Zotero/storage/KY3UKK3I/everything.html}
}

@online{FREEPORTANATOMIESBLACK,
  title = {{{FREEPORT}} 1: {{ANATOMIES OF A BLACK BOX}}},
  shorttitle = {{{FREEPORT}} 1},
  url = {https://www.mataderomadrid.org/en/calls/freeport-1-anatomies-black-box},
  urldate = {2022-04-23},
  abstract = {Taking Joler and Crawford’s “Anatomy of an AI system” as the starting point, you will be led into a roller coaster trip through the simple internet packet delivery system, following with Facebook’s sophisticated algorithmic factory, and going even deeper into fashionable corporate devices that actually operate as planetary-scale systems of knowledge extraction under the futurist label of machine learning and the likes.},
  langid = {english},
  organization = {{Matadero Madrid}},
  file = {/home/ms/Zotero/storage/ZI5HQR4J/freeport-1-anatomies-black-box.html}
}

@article{gadeWhatUbuntuDifferent2012,
  title = {What Is {{{\emph{Ubuntu}}}} ,? {{Different Interpretations}} among {{South Africans}} of {{African Descent}}},
  shorttitle = {What Is {{{\emph{Ubuntu}}}} ,?},
  author = {Gade, Christian B.N.},
  date = {2012-01},
  journaltitle = {South African Journal of Philosophy},
  shortjournal = {South African Journal of Philosophy},
  volume = {31},
  number = {3},
  pages = {484--503},
  issn = {0258-0136, 2073-4867},
  doi = {10.1080/02580136.2012.10751789},
  url = {http://www.tandfonline.com/doi/full/10.1080/02580136.2012.10751789},
  urldate = {2022-01-30},
  abstract = {In this article, I describe and systematize the different answers to the question ‘What is ubuntu?’ that I have been able to identify among South Africans of African descent (SAADs). I show that it is possible to distinguish between two clusters of answers. The answers of the first cluster all define ubuntu as a moral quality of a person, while the answers of the second cluster all define ubuntu as a phenomenon (for instance a philosophy, an ethic, African humanism, or, a worldview) according to which persons are interconnected. The concept of a person is of central importance to all the answers of both clusters, which means that to understand these answers, it is decisive to raise the question of who counts as a person according to SAADs. I show that some SAADs define all Homo sapiens as persons, whereas others hold the view that only some Homo sapiens count as persons: only those who are black, only those who have been incorporated into personhood, or only those who behave in a morally acceptable manner.},
  langid = {english},
  file = {/home/ms/Zotero/storage/NSRIRVRQ/Gade - 2012 - What is Ubuntu , Different Interpretations.pdf}
}

@online{GENDERDIVERSITYGastvortrag,
  title = {GENDER \& DIVERSITY: Gastvortrag von Ekheo - Musik und Kunst Privatuniversität der Stadt Wien},
  shorttitle = {GENDER \& DIVERSITY},
  url = {https://muk.ac.at/veranstaltung/gender-diversity-gastvortrag-von-ekheo.html},
  urldate = {2022-01-24},
  abstract = {Musik und Kunst Privatuniversität der Stadt Wien},
  langid = {austrian},
  file = {/home/ms/Zotero/storage/2A9BM9PL/gender-diversity-gastvortrag-von-ekheo.html}
}

@unpublished{heStreamingEndtoendSpeech2018,
  title = {Streaming {{End-to-end Speech Recognition For Mobile Devices}}},
  author = {He, Yanzhang and Sainath, Tara N. and Prabhavalkar, Rohit and McGraw, Ian and Alvarez, Raziel and Zhao, Ding and Rybach, David and Kannan, Anjuli and Wu, Yonghui and Pang, Ruoming and Liang, Qiao and Bhatia, Deepti and Shangguan, Yuan and Li, Bo and Pundak, Golan and Sim, Khe Chai and Bagby, Tom and Chang, Shuo-yiin and Rao, Kanishka and Gruenstein, Alexander},
  date = {2018-11-15},
  eprint = {1811.06621},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1811.06621},
  urldate = {2022-01-20},
  abstract = {End-to-end (E2E) models, which directly predict output character sequences given input speech, are good candidates for on-device speech recognition. E2E models, however, present numerous challenges: In order to be truly useful, such models must decode speech utterances in a streaming fashion, in real time; they must be robust to the long tail of use cases; they must be able to leverage user-specific context (e.g., contact lists); and above all, they must be extremely accurate. In this work, we describe our efforts at building an E2E speech recognizer using a recurrent neural network transducer. In experimental evaluations, we find that the proposed approach can outperform a conventional CTC-based model in terms of both latency and accuracy in a number of evaluation categories.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/ms/Zotero/storage/64JQ2A7I/He et al. - 2018 - Streaming End-to-end Speech Recognition For Mobile.pdf;/home/ms/Zotero/storage/5L2N43LQ/1811.html}
}

@article{hintonDeepNeuralNetworks2012b,
  title = {Deep {{Neural Networks}} for {{Acoustic Modeling}} in {{Speech Recognition}}: {{The Shared Views}} of {{Four Research Groups}}},
  shorttitle = {Deep {{Neural Networks}} for {{Acoustic Modeling}} in {{Speech Recognition}}},
  author = {Hinton, Geoffrey and Deng, Li and Yu, Dong and Dahl, George and Mohamed, Abdel-rahman and Jaitly, Navdeep and Senior, Andrew and Vanhoucke, Vincent and Nguyen, Patrick and Sainath, Tara and Kingsbury, Brian},
  date = {2012-11},
  journaltitle = {IEEE Signal Processing Magazine},
  shortjournal = {IEEE Signal Process. Mag.},
  volume = {29},
  number = {6},
  pages = {82--97},
  issn = {1053-5888},
  doi = {10.1109/MSP.2012.2205597},
  url = {http://ieeexplore.ieee.org/document/6296526/},
  urldate = {2022-01-20},
  langid = {english},
  file = {/home/ms/Zotero/storage/C3IAM2YE/Hinton et al. - 2012 - Deep Neural Networks for Acoustic Modeling in Spee.pdf}
}

@online{HumanguidedBurritoBots2019,
  title = {Human-Guided Burrito Bots Raise Questions about the Future of Robo-Delivery},
  date = {2019-06-03T10:54:22+00:00},
  url = {https://thehustle.co/kiwibots-autonomous-food-delivery/},
  urldate = {2022-04-28},
  abstract = {A startup called Kiwi Campus created a popular food-delivering robot at UC Berkeley -- but it still requires lots of human hands to run.},
  langid = {american},
  organization = {{The Hustle}},
  file = {/home/ms/Zotero/storage/QAPX9HF2/kiwibots-autonomous-food-delivery.html}
}

@report{hungerKuratierenUndDessen2021,
  title = {Kuratieren und dessen statistische Automatisierung mittels Künstlicher 'Intelligenz'.},
  author = {Hunger, Francis},
  date = {2021-10-21},
  institution = {{Zenodo}},
  doi = {10.5281/zenodo.5589930},
  url = {https://zenodo.org/record/5589930},
  urldate = {2022-01-14},
  abstract = {Das in diesem Working Paper diskutierte Konzept des Post-AI-Curating untersucht das Kuratieren als wissensbildendes Verfahren, unterstützt durch Mustererkennung und gewichtete Netze, die als technische Mittel der Künstlichen ‚Intelligenz‘ zum Einsatz kommen. Dafür diskutiert der Text eine Reihe aufeinander aufbauende Konzepte wie Kuratieren, Kurator*in, das Kuratorische, kuratorische Forschung, Post-Human Curating und Post-AI Curating. Im Anschluss werden eine Reihe von Projekten, die sich dem Kuratieren mit Mitteln Künstlicher ‚Intelligenz‘ nähern, als Fallbeispiele diskutiert: The Next Biennial Should Be Curated by a Machine von UBERMORGEN, Leonardo Impett und Joasia Krysa (2021) als Meta-Kunstwerk über Kuratieren und Biennalen; Tillmann Ohms Projekt Artificial Curator (2020), welches in eine automatisiert kuratierte Ausstellung mündete; und \#Exstrange von Rebekah Modrak and Marialaura Ghidini und weiteren (2017), welches auf der Plattform Ebay die Kunstwerke als Datenobjekte inszeniert. Schließlich schält der Text zusammenfassend Eingebettet-Sein, Big-Data-Infrastrukturen, Räumlichkeit und Informationsmodell, Solutionismus und Digital Humanities, Auswahl, Ähnlichkeit als Momente des Post-AI-Curating heraus.},
  langid = {deu},
  keywords = {Das Kuratorische,Daten,Künstliche Intelligenz,Kuratieren,Kurator*in,Medienkunst,Post-Internet,Training the Archive},
  file = {/home/ms/Zotero/storage/GGQPFBWI/Hunger - 2021 - Kuratieren und dessen statistische Automatisierung.pdf}
}

@article{jemineMasterThesisAutomatic,
  title = {Master Thesis : {{Automatic Multispeaker Voice Cloning}}},
  author = {Jemine, C},
  pages = {38},
  langid = {english},
  file = {/home/ms/Zotero/storage/JNVUEWWZ/Jemine - Master thesis  Automatic Multispeaker Voice Cloni.pdf}
}

@software{jemineRealTimeVoiceCloning2022,
  title = {Real-{{Time Voice Cloning}}},
  author = {Jemine, Corentin},
  date = {2022-01-26T14:01:33Z},
  origdate = {2019-05-26T08:56:15Z},
  url = {https://github.com/CorentinJ/Real-Time-Voice-Cloning},
  urldate = {2022-01-26},
  abstract = {Clone a voice in 5 seconds to generate arbitrary speech in real-time},
  keywords = {deep-learning,python,pytorch,tensorflow,tts,voice-cloning}
}

@article{jeremijenkoDialogueMonologueVoice,
  title = {Dialogue with a {{Monologue}}: {{Voice Chips}} and the {{Products}} of {{Abstract Speech}}},
  author = {Jeremijenko, Natalie},
  pages = {32},
  abstract = {This paper examines the use of integrated circuits that produce speech in consumer products, commonly called voice chips. The goal of this paper is to document what these products actually say and to try to understand what the voices of these products represent, specifically, what they say about techno-social relations. The paper describes how voice chip technology differs from other 'talking hardware' of the recording and communications industries, and places it in a unique social position. I then survey the voice chip patent literature and sample the products currently on the market. Finally, I investigate how the voices of these products can be interpreted as speech and interaction, drawing largely upon Suchman's examination of human-machine interaction. I conclude by using the chips’ voice to question their performance of abstract speech, if they demonstrate preprogrammable interaction, and therefore what we mean when we attribute speech as literal agency to technological products.},
  langid = {english},
  file = {/home/ms/Zotero/storage/HPIRFENX/Jeremijenko - Dialogue with a Monologue Voice Chips and the Pro.pdf}
}

@unpublished{jiaTransferLearningSpeaker2019,
  title = {Transfer {{Learning}} from {{Speaker Verification}} to {{Multispeaker Text-To-Speech Synthesis}}},
  author = {Jia, Ye and Zhang, Yu and Weiss, Ron J. and Wang, Quan and Shen, Jonathan and Ren, Fei and Chen, Zhifeng and Nguyen, Patrick and Pang, Ruoming and Moreno, Ignacio Lopez and Wu, Yonghui},
  date = {2019-01-02},
  eprint = {1806.04558},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  url = {http://arxiv.org/abs/1806.04558},
  urldate = {2022-01-26},
  abstract = {We describe a neural network-based system for text-to-speech (TTS) synthesis that is able to generate speech audio in the voice of different speakers, including those unseen during training. Our system consists of three independently trained components: (1) a speaker encoder network, trained on a speaker verification task using an independent dataset of noisy speech without transcripts from thousands of speakers, to generate a fixed-dimensional embedding vector from only seconds of reference speech from a target speaker; (2) a sequence-to-sequence synthesis network based on Tacotron 2 that generates a mel spectrogram from text, conditioned on the speaker embedding; (3) an auto-regressive WaveNet-based vocoder network that converts the mel spectrogram into time domain waveform samples. We demonstrate that the proposed model is able to transfer the knowledge of speaker variability learned by the discriminatively-trained speaker encoder to the multispeaker TTS task, and is able to synthesize natural speech from speakers unseen during training. We quantify the importance of training the speaker encoder on a large and diverse speaker set in order to obtain the best generalization performance. Finally, we show that randomly sampled speaker embeddings can be used to synthesize speech in the voice of novel speakers dissimilar from those used in training, indicating that the model has learned a high quality speaker representation.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/ms/Zotero/storage/5VZZTST8/Jia et al. - 2019 - Transfer Learning from Speaker Verification to Mul.pdf}
}

@book{kernighanProgrammingLanguage1988,
  title = {The {{C}} Programming Language},
  author = {Kernighan, Brian W. and Ritchie, Dennis M.},
  date = {1988},
  edition = {2nd ed},
  publisher = {{Prentice Hall}},
  location = {{Englewood Cliffs, N.J}},
  isbn = {978-0-13-110370-2 978-0-13-110362-7},
  pagetotal = {272},
  keywords = {C (Computer program language)}
}

@article{krizhevskyImageNetClassificationDeep2017,
  title = {{{ImageNet}} Classification with Deep Convolutional Neural Networks},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  date = {2017-05-24},
  journaltitle = {Communications of the ACM},
  shortjournal = {Commun. ACM},
  volume = {60},
  number = {6},
  pages = {84--90},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/3065386},
  url = {https://dl.acm.org/doi/10.1145/3065386},
  urldate = {2022-01-24},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
  langid = {english},
  file = {/home/ms/Zotero/storage/P3W6G2DH/Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf}
}

@unpublished{krizhevskyOneWeirdTrick2014,
  title = {One Weird Trick for Parallelizing Convolutional Neural Networks},
  author = {Krizhevsky, Alex},
  date = {2014-04-26},
  eprint = {1404.5997},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1404.5997},
  urldate = {2022-01-20},
  abstract = {I present a new way to parallelize the training of convolutional neural networks across multiple GPUs. The method scales significantly better than all alternatives when applied to modern convolutional neural networks.},
  archiveprefix = {arXiv},
  version = {2},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/ms/Zotero/storage/TKUJFK3G/Krizhevsky - 2014 - One weird trick for parallelizing convolutional ne.pdf;/home/ms/Zotero/storage/HAQJLTBI/1404.html}
}

@unpublished{liuMeasuringFairnessSpeech2021,
  title = {Towards {{Measuring Fairness}} in {{Speech Recognition}}: {{Casual Conversations Dataset Transcriptions}}},
  shorttitle = {Towards {{Measuring Fairness}} in {{Speech Recognition}}},
  author = {Liu, Chunxi and Picheny, Michael and Sarı, Leda and Chitkara, Pooja and Xiao, Alex and Zhang, Xiaohui and Chou, Mark and Alvarado, Andres and Hazirbas, Caner and Saraf, Yatharth},
  date = {2021-11-18},
  eprint = {2111.09983},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  url = {http://arxiv.org/abs/2111.09983},
  urldate = {2022-01-21},
  abstract = {It is well known that many machine learning systems demonstrate bias towards specific groups of individuals. This problem has been studied extensively in the Facial Recognition area, but much less so in Automatic Speech Recognition (ASR). This paper presents initial Speech Recognition results on “Casual Conversations” – a publicly released 846 hour corpus designed to help researchers evaluate their computer vision and audio models for accuracy across a diverse set of metadata, including age, gender, and skin tone. The entire corpus has been manually transcribed, allowing for detailed ASR evaluations across these metadata. Multiple ASR models are evaluated, including models trained on LibriSpeech, 14,000 hour transcribed, and over 2 million hour untranscribed social media videos. Significant differences in word error rate across gender and skin tone are observed at times for all models. We are releasing human transcripts from the Casual Conversations dataset to encourage the community to develop a variety of techniques to reduce these statistical biases.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/ms/Zotero/storage/QQ6SWT4X/Liu et al. - 2021 - Towards Measuring Fairness in Speech Recognition .pdf}
}

@software{Lucid2022,
  title = {Lucid},
  date = {2022-01-15T20:25:57Z},
  origdate = {2018-01-25T17:41:44Z},
  url = {https://github.com/tensorflow/lucid},
  urldate = {2022-01-16},
  abstract = {A collection of infrastructure and tools for research in neural network interpretability.},
  organization = {{tensorflow}},
  keywords = {colab,interpretability,jupyter-notebook,machine-learning,tensorflow,visualization}
}

@article{lukoseTextSpeechSynthesizerFormant2017,
  title = {Text to {{Speech Synthesizer-Formant Synthesis}}},
  author = {Lukose, Sneha and Upadhya, Savitha S},
  date = {2017},
  pages = {4},
  abstract = {In this paper, different methods of text to speech synthesizer techniques are discussed to produce intelligible and natural output and a vowel synthesizer using cascade formant technique is implemented. A text to speech output is based on generating corresponding sound output when the text is inputted. Wide range of applications use text to speech technique in medicals, telecommunications fields, etc. The Various speech synthesis methods that have been used for text to speech output for obtaining intelligible and natural output are Concatenative, Formant, Articulatory, Hidden Markov model (HMM).},
  langid = {english},
  file = {/home/ms/Zotero/storage/WUIGD37H/Lukose and Upadhya - 2017 - Text to Speech Synthesizer-Formant Synthesis.pdf}
}

@video{lyrebirdLyrebirdCreateDigital2017,
  title = {Lyrebird - {{Create}} a Digital Copy of Your Voice.},
  editor = {{Lyrebird}},
  date = {2017-09-04},
  url = {https://www.youtube.com/watch?v=YfU_sWHT8mo},
  urldate = {2022-01-24},
  abstract = {Go to www.lyrebird.ai to create a digital copy of your voice. Barack Obama. (In this video, we generate not only the audio but also parts of the video. As far as we know, this is the very first time that it is done.)},
  editortype = {director}
}

@book{millerArtistMachineWorld2019,
  title = {The Artist in the Machine: The World of {{AI}} Powered Creativity},
  shorttitle = {The Artist in the Machine},
  author = {Miller, Arthur I.},
  date = {2019},
  publisher = {{The MIT Press}},
  location = {{Cambridge, Massachusetts}},
  isbn = {978-0-262-04285-7 978-0-262-53962-3},
  pagetotal = {399},
  keywords = {Art and computers,Computer art,Creative ability}
}

@book{molnar10LearnedFeatures,
  title = {10.1 {{Learned Features}} | {{Interpretable Machine Learning}}},
  author = {Molnar, Christoph},
  url = {https://christophm.github.io/interpretable-ml-book/cnn-features.html},
  urldate = {2022-01-16},
  abstract = {Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable.}
}

@article{olahFeatureVisualization2017,
  title = {Feature {{Visualization}}},
  author = {Olah, Chris and Mordvintsev, Alexander and Schubert, Ludwig},
  date = {2017-11-07},
  journaltitle = {Distill},
  shortjournal = {Distill},
  volume = {2},
  number = {11},
  pages = {e7},
  issn = {2476-0757},
  doi = {10.23915/distill.00007},
  url = {https://distill.pub/2017/feature-visualization},
  urldate = {2022-01-16},
  abstract = {How neural networks build up their understanding of images},
  langid = {english}
}

@unpublished{oordWaveNetGenerativeModel2016,
  title = {{{WaveNet}}: {{A Generative Model}} for {{Raw Audio}}},
  shorttitle = {{{WaveNet}}},
  author = {van den Oord, Aaron and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
  date = {2016-09-19},
  eprint = {1609.03499},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1609.03499},
  urldate = {2022-01-24},
  abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-ofthe-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound},
  file = {/home/ms/Zotero/storage/SIAAXZCB/Oord et al. - 2016 - WaveNet A Generative Model for Raw Audio.pdf}
}

@inproceedings{panayotovLibrispeechASRCorpus2015,
  title = {Librispeech: {{An ASR}} Corpus Based on Public Domain Audio Books},
  shorttitle = {Librispeech},
  booktitle = {2015 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Panayotov, Vassil and Chen, Guoguo and Povey, Daniel and Khudanpur, Sanjeev},
  date = {2015-04},
  pages = {5206--5210},
  publisher = {{IEEE}},
  location = {{South Brisbane, Queensland, Australia}},
  doi = {10.1109/ICASSP.2015.7178964},
  url = {http://ieeexplore.ieee.org/document/7178964/},
  urldate = {2022-01-21},
  abstract = {This paper introduces a new corpus of read English speech, suitable for training and evaluating speech recognition systems. The LibriSpeech corpus is derived from audiobooks that are part of the LibriVox project, and contains 1000 hours of speech sampled at 16 kHz. We have made the corpus freely available for download, along with separately prepared language-model training data and pre-built language models. We show that acoustic models trained on LibriSpeech give lower error rate on the Wall Street Journal (WSJ) test sets than models trained on WSJ itself. We are also releasing Kaldi scripts that make it easy to build these systems.},
  eventtitle = {{{ICASSP}} 2015 - 2015 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  isbn = {978-1-4673-6997-8},
  langid = {english},
  file = {/home/ms/Zotero/storage/B6C6XH92/Panayotov et al. - 2015 - Librispeech An ASR corpus based on public domain .pdf}
}

@article{pasquinelliArborescentMindIntelligence,
  title = {The {{Arborescent Mind}}: {{The Intelligence}} of an {{Inverted Tree}}},
  shorttitle = {The {{Arborescent Mind}}},
  author = {Pasquinelli, Matteo},
  url = {https://www.academia.edu/27431916/The_Arborescent_Mind_The_Intelligence_of_an_Inverted_Tree},
  urldate = {2022-01-29},
  abstract = {The aim of this text is not to prove a linear academic thesis, yet to record a spectrum of homologies and resonances across the history of the tree as a symbolic and logic form. The inspiration for this sort of Warburgian excursus comes from the},
  langid = {english},
  file = {/home/ms/Zotero/storage/FZ4LMNN4/The_Arborescent_Mind_The_Intelligence_of.pdf;/home/ms/Zotero/storage/K2R9DGW7/27431916.html}
}

@book{pavlovConditionedReflexesInvestigation1927,
  title = {Conditioned Reflexes: An Investigation of the Physiological Activity of the Cerebral Cortex},
  shorttitle = {Conditioned Reflexes},
  author = {Pavlov, I. P.},
  date = {1927},
  series = {Conditioned Reflexes: An Investigation of the Physiological Activity of the Cerebral Cortex},
  pages = {xv, 430},
  publisher = {{Oxford Univ. Press}},
  location = {{Oxford, England}},
  abstract = {The present volume is the first complete discussion of conditioned reflexes to be translated into one of the more familiar European languages. It contains 23 lectures, most of which were delivered in the spring of 1924 at the Military Medical Academy in Leningrad. After an initial discussion of historical background and of the technical methods employed, Pavlov discusses the following topics: the formation of conditioned reflexes by means of conditioned and direct stimuli; external and internal inhibition of conditioned reflexes; the analyzing and synthesizing activity of the cerebral hemisphere; irradiation and concentration of nervous processes in the cerebral cortex; mutual induction of excitation and inhibition; interaction of irradiation and concentration with induction; the cortex as a mosaic of functions; development of inhibition in the cortex under the influence of conditioned stimuli; internal inhibition and sleep as one and the same process with regard to their intimate mechanism; transition stages between the alert state and complete sleep-hypnotic stages; different types of nervous system; pathological disturbances of the cortex, result of functional and surgical interference; general characteristics of the present investigation and its special difficulties; discovery of certain errors necessitating the modification of some earlier interpretations; and the experimental results obtained with animals in their application to man. Attention is drawn to the similarity of the neuroses and psychoses to behavior observed in the dog during certain of the experiments. A bibliography is given of all papers published from Pavlov's laboratories upon the physiology of conditioned reflexes. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  pagetotal = {xv, 430},
  file = {/home/ms/Zotero/storage/ECG4SZ7U/1927-02531-000.html}
}

@article{pierceHowAppleFinally,
  title = {How {{Apple Finally Made Siri Sound More Human}}},
  author = {Pierce, David},
  journaltitle = {Wired},
  issn = {1059-1028},
  url = {https://www.wired.com/story/how-apple-finally-made-siri-sound-more-human/},
  urldate = {2022-01-23},
  abstract = {If Apple can make Siri sound less like a robot and more like someone you know and trust, it can make the virtual assistant great—even when it fails.},
  entrysubtype = {magazine},
  langid = {american},
  keywords = {apple,artificial intelligence,iphone,siri},
  file = {/home/ms/Zotero/storage/JVSU5RMD/how-apple-finally-made-siri-sound-more-human.html}
}

@audio{PoliticiansDiscussingLyrebird,
  title = {Politicians discussing about Lyrebird},
  url = {https://soundcloud.com/user-535691776/dialog},
  urldate = {2022-01-24},
  abstract = {Listen to Politicians discussing about Lyrebird by Lyrebird \#np on \#SoundCloud},
  langid = {german},
  file = {/home/ms/Zotero/storage/74TDKLTL/dialog.html}
}

@online{RuDALLE,
  title = {{{ruDALL-E}}},
  url = {http://rudalle.ru/en/},
  urldate = {2022-04-28},
  abstract = {Look, which image generated ruDALL-E},
  langid = {english},
  organization = {{ruDALL-E}},
  file = {/home/ms/Zotero/storage/HJQAD2IN/en.html}
}

@book{serexheWolfgangKempelenMan2007,
  title = {Wolfgang von Kempelen: Man-(in the)-Machine = Wolfgang von Kempelen: Mensch-(in der)-Maschine},
  shorttitle = {Wolfgang von Kempelen},
  editor = {Serexhe, Bernhard and Weibel, Peter and von Kempelen, Wolfgang and Zentrum für Kunst und Medientechnologie Karlsruhe},
  date = {2007},
  publisher = {{Matthes \& Seitz}},
  location = {{Berlin}},
  isbn = {978-3-88221-997-5},
  langid = {ger eng},
  pagetotal = {111},
  file = {/home/ms/Zotero/storage/LHMDMRQR/Serexhe et al. - 2007 - Wolfgang von Kempelen Man-(in the)-Machine = Wolf.pdf}
}

@article{shipmanAnimalConnectionHuman2010,
  title = {The {{Animal Connection}} and {{Human Evolution}}},
  author = {Shipman, Pat},
  date = {2010-08},
  journaltitle = {Current Anthropology},
  shortjournal = {Current Anthropology},
  volume = {51},
  number = {4},
  pages = {519--538},
  issn = {0011-3204, 1537-5382},
  doi = {10.1086/653816},
  url = {https://www.journals.uchicago.edu/doi/10.1086/653816},
  urldate = {2022-01-22},
  langid = {english},
  file = {/home/ms/Zotero/storage/4TZ6ZMKQ/Shipman - 2010 - The Animal Connection and Human Evolution.pdf}
}

@unpublished{simonyanDeepConvolutionalNetworks2014,
  title = {Deep {{Inside Convolutional Networks}}: {{Visualising Image Classification Models}} and {{Saliency Maps}}},
  shorttitle = {Deep {{Inside Convolutional Networks}}},
  author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
  date = {2014-04-19},
  eprint = {1312.6034},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1312.6034},
  urldate = {2022-01-20},
  abstract = {This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013].},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/ms/Zotero/storage/4F2FSXI8/Simonyan et al. - 2014 - Deep Inside Convolutional Networks Visualising Im.pdf;/home/ms/Zotero/storage/PTL4QYUJ/1312.html}
}

@online{stattAdobeWorkingAudio2016,
  title = {Adobe Is Working on an Audio App That Lets You Add Words Someone Never Said},
  author = {Statt, Nick},
  date = {2016-11-03T18:30:45-04:00},
  url = {https://www.theverge.com/2016/11/3/13514088/adobe-photoshop-audio-project-voco},
  urldate = {2022-01-24},
  abstract = {Adobe is working on a new piece of software that would act like a Photoshop for audio, according to Adobe developer Zeyu Jin, who spoke at the Adobe MAX conference in San Diego, California today....},
  langid = {english},
  organization = {{The Verge}},
  file = {/home/ms/Zotero/storage/J7K68F8M/adobe-photoshop-audio-project-voco.html}
}

@article{stuppFraudstersUsedAI2019,
  title = {Fraudsters {{Used AI}} to {{Mimic CEO}}’s {{Voice}} in {{Unusual Cybercrime Case}}},
  author = {Stupp, Catherine},
  date = {2019-08-30T16:52:00},
  journaltitle = {Wall Street Journal},
  issn = {0099-9660},
  url = {https://www.wsj.com/articles/fraudsters-use-ai-to-mimic-ceos-voice-in-unusual-cybercrime-case-11567157402},
  urldate = {2022-01-24},
  abstract = {Criminals used artificial intelligence-based software to impersonate a chief executive’s voice and demand a fraudulent transfer of funds in March in what cybercrime experts described as an unusual case of artificial intelligence being used in hacking.},
  entrysubtype = {newspaper},
  journalsubtitle = {WSJ Pro},
  langid = {american},
  keywords = {artificial intelligence,Artificial Intelligence/Machine Learning,Bobby Filar,business in europe,Business in Europe,business in the u.k.,Business in the U.K.,c&e executive news filter,C&E Executive News Filter,c&e industry news filter,C&E Industry News Filter,computer science,Computer Science,content types,Content Types,corporate,corporate crime,Corporate Crime/Legal Action,Corporate/Industrial News,crime,Crime/Legal Action,cybercrime,Cybercrime/Hacking,Euler Hermes Group,factiva filters,Factiva Filters,financial services,Financial Services,fraud,Fraud,general news,hacking,humanities,industrial news,insurance,Insurance,Irakli Beridze,legal action,machine learning,management,Management,non-life insurance,Non-life Insurance,Philipp Amann,political,Political/General News,PRO,Rüdiger Kirsch,sciences,Sciences/Humanities,senior level management,Senior Level Management,trade credit insurance,Trade Credit Insurance,WSJ-PRO-CYBER,WSJ-PRO-WSJ.com},
  file = {/home/ms/Zotero/storage/5CAVQB44/fraudsters-use-ai-to-mimic-ceos-voice-in-unusual-cybercrime-case-11567157402.html}
}

@inproceedings{tatmanGenderDialectBias2017,
  title = {Gender and {{Dialect Bias}} in {{YouTube}}'s {{Automatic Captions}}},
  booktitle = {Proceedings of the {{First ACL Workshop}} on {{Ethics}} in {{Natural Language Processing}}},
  author = {Tatman, Rachael},
  date = {2017},
  pages = {53--59},
  publisher = {{Association for Computational Linguistics}},
  location = {{Valencia, Spain}},
  doi = {10.18653/v1/W17-1606},
  url = {http://aclweb.org/anthology/W17-1606},
  urldate = {2022-01-20},
  abstract = {This project evaluates the accuracy of YouTube’s automatically-generated captions across two genders and five dialects of English. Speakers’ dialect and gender was controlled for by using videos uploaded as part of the “accent tag challenge”, where speakers explicitly identify their language background. The results show robust differences in accuracy across both gender and dialect, with lower accuracy for 1) women and 2) speakers from Scotland. This finding builds on earlier research finding that speaker’s sociolinguistic identity may negatively impact their ability to use automatic speech recognition, and demonstrates the need for sociolinguistically-stratified validation of systems.},
  eventtitle = {Proceedings of the {{First ACL Workshop}} on {{Ethics}} in {{Natural Language Processing}}},
  langid = {english},
  file = {/home/ms/Zotero/storage/8JEA4XT7/Tatman - 2017 - Gender and Dialect Bias in YouTube's Automatic Cap.pdf}
}

@book{vlahosTalkMeAmazon2020,
  title = {Talk to Me: {{Amazon}}, {{Google}}, {{Apple}} and the Race for Voice-Controlled {{AI}}},
  shorttitle = {Talk to Me},
  author = {Vlahos, James},
  date = {2020},
  abstract = {The next great technological disruption is coming. The titans of Silicon Valley are racing to build the last, best computer that the world will ever need. They know that whoever successfully creates it will revolutionise our relationship with technology and make billions of dollars in the process. They call it conversational AI. Computers that can speak and think like humans do may seem like the stuff of science fiction, but they are rapidly moving towards reality. In Talk to Me, veteran tech journalist James Vlahos meets the researchers at Google, Amazon and Apple who are leading the way to a voice computing revolution. He explores how voice tech will transform every sector of society handing untold new powers to businesses, upending traditional notions of privacy, revolutionising access to information, and fundamentally altering the way we understand human consciousness. And he even tries to understand the significance of the revolution firsthand - by building a chatbot version of his terminally ill father. Vlahos's research leads him to one fundamental question: What happens when our computers become as articulate, compassionate, and creative as we are?},
  isbn = {978-1-84794-264-7},
  langid = {english},
  annotation = {OCLC: 1148063381}
}

@article{wangDefiningArtificialIntelligence2019,
  title = {On {{Defining Artificial Intelligence}}},
  author = {Wang, Pei},
  date = {2019-01-01},
  journaltitle = {Journal of Artificial General Intelligence},
  shortjournal = {Journal of Artificial General Intelligence},
  volume = {10},
  pages = {1--37},
  doi = {10.2478/jagi-2019-0002},
  abstract = {This article systematically analyzes the problem of defining “artificial intelligence.” It starts by pointing out that a definition influences the path of the research, then establishes four criteria of a good working definition of a notion: being similar to its common usage, drawing a sharp boundary, leading to fruitful research, and as simple as possible. According to these criteria, the representative definitions in the field are analyzed. A new definition is proposed, according to it intelligence means “adaptation with insufficient knowledge and resources.” The implications of this definition are discussed, and it is compared with the other definitions. It is claimed that this definition sheds light on the solution of many existing problems and sets a sound foundation for the field.},
  file = {/home/ms/Zotero/storage/VXAT5EST/Wang - 2019 - On Defining Artificial Intelligence.pdf}
}

@unpublished{wanGeneralizedEndtoEndLoss2020,
  title = {Generalized {{End-to-End Loss}} for {{Speaker Verification}}},
  author = {Wan, Li and Wang, Quan and Papir, Alan and Moreno, Ignacio Lopez},
  date = {2020-11-09},
  eprint = {1710.10467},
  eprinttype = {arxiv},
  primaryclass = {cs, eess, stat},
  url = {http://arxiv.org/abs/1710.10467},
  urldate = {2022-01-26},
  abstract = {In this paper, we propose a new loss function called generalized end-to-end (GE2E) loss, which makes the training of speaker verification models more efficient than our previous tuple-based endto-end (TE2E) loss function. Unlike TE2E, the GE2E loss function updates the network in a way that emphasizes examples that are difficult to verify at each step of the training process. Additionally, the GE2E loss does not require an initial stage of example selection. With these properties, our model with the new loss function decreases speaker verification EER by more than 10\%, while reducing the training time by 60\% at the same time. We also introduce the MultiReader technique, which allows us to do domain adaptation —training a more accurate model that supports multiple keywords (i.e., “OK Google” and “Hey Google”) as well as multiple dialects.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Audio and Speech Processing,Statistics - Machine Learning},
  file = {/home/ms/Zotero/storage/KWSS9QLC/Wan et al. - 2020 - Generalized End-to-End Loss for Speaker Verificati.pdf}
}

@unpublished{wangTacotronEndtoEndSpeech2017,
  title = {Tacotron: {{Towards End-to-End Speech Synthesis}}},
  shorttitle = {Tacotron},
  author = {Wang, Yuxuan and Skerry-Ryan, R. J. and Stanton, Daisy and Wu, Yonghui and Weiss, Ron J. and Jaitly, Navdeep and Yang, Zongheng and Xiao, Ying and Chen, Zhifeng and Bengio, Samy and Le, Quoc and Agiomyrgiannakis, Yannis and Clark, Rob and Saurous, Rif A.},
  date = {2017-04-06},
  eprint = {1703.10135},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1703.10135},
  urldate = {2022-01-24},
  abstract = {A text-to-speech synthesis system typically consists of multiple stages, such as a text analysis frontend, an acoustic model and an audio synthesis module. Building these components often requires extensive domain expertise and may contain brittle design choices. In this paper, we present Tacotron, an end-to-end generative text-to-speech model that synthesizes speech directly from characters. Given {$<$}text, audio{$>$} pairs, the model can be trained completely from scratch with random initialization. We present several key techniques to make the sequence-tosequence framework perform well for this challenging task. Tacotron achieves a 3.82 subjective 5-scale mean opinion score on US English, outperforming a production parametric system in terms of naturalness. In addition, since Tacotron generates speech at the frame level, it’s substantially faster than sample-level autoregressive methods.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Sound},
  file = {/home/ms/Zotero/storage/8LBZTJI6/Wang et al. - 2017 - Tacotron Towards End-to-End Speech Synthesis.pdf}
}

@unpublished{wangTacotronEndtoEndSpeech2017a,
  title = {Tacotron: {{Towards End-to-End Speech Synthesis}}},
  shorttitle = {Tacotron},
  author = {Wang, Yuxuan and Skerry-Ryan, R. J. and Stanton, Daisy and Wu, Yonghui and Weiss, Ron J. and Jaitly, Navdeep and Yang, Zongheng and Xiao, Ying and Chen, Zhifeng and Bengio, Samy and Le, Quoc and Agiomyrgiannakis, Yannis and Clark, Rob and Saurous, Rif A.},
  date = {2017-04-06},
  eprint = {1703.10135},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1703.10135},
  urldate = {2022-01-26},
  abstract = {A text-to-speech synthesis system typically consists of multiple stages, such as a text analysis frontend, an acoustic model and an audio synthesis module. Building these components often requires extensive domain expertise and may contain brittle design choices. In this paper, we present Tacotron, an end-to-end generative text-to-speech model that synthesizes speech directly from characters. Given {$<$}text, audio{$>$} pairs, the model can be trained completely from scratch with random initialization. We present several key techniques to make the sequence-tosequence framework perform well for this challenging task. Tacotron achieves a 3.82 subjective 5-scale mean opinion score on US English, outperforming a production parametric system in terms of naturalness. In addition, since Tacotron generates speech at the frame level, it’s substantially faster than sample-level autoregressive methods.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Sound},
  file = {/home/ms/Zotero/storage/JWMBYXP4/Wang et al. - 2017 - Tacotron Towards End-to-End Speech Synthesis.pdf}
}

@online{WaveNetLaunchesGoogle,
  title = {WaveNet launches in the Google Assistant},
  url = {https://deepmind.com/blog/article/wavenet-launches-google-assistant},
  urldate = {2022-01-24},
  abstract = {Just over a year ago we presented WaveNet, a new deep neural network for generating raw audio waveforms that is capable of producing better and more realistic-sounding speech than existing techniques. At that time, the model was a research prototype and was too computationally intensive to work in consumer products. ~But over the last 12 months we have worked hard to significantly improve both the speed and quality of our model and today we are proud to announce that an updated version of WaveNet is being used to generate the Google Assistant voices for US English and Japanese across all platforms.Using the new WaveNet model results in a range of more natural sounding voices for the Assistant.},
  langid = {ALL},
  organization = {{Deepmind}},
  file = {/home/ms/Zotero/storage/45Z2BUJB/wavenet-launches-google-assistant.html}
}

@book{zarkadakesOurOwnImage2016,
  title = {In Our Own Image: Savior or Destroyer?: The History and Future of Artificial Intelligence},
  shorttitle = {In Our Own Image},
  author = {Zarkadakēs, Giōrgos},
  date = {2016},
  edition = {First Pegasus books hardcover edition},
  publisher = {{Pegasus Books LLC}},
  location = {{New York, NY}},
  isbn = {978-1-60598-964-8 978-1-84604-435-9 978-1-84604-436-6 978-1-84604-437-3 978-1-5046-8648-8},
  pagetotal = {362},
  keywords = {Artificial intelligence,Computers and civilization,Human-computer interaction,Machine theory,Popular works,Social aspects},
  annotation = {OCLC: ocn911364564}
}


